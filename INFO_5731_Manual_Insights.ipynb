{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd53eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manual_Insights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purpose: to improve dialogue systems by levera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Purpose: the prevalence of unsafe behavior, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Purpose: the problem of hallucinations in neur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Purpose: the issue of generating accurate and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Purpose: the difficulties of optimizing binary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Purpose: the limitations of prior generative a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose: the gap for multi-party dialogues in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Purpose: the task of detecting linguistically ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purpose: the performance of AI pair programmer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Purpose: the time-consuming decoding in k-near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose: They provide MILDecoding, a method th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Purpose: Using psycholinguistic and computatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Purpose: BART and GPTs are two examples of pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Purpose: Using extensive language models, they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Purpose: They suggest a way to express CCG in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Purpose: In order to assess the UIE system's c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Purpose: Retrieval augmentation has been prove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Purpose: The fact that many existing text gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Purpose: One task for fine-grained sentiment c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Purpose: They present a brand-new adversarial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Purpose: to introduce a new approach to deal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Purpose: to develop a model for hate speech de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Purpose: to enhance the performance of of mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Purpose: to find out the usage of Event argume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Purpose: to create a soft-prompt-based approac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Purpose: to find out the extent to which large...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Purpose: to develop a neural machine translati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Purpose: to generate or create a new method to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose: to check the single frame bias in vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Purpose: to develop a method to resolve issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Purpose: Here Authors investigate grounded wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Purpose: To develop a framework to assess how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Purpose: Here the authors aimed to develop a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Purpose: To develop methods that leverage Larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Purpose: The main purpose of the paper is to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Purpose: The main purpose of the paper is to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Purpose: The purpose of the authors is to impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Purpose: The purpose of the paper is to develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Purpose: The purpose of the paper is i.e.., Mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Purpose: The authors of this paper aim to impr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Manual_Insights\n",
       "0   Purpose: to improve dialogue systems by levera...\n",
       "1   Purpose: the prevalence of unsafe behavior, su...\n",
       "2   Purpose: the problem of hallucinations in neur...\n",
       "3   Purpose: the issue of generating accurate and ...\n",
       "4   Purpose: the difficulties of optimizing binary...\n",
       "5   Purpose: the limitations of prior generative a...\n",
       "6   Purpose: the gap for multi-party dialogues in ...\n",
       "7   Purpose: the task of detecting linguistically ...\n",
       "8   Purpose: the performance of AI pair programmer...\n",
       "9   Purpose: the time-consuming decoding in k-near...\n",
       "10  Purpose: They provide MILDecoding, a method th...\n",
       "11  Purpose: Using psycholinguistic and computatio...\n",
       "12  Purpose: BART and GPTs are two examples of pre...\n",
       "13  Purpose: Using extensive language models, they...\n",
       "14  Purpose: They suggest a way to express CCG in ...\n",
       "15  Purpose: In order to assess the UIE system's c...\n",
       "16  Purpose: Retrieval augmentation has been prove...\n",
       "17  Purpose: The fact that many existing text gene...\n",
       "18  Purpose: One task for fine-grained sentiment c...\n",
       "19  Purpose: They present a brand-new adversarial ...\n",
       "20  Purpose: to introduce a new approach to deal w...\n",
       "21  Purpose: to develop a model for hate speech de...\n",
       "22  Purpose: to enhance the performance of of mach...\n",
       "23  Purpose: to find out the usage of Event argume...\n",
       "24  Purpose: to create a soft-prompt-based approac...\n",
       "25  Purpose: to find out the extent to which large...\n",
       "26  Purpose: to develop a neural machine translati...\n",
       "27  Purpose: to generate or create a new method to...\n",
       "28  Purpose: to check the single frame bias in vid...\n",
       "29  Purpose: to develop a method to resolve issues...\n",
       "30  Purpose: Here Authors investigate grounded wor...\n",
       "31  Purpose: To develop a framework to assess how ...\n",
       "32  Purpose: Here the authors aimed to develop a n...\n",
       "33  Purpose: To develop methods that leverage Larg...\n",
       "34  Purpose: The main purpose of the paper is to d...\n",
       "35  Purpose: The main purpose of the paper is to i...\n",
       "36  Purpose: The purpose of the authors is to impr...\n",
       "37  Purpose: The purpose of the paper is to develo...\n",
       "38  Purpose: The purpose of the paper is i.e.., Mu...\n",
       "39  Purpose: The authors of this paper aim to impr..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define your list of texts (replace with your actual texts)\n",
    "texts = [\n",
    "    \"Purpose: to improve dialogue systems by leveraging multiple user simulators simultaneously. Contribution: a new framework called MUST (Multi-user Simulator Training) for training task-oriented dialogue systems. MUST leverages multiple user simulators to train a dialogue system, with the goal of improving the system’s robustness to various user behaviors. Method: MUST (Multi-user Simulator Training). MUST addresses the challenge of training a dialogue system on a single user simulator, which can lead to the system being over-fitted to that specific simulator and under-fitted to others. MUST accomplishes this by leveraging multiple user simulators simultaneously. The authors propose three ways to implement MUST: MUSTmerging, MUSTCRL, and MUSTuniform. MUSTadaptive is the method ultimately chosen by the authors as it tackles the challenges of balancing exploration and exploitation, and avoiding catastrophic forgetting. MUSTadaptive utilizes an adaptively-updated distribution to select user simulators during training. Dataset: didn't find information may have used personal dataset. Conclusion: that MUST leads to a more robust dialogue system that can perform well with unseen user simulators. The authors propose a framework called MUST that leverages multiple user simulators to train task-oriented dialogue systems. MUST addresses the challenges of over-fitting to specific user simulators and catastrophic forgetting by formulating the problem as a Multi-armed bandits (MAB) problem. Their method, MUSTadaptive, achieves better performance than training with a single user simulator. MUSTadaptive balances adapting to different user simulators and uniformly exploring all simulators during training. Future work: we can do advances to the model used in this paper.\",\n",
    "    \"Purpose: the prevalence of unsafe behavior, such as toxic languages and harmful suggestions, in open-domain end-to-end dialogue systems, or chatbots. Contribution: The authors constructed a new dataset called SafeConv for the research of conversational safety. SafeConv provides unsafe spans in an utterance, indicating which words contribute to the detected unsafe behavior, and safe alternative responses to continue the conversation when unsafe behavior is detected. Method: The authors benchmarked three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Dataset: The dataset used in this paper is SafeConv, which is constructed by the authors for the research of conversational safety1. Conclusion: The experiments showed that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. Future work: the findings suggest that there is potential for further research in explaining the emergence of unsafe behavior and detoxifying chatbots.\",\n",
    "    \"Purpose: the problem of hallucinations in neural machine translation. The authors aim to alleviate this issue. Contribution: The authors propose a method that evaluates the percentage of the source contribution to a generated translation. They also show that using sentence similarity from cross-lingual embeddings further improves these results. Method: The authors use a method that evaluates the percentage of the source contribution to a generated translation. They also use sentence similarity from cross-lingual embeddings. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of machine translations. Conclusion: The method proposed by the authors improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Future work: the findings suggest that there is potential for further research in detecting and mitigating hallucinations in machine translation.\",\n",
    "    \"Purpose: the issue of generating accurate and informative explanations for recommendations, especially when historical user reviews of items are often insufficient. Contribution: The authors propose a novel model, ERRA (Explainable Recommendation by personalized Review retrieval and Aspect learning), which can obtain additional information from the training sets with retrieval enhancement. They also incorporate an aspect enhancement component into their model to better capture users’ preferences. Method: The authors use a method that combines personalized review retrieval and aspect learning. By selecting the top-n aspects that users are most concerned about for different items, they can model user representation with more relevant details. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of user reviews. Conclusion: The experiments show that their model outperforms state-of-the-art baselines (for example, 3.4% improvement in prediction and 15.8% improvement in explanation for TripAdvisor). Future work: the findings suggest that there is potential for further research in explainable recommendation with personalized review retrieval and aspect learning.\",\n",
    "    \"Purpose: the difficulties of optimizing binary and ternary neural networks, especially for transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space1. Contribution: The authors demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Method: The authors approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of text for summarization and machine translation. Conclusion: The authors’ ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficient. Their binary model, while less accurate, achieves a highly non-trivial score of 35.6. For machine translation, they achieved BLEU scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full precision mBART model score of 26.81. Future work: the findings suggest that there is potential for further research in binary and ternary natural language generation.\",\n",
    "    \"Purpose: the limitations of prior generative and discriminative approaches in schema-guided dialogue state tracking. The authors aim to achieve better generalization and efficiency. Contribution: The authors introduce SPLAT, a novel architecture that constrains outputs to a limited prediction space, allowing for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. Method: The authors use a method that evaluates the percentage of the source contribution to a generated translation. They also use sentence similarity from cross-lingual embeddings. Dataset: The authors demonstrate the effectiveness of their model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Conclusion: The authors’ approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, they show increased robustness on the SGD-X benchmark - their model outperforms the more than 30x larger D3ST-XXL model by 5.0 points. Future work: the findings suggest that there is potential for further research in schema-guided dialogue state tracking.\",\n",
    "    \"Purpose: the gap for multi-party dialogues in dialogue response generation. Unlike two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Contribution: The authors propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Method: The authors use an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of multi-party dialogues. Conclusion: Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of the proposed method. Future work: the findings suggest that there is potential for further research in multi-party dialogue response generation.\",\n",
    "    \"Purpose: the task of detecting linguistically complex named entities in low-context text. The authors aim to address the data scarcity problem in low-resource complex Named Entity Recognition (NER). Contribution: The authors present ACLM (Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation. Method: ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - they use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of complex named entities. Conclusion: The authors demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all their neural baselines by a significant margin (1%-36%). In addition, they demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods. Future work: The findings suggest that there is potential for further research in generative data augmentation for low-resource complex NER.\",\n",
    "    \"Purpose: the performance of AI pair programmers that automatically synthesize programs for data wrangling and analytic tasks given natural language (NL) intents from users. Contribution: The authors build ARCADE, a benchmark of 1,078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. Method: To establish a strong baseline on this challenging task, the authors develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Dataset: The dataset used in this paper is ARCADE, which is a benchmark of 1,078 code generation problems using the pandas data analysis framework in data science notebooks. Conclusion: The authors explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explain ability of model predictions. Future work: The findings suggest that there is potential for further research in natural language to code generation in interactive data science notebooks.\",\n",
    "    \"Purpose: the time-consuming decoding in k-nearest-neighbor machine translation (kNN-MT). The authors aim to improve the decoding speed of kNN-MT. Contribution: The authors propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efficient distance computation technique that is suitable for subset neighbor search using a look-up table. Method: The authors use a method that retrieves neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences. They also use an efficient distance computation technique that is suitable for subset neighbor search using a look-up table. Dataset: The paper does not explicitly mention the dataset used. However, it can be inferred that they used a dataset of machine translations. Conclusion: The authors’ proposed method achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 De-En translation task and the domain adaptation tasks in De-En and En-Ja. Future work: The findings suggest that there is potential for further research in nearest neighbor machine translation.\",\n",
    "    \n",
    "    \"Purpose: They provide MILDecoding, a method that uses a trained multiple instance learning (MIL) network to interpolate language models and detoxify them at the token level. To forecast the overall toxicity and the toxicity of each word in its context, the MIL model is trained on a corpus where each text has a toxicity label. Intuitively, the MIL network augments the original language model to prevent toxicity by computing a toxicity distribution over subsequent tokens based on the created context. Both automated metrics and human evaluation are used to assess MIL-Decoding, and the results show that it performs better in detoxification than other baselines and has little negative effects on generation fluency. Contribution: The paper's unique approach to solving the widespread problem of toxicity in language generation models is what makes it noteworthy. The research presents a viable way to prevent harmful language creation by presenting MIL-Decoding, which combines a multiple instance learning (MIL) network to analyze and reduce toxicity at the token level. This breakthrough preserves generating fluency while improving the quality of generated text by lowering toxicity, as both automated metrics and human assessment bear this. This kind of contribution is important for society because it encourages the appropriate use of language. Method: The core idea of MIL-Decoding is to enhance the LM probability distribution with a MIL network that computes a toxicity score. They first introduce the MIL network architecture and analyze the toxicity score produced by the network. And then, they provide a detailed description of our approach MIL-Decoding in section. 1. MIL Network 2. MIL-Decoding Detoxification Dataset: This paper conduct experiments on two datasets. RealToxicityPrompts is extracted from sentences in OPENWEBTEXT CORPUS, a large English corpus of web text that consists of 100K prompts. Authors, randomly sampled 10K prompts from RealToxicityPrompts for evaluation, since the test time of some baselines is extremely long. Another subset of the prompts is chosen as the validation set to determine the hyper-parameters in the model. Conclusion: They have introduced MIL-Decoding, which can detoxify pre-trained LMs at token-level and outperform other methods in toxicity mitigation. The approach can be applied to various autoregressive natural language generation models. The success of their proposed method in detoxification illustrates the importance of combing token generation with contextual semantics. Future work: In future week, they will explore how to balance generation fluency better.\",\n",
    "    \"Purpose: Using psycholinguistic and computational experiments they compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically guided antecedent retrieval processes. Contribution: (i) the release of wide-covering and highly controlled datasets to evaluate control structures in Spanish and Galician, (ii) a psycholinguistic evaluation of humans’ performance, a computational evaluation of monolingual and multilingual LMs’ performance, and a careful comparison between humans and LMs; (iii) a demonstration of the limitations of LMs to capture grammatical information thanks to the adversarial example of control constructions. Method: This paper's methodological approach combines psycholinguistic tests and computer analysis in a multimodal way to investigate how control dependencies in Spanish phrases are understood. The study performs human acceptability evaluation tests and evaluates the effectiveness of several pre-trained language models (LMs) in detecting control structures using carefully designed datasets. The research systematically assesses human and LM comprehension and processing of control interdependence using three different experiments, including human assessment and LM prediction tasks. Dataset: They have created several datasets that have been used for a human acceptability judgement task (Experiment 1), a LM acceptability task (Experiment 2), and a LM prediction task (Experiment 3). For the main dataset, used in Experiments 1 and 2, the experimental materials consisted of 96 items that had 8 different versions (768 experimental sentences). Conclusion: The empirical evidence gathered in this work provides a very straightforward picture: whereas humans’ can coordinate lexico-semantic and syntactic information in order to determine the (un)acceptability of control structures, LMs resort to a heuristic based on linear proximity, disregarding control information. These findings are robust, as they replicate across tasks (acceptability and masked prediction), models (monolingual and multilingual), languages (Spanish and Galician LMs), and type of antecedent (names and pronouns). Furthermore, they go in line with evidence advanced in for English with respect to autoregressive language models. Future work: The results of this article may be expanded upon in future research to help create more linguistically complicated language models that better comprehend and represent intricate syntactic and semantic relationships, especially in languages other than English. This may include investigating new designs that explicitly incorporate syntactic and semantic information, or it could be improving the pre-training objectives that are currently in place.\",\n",
    "    \"Purpose: BART and GPTs are two examples of pre-trained autoregressive (AR) language models that have dominated Open-ended Long Text Generation (OpenLTG). However, their implementation in Open-LTG is hindered by the AR nature, which decreases the inference efficiency as generation duration increases. They also investigate the possibilities of pre-trained masked language models (MLMs) in conjunction with a representative iterative non-autoregressive (NAR) decoding technique for Open-LTG in order to increase inference efficiency. According to our first research, pre-trained MLMs are only capable of producing brief text; they will break down when it comes to modeling large texts. Contribution: The limits of autoregressive (AR) language models, such as BART and GPTs, with longer generations of data—a lower inference efficiency—in open-ended long text generation (OpenLTG are discussed in this study. To address this obstacle, it suggests utilizing non-autoregressive (NAR) decoding algorithms in combination with pre-trained masked language models (MLMs). The research presents novel approaches to address model collapse problems in MLMs during OpenLTG by introducing Dynamic Sliding Window Attention (DSWA) and Linear Temperature Decay (LTD). Experiments show notable gains in speed and performance, indicating that pre-trained MLMs are viable options for the OpenLTG community. Method: They propose two simple yet effective strategies for attention mechanism and inference to mitigate the model collapse problems: Dynamic Sliding Window Attention (DSWA) and Linear Temperature Decay (LTD). These designs do not break the paradigm of MLM so that it can flexibly adapt to the pre-trained models. Dataset: Experiments on two OpenLTG tasks (i.e., storytelling and multi-paragraph opinionated article writing) with four widely-used datasets. They use Writing Prompt (WP) and ROC Stories (ROC) datasets to conduct experiments for validating whether pre-trained MLMs can work better on Open-LTG tasks. Conclusion: This work investigates Open-LTG using NAR models that are trained on pre-trained MLMs. For open-ended text generation, They first looked at the capabilities and constraints of MLMs in conjunction with the iterative NAR inference. They found that MLMs would collapse for Open-LTG. After a thorough investigation and analysis, They determined that the improper attention mechanism and inference methodologies were to blame. Then presented two straightforward solutions to this issue: dynamic sliding window attention and linear temperature decay. Experiments show that our approach delivers a notable speedup and competitive performance. Our study aims to establish pre-trained MLMs as fresh prospects for the Open-LTG community. Future work: Paper’s primary concern focuses on accelerating the generation speed, and they will put how to solve these problems in future work. They will add grammar corrections for each iteration in future work to help the model produce better results. They also will improve the text quality and overall fluency and solve the above problems for Open-LTG scenarios in future work.\",\n",
    "    \"Purpose: Using extensive language models, they investigate semantic constructs in grammatical constructions. Firstly, the project word embeddings from context into three alternative interpretable semantic spaces, each with its own set of norms for psycholinguistic features. After validating these interpretable spaces, then apply them to automatically extract semantic characterizations of lexical items in two grammatical constructions: the AANN construction (e.g., 'a beautiful three days') and nouns in subject or object position inside the same sentence. It is demonstrated that a word in subject position is seen to be more agentive than the identical word in object position. Additionally, nouns in the AANN construction are understood to be more measurement-like than they are in the standard alternation paradigm. Contribution: The study uses extensive language models and interpretable semantic spaces to present a novel method for comprehending semantic constructs within grammatical constructions. Through mapping psycholinguistic feature norms to contextual word embeddings, the approach provides insights into the semantic properties of lexical items within particular constructions. This advances our knowledge of how language constructs meaning and offers a useful tool for examining syntactic phenomena that are not as well understood. The approach advances linguistic research by facilitating the automatic extraction of semantic characterizations and provides a flexible methodology that can be applied to different linguistic domains. Method: Learning to map the contextual word embedding space to an interpretable space defined by feature norms, where each dimension corresponds to a semantic feature, is the task at hand. By matching embeddings from contextual word vectors with feature norms, they build the training set. They train models at the type level, such as the one depicted in the upper half of Figure 1, mapping the embedding vectors for the word ring to the set of feature norms for ring. Ultimately, however, they forecast semantic characteristics for specific tokens using the model. By projecting the token vector of a single instance of the word 'ring' into the feature space discovered at the type-level, as illustrated in the lower portion of Figure1. Dataset: They have constructed three semantic spaces, trained from three datasets of psycholinguistic feature norms. The feature norms comprise 541 concrete English nouns and 2,526 features.The feature norms consist of over 4000 English words and 3,981 distinct features, from all open-class parts of speech, and include abstract words. The data consists of 535 English words rated for the relevance of 65 predefined features. Conclusion: In this paper, they honed techniques for predicting semantic features for token embeddings. These projections are versatile. Once created, one and the same model can be used to study a wide array of phenomena. They explored their utility for studying semantic construal in syntactic constructions. They emphasize the potential of this method to answer linguistic questions about meaning differences in constructions that are less well-understood and well-theorized than the ones studied here. As such, They hope it will be possible to use this method to generate linguistic insight. Future work: In this area could concentrate on various directions for additional research and improvement. First off, expanding the method's applicability to a wider variety of syntactic constructions than those covered in the paper may provide more in-depth understanding of the subtle differences in semantic interpretation in various linguistic contexts. Furthermore, investigating the incorporation of extra linguistic features or broader feature norms may improve the precision and level of granularity in semantic characterizations\",\n",
    "    \"Purpose: They suggest a way to express CCG in a continuous vector space as a recursive composition. While recent CCG supertagging and parsing models show good performance overall, they implicitly model phrase structure dependencies using black-box neural architectures. Rather, they make use of the technique of holographic embeddings as a compositional operator to explicitly model the dependencies within the embedding space between phrase structures and words. According to experimental results, when utilizing a C&C parser, holographic composition effectively increases the supertagging accuracy to achieve state-of-the-art parsing performance. Contribution: The study presents an innovative approach for modeling the dependencies between phrase structures and words in Combinatory Categorial Grammar (CCG) in a continuous vector space by using holographic embeddings as compositional operators. Utilizing a C&C parser, the authors show enhanced supertagging accuracy and cutting-edge parsing performance by utilizing this technique.  Method: They use recursive composition operations to create phrase- and sentence-level representations from word embeddings. The effectiveness of holographic compositions in capturing dependencies between sentence components has been validated by experimental results on the CCGbank dataset, which results in improved parsing and supertagging performance. The paper also illustrates the possibility of phrase-level text-infilling using the suggested model.  Dataset: Hockenmaier and Steedman (2007) used a standard split scheme in their experiments on CCGbank, with sections 02–21 designated for training, 00 for development, and 23 for testing. CCGbank statistics are displayed in Appendix B. Conclusion: In this paper, they proposed a new way to build phrase/sentence-level representations from word representations and formulate CCG as a recursive composition operation on a continuous vector space. They gave examples of how useful it is for parsing and supertagging. Tests showed that holographic compositions are useful for explicitly modeling the dependencies between sentence components, leading to better performance and cutting-edge outcomes for parsing and supertagging with the C&C parser. Furthermore, they demonstrated that phrase-level text-infilling is achievable in the suggested model by utilizing the decomposable property of the holographic representation. Future work: Subsequent research endeavors may delve deeper into the more extensive utilization of holographic embeddings in natural language processing assignments. This could potentially lead to advancements in the fields of information retrieval, machine translation, and dialogue systems by advancing language understanding and generation technologies.\",\n",
    "    \"Purpose: In order to assess the UIE system's capabilities in three areas—knowledge sharing and expansion, catastrophic forgetting prevention, and rapid generalization on few-shot and unseen tasks—they study the system in more demanding but realistic scenarios, namely 'lifelong learning' settings. They present Lottery Prompt Tuning (LPT), a novel parameter- and deployment-efficient prompt tuning technique, in order to accomplish these three goals. Using a binary prompt mask, LPT freezes the PLM's parameters and sequentially learns compact pruned prompt vectors for each task, leaving the prompt parameters chosen by earlier tasks unaffected. Furthermore, they demonstrate the potent transferability of Lottery Prompts to novel tasks and perform mask selection using a straightforward yet efficient method. Contribution: introduces a challenging benchmark for universal information extraction (UIE) systems in lifelong learning scenarios. The authors propose a novel method called Lottery Prompt Tuning (LPT), which freezes pre-trained language model parameters and sequentially learns compact pruned prompt vectors for each task. LPT achieves state-of-the-art performance and demonstrates powerful transferability to novel tasks, ensuring robust adaptation and generalization. Method: Figure 2 depicts the general LPT procedure. They simultaneously learn the prompt vectors and a paired task-aware binary prompt mask in order to continuously learn new IE tasks. The mask's purpose is to generate a pruned prompt, such as the Lottery Prompt. To promote knowledge transfer, LPT can selectively reuse previously learned prompt parameters during training for each incoming task. Only soft prompt parameters that haven't been chosen by prior tasks are updated with new parameters. Ultimately, the model solves catastrophic forgetting by using binary masks to isolate the share parameters and obtain the lottery prompt for each task, even though it shares the same set of soft prompts for all tasks. Dataset: They formalize the lifelong UIE benchmark by using 13 IE datasets to build the task sequence, covering all four common IE task types (NER, relation extraction, event extraction, and sentiment extraction). To be more precise, relation extraction tasks include CoNLL04, ACE05-Rel, SciERC, and NYT; NER tasks include ACE04, ACE05-Ent, and CoNLL03.  Tasks for aspect-based sentiment analysis (ABSA) include SemEval-14, SemEval-15, and SemEval-16; tasks for event extraction include CASIE and ACE05-Evt. Conclusion: They examine a lifelong learning paradigm for UIE systems in this paper, which they believe is a significant step in the direction of general IE intelligence. They suggest Lottery Prompt Tuning (LPT), a novel parameter-efficient framework, to accomplish positive knowledge transfer, catastrophic forgetting prevention, and quick generalization. The method's capability is validated by experimental results on three different settings. Future work: Subsequent research endeavors may delve deeper into the more extensive utilization of holographic embeddings in natural language processing assignments. This could potentially lead to advancements in the fields of information retrieval, machine translation, and dialogue systems by advancing language understanding and generation technologies.\",\n",
    "    \"Purpose: Retrieval augmentation has been proven to be effective in a variety of generative NLP tasks by recent studies. With the help of these retrieval-augmented techniques, models can explicitly and non-parametrically gain prior external knowledge, using the retrieved reference instances as cues to improve text generation. These techniques make use of similarity-based retrieval, which is predicated on the straightforward idea that the likelihood that the demonstration label will resemble the input label increases with the degree to which the retrieved demonstration resembles the original input. This theory, however, is not always true in document-level EAE because of the sparsity of event arguments and the complexity of event labels. Contribution: It, greatly advances the field of natural language processing (NLP), especially in the area of document-level event argument extraction (EAE). The paper improves text generation processes by incorporating retrieval-augmented strategies into the EAE framework, which improves the models' ability to incorporate external knowledge. This breakthrough expands the use of retrieval-augmented techniques in generative NLP tasks and improves the performance of EAE models. The authors present a novel method that improves the model's analogical ability and enables the generation of reference vectors as depth cues, further enhancing the comprehension and contextuality of the generated text. This is achieved through their proposed adaptive hybrid retrieval augmentation paradigm. Method: They use the same formulation of document level EAE as Ebner et al. (2020). A document x = {w1, w2,..., w|x|} has a set of events E that are described. Every event e ∈ E has a trigger (a text span in x) that designates it, along with its event type (t). A role set Rt is specified for each event type t. The event type and the role set that goes along with it make up the event schema e. The goal of the task is to extract all (a, r) pairs for each e ∈ E, where r ∈ Rt is the role that a plays and a ∈ x is an argument (a text span in x). In order to produce role records (a series of (a, r) pairs) for retrieval-augmented document-level EAE, we first retrieve the top-k potentially useful demonstrations (discrete or continuous), then fuse them into the decoder. The following describes different retrieval settings after introducing the process of reformulating document-level EAE as Retrieval-Augmented Generation (RAG). Dataset: They use RAMS and WikiEvents, two popular document-level EAE datasets, for our experiments. RAMS offers 9,124 news articles with annotations based on 65 roles and 139 different types of events. 246 annotated documents based on 50 event types and 59 roles are available on WikiEvents. Conclusion: In this paper, they investigate the input and label distribution views of retrieval augmented strategy design for document-level EAE. Furthermore, the retrieval strategies they have introduced are able to recall examples that can be useful in illustrating how the model should approach the task. In order to improve the model's analogical capability, they also suggest a novel adaptive hybrid retrieval augmentation paradigm that generates reference vectors as depth cues. They validate our recently introduced retrieval-augmented models through extensive experiments on the RAMS and WikiEvents datasets. They intend to modify their approach in the future to handle different document-level extraction jobs, like relation extraction at the document level. Future work: In future, they plan to adapt their method to other document-level extraction tasks, such as document-level relation extraction.\",\n",
    "    \"Purpose: The fact that many existing text generation models produce text that is factually inconsistent with inputs in an uncontrollable manner is a serious problem. Current factual consistency metrics are typically trained on synthetic texts or directly transferred from other related tasks, like question answering (QA) and natural language inference (NLI), due to a lack of annotated data. They perform poorly on text that is actually generated by language models due to bias in synthetic text or upstream tasks, particularly for general evaluation for various tasks. WeCheck, a weakly supervised framework that is directly trained on real generated samples from language models with poorly annotated labels, is our solution to this issue. WeCheck first aggregates weak labels from various sources using a generative model to infer the factual labels of generated samples. Second, they use the inferred weakly supervised information to train a target metric, which is a straightforward noise-aware classification model. Contribution: The important problem of factual inconsistency in text generation models is addressed in the paper, which makes a substantial contribution to the field. The paper presents a novel approach that outperforms existing techniques in terms of both performance and time efficiency across various tasks including text summarization, dialogue generation, paraphrasing, and fact checking. It does this by introducing WeCheck, a weakly supervised framework trained directly on real generated samples. WeCheck outperforms previous metrics by combining weak labels from various sources and employing a noise-aware fine-tuning phase. This suggests a viable approach to improving overall performance in natural language processing tasks. Method: In addition to increasing the accuracy of factual consistency checking, the approach presented in this paper paves the way for future developments of more robust metrics, which will eventually aid in the advancement of AI systems that can produce text outputs that are more accurate and dependable. This will benefit society by boosting the dependability and trustworthiness of automated content generation systems. Dataset: A benchmark consisting of 11 datasets of 4 tasks including text summarization, dialogue generation, paraphrasing, and fact checking, where each sample in datasets is annotated with a binary label manually. Separately on each dataset, WeCheck achieves either the best (6 out of 9) or the second best performance in each dataset. Conclusion: In this paper, they propose a weakly supervised framework, WeCheck, that learns a target metric model in a noise-aware fashion by aggregating weakly supervised signals from various sources. WeCheck uses actual generated text for training instead of artificial data or tasks from other metrics, which is how previous metrics were trained. WeCheck first uses a labeling function that aggregates multiple resources to annotate each sample with a probabilistic label. Next, WeCheck uses probabilistic labels to train the target metric model during the noise-aware fine-tuning phase. According to experimental results, WeCheck outperforms earlier techniques in terms of performance and time efficiency. Furthermore, WeCheck has the potential to work with stronger metrics in the future and enhance overall performance even more. Future work: Subsequent research endeavours may centre on enhancing and expanding the functionalities of WeCheck, in addition to investigating its possible uses in domains beyond text production.\",\n",
    "    \"Purpose: One task for fine-grained sentiment classification is aspect-based sentiment analysis (ABSA). Dependency trees have been used extensively in recent work to extract the relationship between aspects and contexts, with notable improvements. However, given that sentiment classification is a semantic task and the dependency tree is a syntactic structure, there isn't much room for improvement. In order to close this gap, they propose the Abstract Meaning Representation (AMR) semantic structure in place of the syntactic dependency tree. Then they propose an AMR-based Path Aggregation Relational Network (APARN) model that fully utilizes semantic structures. Contribution: They modify the ABSA task to include Abstract Meaning Representations. The AMR is better suited for sentiment analysis tasks because it is a semantic structure. To fully exploit semantic structure information and alleviate parser unreliability, they propose a new model called APARN that integrates information from original sentences and AMRs via the path aggregator and the relation-enhanced self-attention mechanism. They test their Aparn on four publicly available datasets, and their results show that it is effective by outperforming the most recent baselines. Method: The approach put forth in this paper makes a substantial contribution to the advancement of sentiment analysis in general and aspect-based sentiment analysis in particular (ABSA). The method is more in line with the semantic requirements of sentiment analysis tasks by introducing the use of Abstract Meaning Representation (AMR) semantic structures rather than more conventional syntactic dependency trees. By incorporating AMRs into the Path Aggregation Relational Network (APARN) model, sentiment classification accuracy is improved by providing a more thorough comprehension of the relationship between aspects and contexts. Furthermore, relation-enhanced self-attention mechanism and path aggregator enable efficient information integration between original sentences and AMRs, resolving parser unreliability concerns and resulting in better performance than current baselines.  Dataset: They use four widely used public standard datasets for their experiments. The Restaurant and Laptop dataset is derived from the SemEval 2014 Task (Pontiki et al., 2014), whereas the Twitter dataset is a compilation of tweets created by Dong et al. (2014). Jiang et al. have made available MAMS, a large-scale multi-aspect dataset (2019). Conclusion: They present the AMR-based Path Aggregation Relational Network for ABSA, or APARN, in this paper. Their model uses the Abstract Meaning Representation semantic structure, which is more in line with the sentiment analysis task, as opposed to the traditional ABSA model, which uses a syntactic structure like a dependency tree. For effective AMR exploitation and information integration between AMRs and input sentences, they suggest the path aggregator and the relation-enhanced selfattention mechanism. Their model is able to outperform other models thanks to these designs. According to experiments conducted on four public datasets, APARN performs better than rival baselines. Future work: In the future, APARN with higher quality AMRs is expected to further improve the level of the ABSA task. They will further generalize it. to more complex sentiment analysis tasks in the future work.\",\n",
    "    \"Purpose: They present a brand-new adversarial purification technique that is centered on fending off textual adversarial attacks. By masking input texts and reconstructing the masked texts using the masked language models, they can introduce noise with the aid of language models. By doing this, they build a process of adversarial purification for textual models that defends against the most popular adversarial attacks, which involve word substitution. They test a number of powerful adversarial attack techniques, such as Textfooler and BERT-Attack, with their suggested adversarial purification method. The experimental findings show that the purification algorithm is capable of successfully fending off strong word-substitution attacks. Contribution: They bring up the issue of fending off adversarial attacks based on substitution without addressing the attacks' form in NLP tasks. To the best of their knowledge, they were the first to combine text adversarial purification with pre-trained models and to take into account adversarial purification as a defense against textual adversarial attacks, such as strong word-substitution attacks. Method: The work makes a substantial contribution to the development of adversarial defense strategies for tasks involving natural language processing (NLP). The authors tackle the urgent problem of thwarting adversarial attacks, especially those that involve word substitution, by presenting a novel adversarial purification technique designed especially for textual models. In order to create an adversarial purification process, their method consists of masking input texts, reconstructing them using masked language models, and introducing noise through language models. They show how well their method works to thwart strong word-substitution attacks through thorough testing against formidable adversarial attack techniques like Textfooler and BERT-Attack, as well as through rigorous experimentation on widely used text classification datasets. Dataset: In their experiments, they make use of two popular text classification datasets: AG's News 2 (Zhang et al., 2015) and IMDB 1 (Maas et al., 2011). The AG's News dataset is a four-class news genre classification task; the IMDB dataset is a bi-polar movie review classification task. In the IMDB dataset, the average length is 220 words, while in the AG's News dataset, it is 40 words. Since the attacking process is significantly slowed down when the model is defensive, they use the test set that comes after the Textfooler 1k test set in the main result and sample 100 samples for the remaining experiments. Conclusion: To counter substitution-based adversarial attacks, they present a textual adversarial purification algorithm in this paper. They recover noisy texts by using pre-trained models' mask-infill capability, and then they use these cleaned texts to generate predictions. Tests demonstrate that the purification technique works well against powerful adversarial attacks without taking into account their substitution range. While prior adversarial purification strategy successes have primarily focused on the image field. Future work: They are the first to consider the adversarial purification method with a multiple-recovering strategy in the text domain. Thus, they anticipate that the adversarial purification technique will be further investigated in NLP applications as an effective defensive tactic.\",\n",
    "    \n",
    "    \"Purpose: to introduce a new approach to deal with NLP issues which is Novel structured prediction framework called it as SPEEH(structured prediction with Energy based Event Centric Hyperspheres).Author also explained how to use this frame and how effective this approach can give accurate results. Contribution: taking the new approach or framework of SPEECH to all people who are struggling to solve so many NLP issues. This approach used by anyone and it also allows capturing computational efficiency. Method: The author used the method or framework of SPEECH which is to take inputs from different types of data and modelling it into single output. It also uses various kinds of parameters to create various results so that to compare and find the best methods and best parameters at which best results can be obtained through this approach. Dataset: Author has used different kinds of datasets for different kinds of tasks and for various purposes.  Conclusion: The author finds that the new approach of SPEECH achieves different kinds of results for different kind of tasks. On overall , the approach worked well for the analysis of intricate relationships. Future work: To take the advancements in this approach of SPEECH, author has decided to test this approach on several other datasets which are both structured and unstructured data.\",\n",
    "    \"Purpose: to develop a model for hate speech detection by using some rules and by training with some examples for the model. This is to predict the sentiment of the human speech and also applies to text or applies to chatbots which are used in every organization. Contribution: This paper supports the Rule based model called “Rule By Example “ for speech detection, in which author mainly focused to detect hate messages and speech. Author also created some logical rules to find the hate speech from training the model with examples. Method: The author used the Rule By Example framework for the model creation and also used for the evaluation. This method is mainly focus on Rules that were designed by the author and based on that trained the model. The outputs are also provided based on that  rules designed. Dataset: The author has used 3 datasets for the analysis, which are Hatexplain is a largescale benchmark dataset, Jigsaw is a large scale wikepedia dataset, Contextual abuse dataset. Conclusion: Author finds novel dual-encoder model architecture designed to produce meaningful rule and text representations. RBE leverages a novel exemplar-based contrastive learning objective that converges the representations of rules and text inputs of similar classes. Future work: This rule based example model has only worked on the model based on the trained examples . Inorder to update the model ,we need to keep on updating the rules as updated everytime if there is a change . so it is going to be to a continuous process to update the rules. \",\n",
    "    \"Purpose: to enhance the performance of of machines in translation and handling neo pronouns. The author also want to discuss about the challenges that he faced through this process of machine translation. Contribution: The author explained how machine translation happens and how it works with binary coding in machine. Author also highlighting the problems in the process of identifying and translation of neo-pronouns. Method: They use this dataset to evaluate the performance of five commercial MT systems (Google Translate, DeepL, Amazon Translate, Microsoft Translator, and SYSTRAN) in translating these sentences into various target languages. They analyze the outputs and assess the accuracy and inclusiveness of the translations. Dataset: The authors created a dataset of sentences containing (neo-)pronouns in multiple languages, including English, German, Spanish, and French. Conclusion: The author used multiple platform to transalate pronouns and its use but the results shows that every time , there is a difference in usage of pronouns and some machines were good at using those but some were not upto the mark. By the conclusion , we still need to train the machines to get the best results from those models while handling pronouns. Future work: The author suggest that the handling of pronouns is difficults and sill need to train models for accurate and best results.\",\n",
    "    \"Purpose: to find out the usage of Event argument extraction(EAE) and to find out the performance of this EAE system by using different kinds of datasets. Contribution: The paper contribute towards finding and enhancing the techniques regarding Auxilary prompt ensemble(APE) and EAE. For which author used multiple datasets and so that performance can be compared and find insights from the results. Method: This paper includes APE model which has 3 components to work on. They are prompt encoder and task encoder and fusion module.  Dataset: The author used multiple datasets to train the model as like all the datasets are transferred to one dataset by changing the prefixes and then trained the model for each dataset which makes the model learn overlap knowledge. Conclusion: The results of the finding s shows that the APE model  can learn and provide accurate results when it has abundant data to gain knowledge from , which results in great results for the targeted dataset or for experiments. Future work: They also propose extending the APE model to other natural language processing tasks that can benefit from transfer learning and leveraging overlap knowledge across datasets.\",\n",
    "    \"Purpose: to create a soft-prompt-based approach for automatically controlled text generation. It also aims to resolve issues around this process and also existing methods to generate text and maintain attribute consistency along with it. Contribution: The contribution is the introduction of Soft-prompt-based framework for attributes to generate automatic controlled text. The authors also contribute to find out new issues in the process of generating text through these models. Method: The method used by the author is tailor framework in which there are 2 main components. They are prompt encoder and generation model. The soft prompt is integrate with input data or text and then into generation model where it generates text based on inputs. Dataset: author conducted experiments on the widely used benchmark dataset YELP (Lample et al., 2019). It contains multiple single-attribute data that can verify Tailor’s performance on both singleattribute and multi-attribute CTG, while ensuring that the combination of these attributes is reasonable. Conclusion: After overall testing , author identifies that tailor framework performed so well in generation text with the prompts and it also generated high quality text as result. Future work: They also propose investigating more advanced prompt engineering techniques and exploring the potential of Tailor for other natural language generation tasks that require fine-grained control over the output. \",\n",
    "    \"Purpose: to find out the extent to which large language models (LLMs) have acquired knowledge of cultural moral norms, and how this knowledge is used and utilized by the models. Contribution: The main contribution of the paper is to represent and utilization of moral norms. Also it provides insights on how LLMs can be trained by the text data , how to evaluate the results coming from the LLMS. Method: The authors developed the Moral Norm Prediction Task (MNPT), which  predicts the moral norm associated with a given situation or action based on the cultural context. They evaluate several LLMs, including GPT-3, PaLM, and BERT, on the MNPT using a dataset of moral norms from various cultures.  Dataset: The authors used 2kinds of datasets for the testing, world values survey which is primary dataset and PEW 2013 global attitude survey. Conclusion: The authors find that LLMs exhibit a significant understanding of cultural moral norms, with their performance on the MNPT being above chance level. However, the models still struggle with accurately predicting moral norms in certain contexts, particularly for cultures underrepresented in their training data. Future work: The authors suggest several directions for future research, including developing more comprehensive and diverse datasets for evaluating moral knowledge in LLMs, exploring techniques to mitigate biases and improve cross-cultural understanding.\",\n",
    "    \"Purpose: to develop a neural machine translation for song lyrics which produce translations or text. Also author tried to explain the challenges and its possible solutions for each issue. Contribution: The authors propose techniques for controlling rhyme, meter, and language style during translation, enabling the generation of translation and stylistically consistent lyric translations. They also contribute a new dataset of parallel lyrics for evaluation and benchmarking. Method: The author used to create new method which is to translate lyrics into text and it has base machine translation model, a rhyme control module, a meter control module, and a style control module. The base model is trained on parallel lyric data, while the control modules are to guide translation. Dataset: The authors create a new dataset called LyriCorpus, which consists of parallel lyrics in multiple language pairs, including English-Spanish, English-French, and English-German. Conclusion: The results demonstrate the importance of incorporating controllable attributes in lyric translation to preserve the essence and artistry of songs. Future work: They also propose investigating techniques for adapting lyric translations to different musical genres and styles, as well as extending the framework to support other language pairs and cultural contexts.\",\n",
    "    \"Purpose: to generate or create a new method to generate high quality Chinese smiles. These smiles should have meaningful sentiment for each and these should also be trend setting too. Contribution: The author tried to create the smiles in which they are used in large language models to identify the expressions or sentiment of users who used it. Method: The author used GPT-2 to create smiles and then these smiles has been enhanced by using different parameters. Dataset: The authors created a dataset of Chinese similes, consisting of over 200,000 smile instances collected from various online sources. Conclusion: In the conclusion, author has been successful in creating smiles by having specific sentiment for each smile emoji. Future work: The author has been successful in creating the smile emojis and also expecting to enhance these smile emojis with more options like specific smiles.\",\n",
    "    \"Purpose: to check the single frame bias in video and language model at which people are most probably used single frame and multiple frames in video recording. Contribution: The single frame bias network model proposed by the author worked well and its contribute to  the critical sampling strategies and also for existing models. Method: Single Frame Bias Probing (SFBP) framework, which involves creating video-and-language datasets with controlled variations in video sequences and corresponding text. Dataset: The authors use multiple existing video-and-language datasets, including MSRVTT to evaluate single frame bias. Conclusion: The proposed critical sampling strategy during training of single frame bias, leading to improved performance on different tasks that require understanding the entire video sequence. Future work: The author expecting to use different kind of datasets again and try with them, also he expecting different results with different datasets.\",\n",
    "    \"Purpose: to develop a method to resolve issues in dealing with the partially annotated data. Contribution: It helps in finding out ways to learn event detection in partially annotated data in which both labelled and unlabeled data. Method: The unsupervised loss encourages the model to learn from the unlabeled instances by enforcing consistency between the predictions and the model's attention scores. Dataset: The authors evaluate PANA on two event detection datasets: the ACE 2005 dataset and the DEFT-Rich ERE dataset. Conclusion: PANA effectively leverages the unlabeled instances in the partially annotated data, improving event detection performance and reducing the need for expensive fully annotated data. Future work: The author propose investigating more advanced attention mechanisms and exploring the integration of external knowledge sources to further improve the performance of PANA.\",\n",
    "    \n",
    "    \"Purpose: Here Authors investigate grounded word acquisition in vision language models and propose a model for fast mapping new words. Contribution: The authors introduce a Grounded Open Vocabulary Acquisition (GOVA) framework and propose World-to-Words (W2W) model for grounded word learning. Method: We develop World-to-Words (W2W) model with grounding supervision trained on image-text pairs. Unlike many existing Vision-Language Models (VLMs), W2W performs language modeling upon explicit object representations. The model first acquires the ability to ground during pretraining, and then transfers this intrinsic ability to learn unseen words when grounded supervision is no longer available. The model is pretrained on image-text pairs, emphasizing grounding as an objective. Dataset: No dataset is used, but image-text pairs with grounding supervision are used. Conclusion: W2W with grounding supervision outperforms models without grounding supervision on word prediction and object localization. W2W exhibits word-agnostic grounding. From W2W we can learn new words through few-shot learning. The model's performance can be predicted by various factors related to word properties and co-occurrence patterns. Future work: Authors develop language learning agents that acquire words through physical interactions. By this we extend the setting from images to videos and physical interactions with the environment and to Investigate grounded word acquisition from natural dialogue. \",\n",
    "    \"Purpose: To develop a framework to assess how well large language models (LLMs) reason mathematically using word problems and to analyze the robustness of LLMs in solving these problems (avoiding reliance on superficial patterns). Contribution: It introduces a novel causal framework for evaluating LLM performance in math word problems and this framework separates the influence of different factors (wording, numbers, operations) on the model's solution. It applies the framework to showcase its effectiveness and identify a particularly robust model (GPT-3 Davinci). Method: we use is the concept of the casual NLP, which explores causal relationships in natural language processing. This framework analyzes how changes in a word problem's wording, numbers, or operations causally affect the model's answer. This allows researchers to pinpoint which factors the model relies on most heavily and is then applied to test LLMs on a collection of math word problems. Dataset: Here we use the existing Math Word Problem datasets. Conclusion: Here in the paper, size of the LLM model doesn't directly correlate with robustness in math reasoning. The GPT-3 Davinci model (175B parameters) demonstrates a significant improvement in both robustness and sensitivity compared to other models tested. We use instruction tuning, a technique to improve model performance on a specific task, may not significantly improve overall performance. Future work: To extend the framework's applicability to languages other than English. It paves the way for future research on improving LLM performance in mathematical reasoning tasks. \"    \"Purpose: Here the authors aimed to develop a new method for automatically evaluating open-domain dialogue systems. Existing metrics often struggle with responses that are semantically different but still relevant to the conversation. Contribution: The paper proposes CMN, a novel method that leverages Conditional Variational Autoencoders (CVAEs) with Next Sentence Prediction (NSP) and Mutual Information (MI) for dialogue system evaluation. This approach addresses the limitations of current metrics by focusing on semantic coherence and relevance between the context and response. Method: Authors focused on are evaluate CMN on two dialogue datasets and they compare CMN to various baseline metrics used for dialogue evaluation, encompassing reference-based, embedding-based, and learning-based approaches. They also assess how CMN handles responses with varying semantic similarity, two evaluation sets are created: standard (similar responses) and diverse (semantically distant responses).  Dataset: Authors used two dialogue datasets , those are PersonaChat Dataset and DailyDialog DatasetConclusion: In this paper they found that CMN outperforms all baseline metrics in terms of correlation with human annotations on both standard and diverse evaluation sets. This signifies CMN's effectiveness in evaluating responses with varying degrees of semantic similarity to the reference response. CMN utilizes CVAEs with NSP and MI to capture semantic coherence and relevance, demonstrating superior performance compared to existing metrics, especially for responses with varying semantic similarity. The paper highlights the limitation of CMN focusing only on large-scale semantic dependencies. Future work: It will involve 'disentanglement studies' to analyze text based on various attributes considered in human evaluation (e.g., sentiment, humor). This could improve model performance and interpretability. \",\n",
    "    \"Purpose: Here the authors aimed to develop a new method for automatically evaluating open-domain dialogue systems. Existing metrics often struggle with responses that are semantically different but still relevant to the conversation. Contribution: The paper proposes CMN, a novel method that leverages Conditional Variational Autoencoders (CVAEs) with Next Sentence Prediction (NSP) and Mutual Information (MI) for dialogue system evaluation. This approach addresses the limitations of current metrics by focusing on semantic coherence and relevance between the context and response. Method: Authors focused on are evaluate CMN on two dialogue datasets and they compare CMN to various baseline metrics used for dialogue evaluation, encompassing reference-based, embedding-based, and learning-based approaches. They also assess how CMN handles responses with varying semantic similarity, two evaluation sets are created: standard (similar responses) and diverse (semantically distant responses).  Dataset: Authors used two dialogue datasets, those are PersonaChat Dataset and DailyDialog DatasetConclusion: In this paper they found that CMN outperforms all baseline metrics in terms of correlation with human annotations on both standard and diverse evaluation sets. This signifies CMN's effectiveness in evaluating responses with varying degrees of semantic similarity to the reference response. CMN utilizes CVAEs with NSP and MI to capture semantic coherence and relevance, demonstrating superior performance compared to existing metrics, especially for responses with varying semantic similarity. The paper highlights the limitation of CMN focusing only on large-scale semantic dependencies. Future work: It will involve 'disentanglement studies' to analyze text based on various attributes considered in human evaluation (e.g., sentiment, humor). This could improve model performance and interpretability. \"    \"Purpose: to improve the machine translation (MT) by evaluating how well models handle discourse phenomena that rely on context across sentences. Contribution: The main contribution is MuDA, where it evaluates MT on its ability to consider text during translation. It identifies and assesses various phenomena in translations.Method: The authors propose a method called PCXMI, a metric to measure how much context affects a model's translation choices. They then use PCXMI to develop MuDA, a data-driven approach that analyzes parallel translated documents across 14 languages. MuDA automatically tags words requiring context for accurate translation.Dataset: There is no dataset used, instead the authors use parallel translated documents from TED Talks, encompassing 14 languages. Conclusion: From this paper we found that MT models aren’t good at handling phenomena identified by MuDA. We need some improvement in MT. Future work: In future use, they call for further development of MT models specifically focused on improving context-aware translation. MuDA paves the way for more targeted evaluation and future advancements in MT.\",\n",
    "    \"Purpose: To develop methods that leverage Large Language Models (LLMs) and human effort to generate text classification datasets with high accuracy and diversity. Contribution: It introduces two text diversification approaches using LLMs and analyzes their impact on model accuracy and evaluates two human intervention methods to improve the accuracy of models trained on diversified datasets. It also identifies limitations in the current approach and proposes future work directions. Method: Authors implemented two text diversification techniques (logit suppression & high temperature) to increase the variety of generated text. To improve the accuracy of the generated data, human intervention involves replacing incorrect labels with more suitable ones and filtering out-of-scope instances (OOSF) that don't belong to the desired category. Dataset: In this no data set is used, as most of the input data is Text Data. Conclusion: The findings in the paper are diversification techniques successfully generate more diverse text data, but this comes at the cost of reduced accuracy in assigning labels.Future work: In future, we will develop methods for training proxy models throughout the LLM text generation process to address data imbalance. We also design better OOSF strategies like filtering based on various criteria and testing the approach with a wider range of LLM models and prompts. \",\n",
    "    \"Purpose: The main purpose of the paper is to develop a more efficient method for pretraining large language models (PLMs) by reducing their size and computational cost. Contribution: Here the main contribution of the paper is introducing a Static Model Pruning (SMP), a novel method for pruning PLMs that doesn't require fine-tuning after pruning and proposes a new masking function for selecting important weights to retain during pruning. By this it also demonstrates that SMP achieves better accuracy than previous methods, especially at high sparsity levels (when many weights are removed). Method: Here the paper proposes a method called SMP, which works by freezing weights of a pre-trained PLM by using movement pruning (analyzing how weights change during training) to identify unimportant weights for removal. It also implements a new masking function to improve the selection of weights to keep and by freezing the task-specific head (part of the PLM adapted for a specific task) and initializing it based on relevant labels.Dataset:  Here also no dataset is used as the input is Text input.Conclusion:  From this paper we can say that SMP outperforms prior pruning methods in terms of accuracy, especially when a high percentage of weights are removed from the PLM and this method allows for significant reductions in the number of parameters required for training, making PLMs faster and more efficient to use for various tasks.Future work: The future work of the paper is Investigating the impact of SMP on various natural language processing tasks beyond those studied in the paper and developing techniques to further improve the efficiency and accuracy of the SMP method.\",\n",
    "    \"Purpose: The main purpose of the paper is to improve the machine translation (MT) by evaluating how well models handle discourse phenomena that rely on context across sentences. Contribution: The main contribution is MuDA, where it evaluates MT on its ability to consider text during translation. It identifies and assesses various phenomena in translations.Method: The authors propose a method called PCXMI, a metric to measure how much context affects a model's translation choices. They then use PCXMI to develop MuDA, a data-driven approach that analyzes parallel translated documents across 14 languages. MuDA automatically tags words requiring context for accurate translation.Dataset: There is no dataset used, instead the authors use parallel translated documents from TED Talks, encompassing 14 languages. Conclusion: From this paper we found that MT models aren’t good at handling phenomena identified by MuDA. We need some improvement in MT. Future work: In future use, they call for further development of MT models specifically focused on improving context-aware translation. MuDA paves the way for more targeted evaluation and future advancements in MT.\",\n",
    "    \"Purpose: The purpose of the authors is to improve fake news detection models by mitigating bias in the training data.Contribution:  The contribution of the paper is a novel framework called Causal Counterfactual Debiasing (CCD) that addresses two types of bias: psycholinguistic bias and image-only bias. Psycholinguistic bias refers to the tendency of fake news articles to use emotionally charged language, which can mislead training models. Image-only bias occurs when a model assigns a news label based solely on the image content, even if the image itself is unrelated to the news article.Method:  The method involves causal intervention to remove the effect of emotional language in text features during training, and counterfactual reasoning to estimate the effect of the image alone on the label by imagining a scenario where the model only sees the image and not the text.Dataset:  The authors used two benchmark datasets: Twitter and Pheme, which contain text and image data labelled as real or fake news. Conclusion:  The findings of the paper are that the CCD framework can significantly improve the accuracy of fake news detection models compared to baseline models, and that both causal intervention and counterfactual reasoning contribute to the overall effectiveness.Future work: Future work mentioned in the paper involves improving the confounder dictionary used for causal intervention by leveraging external knowledge.\",\n",
    "    \"Purpose: The purpose of the paper is to develop LEXSYM, a data augmentation method for neural networks to improve compositional generalization. Contribution: Here the LEXSYM promotes compositional generalization by leveraging symmetries in data distributions. LEXSYM leverages symmetries in the data to generate informative training examples, improving the model's ability to learn from small datasets and perform well on unseen examples. Lexicon Flexibility: Handles simple or complex relationships between words, inferred automatically from data. Learned Lexicon Tolerance: Functions well even with noisy or unclear token mappings. Method: The methods used in the paper are LEXSYM involves: 1. Lexicon creation (manual/automatic) defining word relationships. 2. Symmetry identification based on the lexicon.  3.Data augmentation using the symmetries to generate new training examples.Dataset: The paper mentions that it uses various datasets across domains like semantic parsing, instruction following, and visual question answering like COGS dataset, COGENT dataset, SCAN dataset, ALCHEMY Dataset and CLEVR datasetConclusion: From this paper we found that LEXSYM improves neural network performance on tasks requiring compositional reasoning, especially for unseen examples. LEXSYM is model-agnostic, lexicon-flexible, and handles noisy learned lexicons.Future work: The future work mentioned in the paper is to extend LEXSYM beyond single-token swaps to handle larger structures and incorporate recursion modeling for broader human-like generalizations. It also adapts LEXSYM to different tokenization schemes (e.g., morphological analyzers) and by exploring LEXSYM as a foundation for future generalization methods.\",\n",
    "    \"Purpose: The purpose of the paper is i.e.., Multimodal emotion recognition it is a task of automatically detecting emotions from human according to different modalities, such as text, audio, and vision.Contribution: Existing datasets mostly annotated the samples with the same labels for all modalities, while authors argue that separate annotations for each modality can improve the performance. The authors proposed a new dataset named CHERMA for multimodal emotion recognition. CHERMA is annotated with separate labels for each modality (text, audio, and vision) and a joint label for all modalities. The statistics of the dataset, including the number of samples, distribution of emotions, and gender/age information are presented. Authors also explore the inconsistency between labels from different modalities. Method: The proposed model, LFMIM, is a transformer-based model with two main components: three unimodal transformers for processing individual modalities (text, audio, and vision) and one multimodal transformer for fusing information from all modalities. The unimodal transformers are trained with unimodal labels, while the multimodal transformer is trained with multimodal labels. The information flow in the model is unidirectional, from unimodal transformers to the multimodal transformer. Dataset: Authors created a new dataset called CHERMA dataset. Conclusion: Further analysis shows that LFMIM's design choices (unimodal labels, unidirectional information flow) contribute to its success. Limitations include using the same settings for all modalities and lacking theoretical foundation for the balance between modality dependence and independence. Future work: Future work aims to find this optimal balance. Overall, the paper advocates for modality independence in MER and demonstrates its effectiveness through LFMIM.\",\n",
    "    \"Purpose: The authors of this paper aim to improve the detection of adversarial samples in text data, which can trick AI models. Contribution: Their contribution is a new method called CASN (Class-Aware Score Network) that outperforms existing density-based approaches. Method: CASN estimates the difference between normal and adversarial text by analyzing how the data representation changes during a special denoising process. Dataset: The researchers test CASN on three datasets commonly used for sentiment analysis and topic classification. Those three datasets are SST-2 dataset, IMDB dataset and AGNEWS dataset. Conclusion: CASN achieves impressive results, nearly perfectly detecting adversarial samples in some cases, while also maintaining the accuracy of normal text data. However, CASN's denoising process can be slow, and the model may not work well across completely different datasets. Future work: The future work could involve improving CASN's efficiency and generalizability. \"\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# Create a DataFrame from the list of texts with 'Manual_Insights' column\n",
    "Manualdf = pd.DataFrame({'Manual_Insights': texts})\n",
    "\n",
    "# Display the DataFrame\n",
    "Manualdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4df919de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manual_Insights</th>\n",
       "      <th>Purpose</th>\n",
       "      <th>Contribution</th>\n",
       "      <th>Method</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Future work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purpose: to improve dialogue systems by levera...</td>\n",
       "      <td>to improve dialogue systems by leveraging mult...</td>\n",
       "      <td>a new framework called MUST (Multi-user Simula...</td>\n",
       "      <td>MUST (Multi-user Simulator Training). MUST add...</td>\n",
       "      <td>didn't find information may have used personal...</td>\n",
       "      <td>that MUST leads to a more robust dialogue syst...</td>\n",
       "      <td>we can do advances to the model used in this p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Purpose: the prevalence of unsafe behavior, su...</td>\n",
       "      <td>the prevalence of unsafe behavior, such as tox...</td>\n",
       "      <td>The authors constructed a new dataset called S...</td>\n",
       "      <td>The authors benchmarked three powerful models ...</td>\n",
       "      <td>The dataset used in this paper is SafeConv, wh...</td>\n",
       "      <td>The experiments showed that the detected unsaf...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Purpose: the problem of hallucinations in neur...</td>\n",
       "      <td>the problem of hallucinations in neural machin...</td>\n",
       "      <td>The authors propose a method that evaluates th...</td>\n",
       "      <td>The authors use a method that evaluates the pe...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>The method proposed by the authors improves de...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Purpose: the issue of generating accurate and ...</td>\n",
       "      <td>the issue of generating accurate and informati...</td>\n",
       "      <td>The authors propose a novel model, ERRA (Expla...</td>\n",
       "      <td>The authors use a method that combines persona...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>The experiments show that their model outperfo...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Purpose: the difficulties of optimizing binary...</td>\n",
       "      <td>the difficulties of optimizing binary and tern...</td>\n",
       "      <td>The authors demonstrate the first ternary and ...</td>\n",
       "      <td>The authors approach the problem with a mix of...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>The authors’ ternary BART base achieves an R1 ...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Purpose: the limitations of prior generative a...</td>\n",
       "      <td>the limitations of prior generative and discri...</td>\n",
       "      <td>The authors introduce SPLAT, a novel architect...</td>\n",
       "      <td>The authors use a method that evaluates the pe...</td>\n",
       "      <td>The authors demonstrate the effectiveness of t...</td>\n",
       "      <td>The authors’ approach significantly improves u...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Purpose: the gap for multi-party dialogues in ...</td>\n",
       "      <td>the gap for multi-party dialogues in dialogue ...</td>\n",
       "      <td>The authors propose an Expectation-Maximizatio...</td>\n",
       "      <td>The authors use an Expectation-Maximization (E...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>Theoretical analyses and extensive experiments...</td>\n",
       "      <td>the findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Purpose: the task of detecting linguistically ...</td>\n",
       "      <td>the task of detecting linguistically complex n...</td>\n",
       "      <td>The authors present ACLM (Attention-map aware ...</td>\n",
       "      <td>ACLM builds on BART and is optimized on a nove...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>The authors demonstrate the effectiveness of A...</td>\n",
       "      <td>The findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purpose: the performance of AI pair programmer...</td>\n",
       "      <td>the performance of AI pair programmers that au...</td>\n",
       "      <td>The authors build ARCADE, a benchmark of 1,078...</td>\n",
       "      <td>To establish a strong baseline on this challen...</td>\n",
       "      <td>The dataset used in this paper is ARCADE, whic...</td>\n",
       "      <td>The authors explore few-shot prompting strateg...</td>\n",
       "      <td>The findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Purpose: the time-consuming decoding in k-near...</td>\n",
       "      <td>the time-consuming decoding in k-nearest-neigh...</td>\n",
       "      <td>The authors propose “Subset kNN-MT”, which imp...</td>\n",
       "      <td>The authors use a method that retrieves neighb...</td>\n",
       "      <td>The paper does not explicitly mention the data...</td>\n",
       "      <td>The authors’ proposed method achieved a speed-...</td>\n",
       "      <td>The findings suggest that there is potential f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Purpose: They provide MILDecoding, a method th...</td>\n",
       "      <td>They provide MILDecoding, a method that uses a...</td>\n",
       "      <td>The paper's unique approach to solving the wid...</td>\n",
       "      <td>The core idea of MIL-Decoding is to enhance th...</td>\n",
       "      <td>This paper conduct experiments on two datasets...</td>\n",
       "      <td>They have introduced MIL-Decoding, which can d...</td>\n",
       "      <td>In future week, they will explore how to balan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Purpose: Using psycholinguistic and computatio...</td>\n",
       "      <td>Using psycholinguistic and computational exper...</td>\n",
       "      <td>(i) the release of wide-covering and highly co...</td>\n",
       "      <td>This paper's methodological approach combines ...</td>\n",
       "      <td>They have created several datasets that have b...</td>\n",
       "      <td>The empirical evidence gathered in this work p...</td>\n",
       "      <td>The results of this article may be expanded up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Purpose: BART and GPTs are two examples of pre...</td>\n",
       "      <td>BART and GPTs are two examples of pre-trained ...</td>\n",
       "      <td>The limits of autoregressive (AR) language mod...</td>\n",
       "      <td>They propose two simple yet effective strategi...</td>\n",
       "      <td>Experiments on two OpenLTG tasks (i.e., storyt...</td>\n",
       "      <td>This work investigates Open-LTG using NAR mode...</td>\n",
       "      <td>Paper’s primary concern focuses on acceleratin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Purpose: Using extensive language models, they...</td>\n",
       "      <td>Using extensive language models, they investig...</td>\n",
       "      <td>The study uses extensive language models and i...</td>\n",
       "      <td>Learning to map the contextual word embedding ...</td>\n",
       "      <td>They have constructed three semantic spaces, t...</td>\n",
       "      <td>In this paper, they honed techniques for predi...</td>\n",
       "      <td>In this area could concentrate on various dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Purpose: They suggest a way to express CCG in ...</td>\n",
       "      <td>They suggest a way to express CCG in a continu...</td>\n",
       "      <td>The study presents an innovative approach for ...</td>\n",
       "      <td>They use recursive composition operations to c...</td>\n",
       "      <td>Hockenmaier and Steedman (2007) used a standar...</td>\n",
       "      <td>In this paper, they proposed a new way to buil...</td>\n",
       "      <td>Subsequent research endeavors may delve deeper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Purpose: In order to assess the UIE system's c...</td>\n",
       "      <td>In order to assess the UIE system's capabiliti...</td>\n",
       "      <td>introduces a challenging benchmark for univers...</td>\n",
       "      <td>Figure 2 depicts the general LPT procedure. Th...</td>\n",
       "      <td>They formalize the lifelong UIE benchmark by u...</td>\n",
       "      <td>They examine a lifelong learning paradigm for ...</td>\n",
       "      <td>Subsequent research endeavors may delve deeper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Purpose: Retrieval augmentation has been prove...</td>\n",
       "      <td>Retrieval augmentation has been proven to be e...</td>\n",
       "      <td>It, greatly advances the field of natural lang...</td>\n",
       "      <td>They use the same formulation of document leve...</td>\n",
       "      <td>They use RAMS and WikiEvents, two popular docu...</td>\n",
       "      <td>In this paper, they investigate the input and ...</td>\n",
       "      <td>In future, they plan to adapt their method to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Purpose: The fact that many existing text gene...</td>\n",
       "      <td>The fact that many existing text generation mo...</td>\n",
       "      <td>The important problem of factual inconsistency...</td>\n",
       "      <td>In addition to increasing the accuracy of fact...</td>\n",
       "      <td>A benchmark consisting of 11 datasets of 4 tas...</td>\n",
       "      <td>In this paper, they propose a weakly supervise...</td>\n",
       "      <td>Subsequent research endeavours may centre on e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Purpose: One task for fine-grained sentiment c...</td>\n",
       "      <td>One task for fine-grained sentiment classifica...</td>\n",
       "      <td>They modify the ABSA task to include Abstract ...</td>\n",
       "      <td>The approach put forth in this paper makes a s...</td>\n",
       "      <td>They use four widely used public standard data...</td>\n",
       "      <td>They present the AMR-based Path Aggregation Re...</td>\n",
       "      <td>In the future, APARN with higher quality AMRs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Purpose: They present a brand-new adversarial ...</td>\n",
       "      <td>They present a brand-new adversarial purificat...</td>\n",
       "      <td>They bring up the issue of fending off adversa...</td>\n",
       "      <td>The work makes a substantial contribution to t...</td>\n",
       "      <td>In their experiments, they make use of two pop...</td>\n",
       "      <td>To counter substitution-based adversarial atta...</td>\n",
       "      <td>They are the first to consider the adversarial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Purpose: to introduce a new approach to deal w...</td>\n",
       "      <td>to introduce a new approach to deal with NLP i...</td>\n",
       "      <td>taking the new approach or framework of SPEECH...</td>\n",
       "      <td>The author used the method or framework of SPE...</td>\n",
       "      <td>Author has used different kinds of datasets fo...</td>\n",
       "      <td>The author finds that the new approach of SPEE...</td>\n",
       "      <td>To take the advancements in this approach of S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Purpose: to develop a model for hate speech de...</td>\n",
       "      <td>to develop a model for hate speech detection b...</td>\n",
       "      <td>This paper supports the Rule based model calle...</td>\n",
       "      <td>The author used the Rule By Example framework ...</td>\n",
       "      <td>The author has used 3 datasets for the analysi...</td>\n",
       "      <td>Author finds novel dual-encoder model architec...</td>\n",
       "      <td>This rule based example model has only worked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Purpose: to enhance the performance of of mach...</td>\n",
       "      <td>to enhance the performance of of machines in t...</td>\n",
       "      <td>The author explained how machine translation h...</td>\n",
       "      <td>They use this dataset to evaluate the performa...</td>\n",
       "      <td>The authors created a dataset of sentences con...</td>\n",
       "      <td>The author used multiple platform to transalat...</td>\n",
       "      <td>The author suggest that the handling of pronou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Purpose: to find out the usage of Event argume...</td>\n",
       "      <td>to find out the usage of Event argument extrac...</td>\n",
       "      <td>The paper contribute towards finding and enhan...</td>\n",
       "      <td>This paper includes APE model which has 3 comp...</td>\n",
       "      <td>The author used multiple datasets to train the...</td>\n",
       "      <td>The results of the finding s shows that the AP...</td>\n",
       "      <td>They also propose extending the APE model to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Purpose: to create a soft-prompt-based approac...</td>\n",
       "      <td>to create a soft-prompt-based approach for aut...</td>\n",
       "      <td>The contribution is the introduction of Soft-p...</td>\n",
       "      <td>The method used by the author is tailor framew...</td>\n",
       "      <td>author conducted experiments on the widely use...</td>\n",
       "      <td>After overall testing , author identifies that...</td>\n",
       "      <td>They also propose investigating more advanced ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Purpose: to find out the extent to which large...</td>\n",
       "      <td>to find out the extent to which large language...</td>\n",
       "      <td>The main contribution of the paper is to repre...</td>\n",
       "      <td>The authors developed the Moral Norm Predictio...</td>\n",
       "      <td>The authors used 2kinds of datasets for the te...</td>\n",
       "      <td>The authors find that LLMs exhibit a significa...</td>\n",
       "      <td>The authors suggest several directions for fut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Purpose: to develop a neural machine translati...</td>\n",
       "      <td>to develop a neural machine translation for so...</td>\n",
       "      <td>The authors propose techniques for controlling...</td>\n",
       "      <td>The author used to create new method which is ...</td>\n",
       "      <td>The authors create a new dataset called LyriCo...</td>\n",
       "      <td>The results demonstrate the importance of inco...</td>\n",
       "      <td>They also propose investigating techniques for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Purpose: to generate or create a new method to...</td>\n",
       "      <td>to generate or create a new method to generate...</td>\n",
       "      <td>The author tried to create the smiles in which...</td>\n",
       "      <td>The author used GPT-2 to create smiles and the...</td>\n",
       "      <td>The authors created a dataset of Chinese simil...</td>\n",
       "      <td>In the conclusion, author has been successful ...</td>\n",
       "      <td>The author has been successful in creating the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Purpose: to check the single frame bias in vid...</td>\n",
       "      <td>to check the single frame bias in video and la...</td>\n",
       "      <td>The single frame bias network model proposed b...</td>\n",
       "      <td>Single Frame Bias Probing (SFBP) framework, wh...</td>\n",
       "      <td>The authors use multiple existing video-and-la...</td>\n",
       "      <td>The proposed critical sampling strategy during...</td>\n",
       "      <td>The author expecting to use different kind of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Purpose: to develop a method to resolve issues...</td>\n",
       "      <td>to develop a method to resolve issues in deali...</td>\n",
       "      <td>It helps in finding out ways to learn event de...</td>\n",
       "      <td>The unsupervised loss encourages the model to ...</td>\n",
       "      <td>The authors evaluate PANA on two event detecti...</td>\n",
       "      <td>PANA effectively leverages the unlabeled insta...</td>\n",
       "      <td>The author propose investigating more advanced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Purpose: Here Authors investigate grounded wor...</td>\n",
       "      <td>Here Authors investigate grounded word acquisi...</td>\n",
       "      <td>The authors introduce a Grounded Open Vocabula...</td>\n",
       "      <td>We develop World-to-Words (W2W) model with gro...</td>\n",
       "      <td>No dataset is used, but image-text pairs with ...</td>\n",
       "      <td>W2W with grounding supervision outperforms mod...</td>\n",
       "      <td>Authors develop language learning agents that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Purpose: To develop a framework to assess how ...</td>\n",
       "      <td>To develop a framework to assess how well larg...</td>\n",
       "      <td>It introduces a novel causal framework for eva...</td>\n",
       "      <td>we use is the concept of the casual NLP, which...</td>\n",
       "      <td>Here we use the existing Math Word Problem dat...</td>\n",
       "      <td>Here in the paper, size of the LLM model doesn...</td>\n",
       "      <td>To extend the framework's applicability to lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Purpose: Here the authors aimed to develop a n...</td>\n",
       "      <td>Here the authors aimed to develop a new method...</td>\n",
       "      <td>The paper proposes CMN, a novel method that le...</td>\n",
       "      <td>Authors focused on are evaluate CMN on two dia...</td>\n",
       "      <td>Authors used two dialogue datasets, those are ...</td>\n",
       "      <td>In this paper they found that CMN outperforms ...</td>\n",
       "      <td>It will involve 'disentanglement studies' to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Purpose: To develop methods that leverage Larg...</td>\n",
       "      <td>To develop methods that leverage Large Languag...</td>\n",
       "      <td>It introduces two text diversification approac...</td>\n",
       "      <td>Authors implemented two text diversification t...</td>\n",
       "      <td>In this no data set is used, as most of the in...</td>\n",
       "      <td>The findings in the paper are diversification ...</td>\n",
       "      <td>In future, we will develop methods for trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Purpose: The main purpose of the paper is to d...</td>\n",
       "      <td>The main purpose of the paper is to develop a ...</td>\n",
       "      <td>Here the main contribution of the paper is int...</td>\n",
       "      <td>Here the paper proposes a method called SMP, w...</td>\n",
       "      <td>Here also no dataset is used as the input is T...</td>\n",
       "      <td>From this paper we can say that SMP outperform...</td>\n",
       "      <td>The future work of the paper is Investigating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Purpose: The main purpose of the paper is to i...</td>\n",
       "      <td>The main purpose of the paper is to improve th...</td>\n",
       "      <td>The main contribution is MuDA, where it evalua...</td>\n",
       "      <td>The authors propose a method called PCXMI, a m...</td>\n",
       "      <td>There is no dataset used, instead the authors ...</td>\n",
       "      <td>From this paper we found that MT models aren’t...</td>\n",
       "      <td>In future use, they call for further developme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Purpose: The purpose of the authors is to impr...</td>\n",
       "      <td>The purpose of the authors is to improve fake ...</td>\n",
       "      <td>The contribution of the paper is a novel frame...</td>\n",
       "      <td>The method involves causal intervention to rem...</td>\n",
       "      <td>The authors used two benchmark datasets: Twitt...</td>\n",
       "      <td>The findings of the paper are that the CCD fra...</td>\n",
       "      <td>Future work mentioned in the paper involves im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Purpose: The purpose of the paper is to develo...</td>\n",
       "      <td>The purpose of the paper is to develop LEXSYM,...</td>\n",
       "      <td>Here the LEXSYM promotes compositional general...</td>\n",
       "      <td>The methods used in the paper are LEXSYM invol...</td>\n",
       "      <td>The paper mentions that it uses various datase...</td>\n",
       "      <td>From this paper we found that LEXSYM improves ...</td>\n",
       "      <td>The future work mentioned in the paper is to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Purpose: The purpose of the paper is i.e.., Mu...</td>\n",
       "      <td>The purpose of the paper is i.e.., Multimodal ...</td>\n",
       "      <td>Existing datasets mostly annotated the samples...</td>\n",
       "      <td>The proposed model, LFMIM, is a transformer-ba...</td>\n",
       "      <td>Authors created a new dataset called CHERMA da...</td>\n",
       "      <td>Further analysis shows that LFMIM's design cho...</td>\n",
       "      <td>Future work aims to find this optimal balance....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Purpose: The authors of this paper aim to impr...</td>\n",
       "      <td>The authors of this paper aim to improve the d...</td>\n",
       "      <td>Their contribution is a new method called CASN...</td>\n",
       "      <td>CASN estimates the difference between normal a...</td>\n",
       "      <td>The researchers test CASN on three datasets co...</td>\n",
       "      <td>CASN achieves impressive results, nearly perfe...</td>\n",
       "      <td>The future work could involve improving CASN's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Manual_Insights  \\\n",
       "0   Purpose: to improve dialogue systems by levera...   \n",
       "1   Purpose: the prevalence of unsafe behavior, su...   \n",
       "2   Purpose: the problem of hallucinations in neur...   \n",
       "3   Purpose: the issue of generating accurate and ...   \n",
       "4   Purpose: the difficulties of optimizing binary...   \n",
       "5   Purpose: the limitations of prior generative a...   \n",
       "6   Purpose: the gap for multi-party dialogues in ...   \n",
       "7   Purpose: the task of detecting linguistically ...   \n",
       "8   Purpose: the performance of AI pair programmer...   \n",
       "9   Purpose: the time-consuming decoding in k-near...   \n",
       "10  Purpose: They provide MILDecoding, a method th...   \n",
       "11  Purpose: Using psycholinguistic and computatio...   \n",
       "12  Purpose: BART and GPTs are two examples of pre...   \n",
       "13  Purpose: Using extensive language models, they...   \n",
       "14  Purpose: They suggest a way to express CCG in ...   \n",
       "15  Purpose: In order to assess the UIE system's c...   \n",
       "16  Purpose: Retrieval augmentation has been prove...   \n",
       "17  Purpose: The fact that many existing text gene...   \n",
       "18  Purpose: One task for fine-grained sentiment c...   \n",
       "19  Purpose: They present a brand-new adversarial ...   \n",
       "20  Purpose: to introduce a new approach to deal w...   \n",
       "21  Purpose: to develop a model for hate speech de...   \n",
       "22  Purpose: to enhance the performance of of mach...   \n",
       "23  Purpose: to find out the usage of Event argume...   \n",
       "24  Purpose: to create a soft-prompt-based approac...   \n",
       "25  Purpose: to find out the extent to which large...   \n",
       "26  Purpose: to develop a neural machine translati...   \n",
       "27  Purpose: to generate or create a new method to...   \n",
       "28  Purpose: to check the single frame bias in vid...   \n",
       "29  Purpose: to develop a method to resolve issues...   \n",
       "30  Purpose: Here Authors investigate grounded wor...   \n",
       "31  Purpose: To develop a framework to assess how ...   \n",
       "32  Purpose: Here the authors aimed to develop a n...   \n",
       "33  Purpose: To develop methods that leverage Larg...   \n",
       "34  Purpose: The main purpose of the paper is to d...   \n",
       "35  Purpose: The main purpose of the paper is to i...   \n",
       "36  Purpose: The purpose of the authors is to impr...   \n",
       "37  Purpose: The purpose of the paper is to develo...   \n",
       "38  Purpose: The purpose of the paper is i.e.., Mu...   \n",
       "39  Purpose: The authors of this paper aim to impr...   \n",
       "\n",
       "                                              Purpose  \\\n",
       "0   to improve dialogue systems by leveraging mult...   \n",
       "1   the prevalence of unsafe behavior, such as tox...   \n",
       "2   the problem of hallucinations in neural machin...   \n",
       "3   the issue of generating accurate and informati...   \n",
       "4   the difficulties of optimizing binary and tern...   \n",
       "5   the limitations of prior generative and discri...   \n",
       "6   the gap for multi-party dialogues in dialogue ...   \n",
       "7   the task of detecting linguistically complex n...   \n",
       "8   the performance of AI pair programmers that au...   \n",
       "9   the time-consuming decoding in k-nearest-neigh...   \n",
       "10  They provide MILDecoding, a method that uses a...   \n",
       "11  Using psycholinguistic and computational exper...   \n",
       "12  BART and GPTs are two examples of pre-trained ...   \n",
       "13  Using extensive language models, they investig...   \n",
       "14  They suggest a way to express CCG in a continu...   \n",
       "15  In order to assess the UIE system's capabiliti...   \n",
       "16  Retrieval augmentation has been proven to be e...   \n",
       "17  The fact that many existing text generation mo...   \n",
       "18  One task for fine-grained sentiment classifica...   \n",
       "19  They present a brand-new adversarial purificat...   \n",
       "20  to introduce a new approach to deal with NLP i...   \n",
       "21  to develop a model for hate speech detection b...   \n",
       "22  to enhance the performance of of machines in t...   \n",
       "23  to find out the usage of Event argument extrac...   \n",
       "24  to create a soft-prompt-based approach for aut...   \n",
       "25  to find out the extent to which large language...   \n",
       "26  to develop a neural machine translation for so...   \n",
       "27  to generate or create a new method to generate...   \n",
       "28  to check the single frame bias in video and la...   \n",
       "29  to develop a method to resolve issues in deali...   \n",
       "30  Here Authors investigate grounded word acquisi...   \n",
       "31  To develop a framework to assess how well larg...   \n",
       "32  Here the authors aimed to develop a new method...   \n",
       "33  To develop methods that leverage Large Languag...   \n",
       "34  The main purpose of the paper is to develop a ...   \n",
       "35  The main purpose of the paper is to improve th...   \n",
       "36  The purpose of the authors is to improve fake ...   \n",
       "37  The purpose of the paper is to develop LEXSYM,...   \n",
       "38  The purpose of the paper is i.e.., Multimodal ...   \n",
       "39  The authors of this paper aim to improve the d...   \n",
       "\n",
       "                                         Contribution  \\\n",
       "0   a new framework called MUST (Multi-user Simula...   \n",
       "1   The authors constructed a new dataset called S...   \n",
       "2   The authors propose a method that evaluates th...   \n",
       "3   The authors propose a novel model, ERRA (Expla...   \n",
       "4   The authors demonstrate the first ternary and ...   \n",
       "5   The authors introduce SPLAT, a novel architect...   \n",
       "6   The authors propose an Expectation-Maximizatio...   \n",
       "7   The authors present ACLM (Attention-map aware ...   \n",
       "8   The authors build ARCADE, a benchmark of 1,078...   \n",
       "9   The authors propose “Subset kNN-MT”, which imp...   \n",
       "10  The paper's unique approach to solving the wid...   \n",
       "11  (i) the release of wide-covering and highly co...   \n",
       "12  The limits of autoregressive (AR) language mod...   \n",
       "13  The study uses extensive language models and i...   \n",
       "14  The study presents an innovative approach for ...   \n",
       "15  introduces a challenging benchmark for univers...   \n",
       "16  It, greatly advances the field of natural lang...   \n",
       "17  The important problem of factual inconsistency...   \n",
       "18  They modify the ABSA task to include Abstract ...   \n",
       "19  They bring up the issue of fending off adversa...   \n",
       "20  taking the new approach or framework of SPEECH...   \n",
       "21  This paper supports the Rule based model calle...   \n",
       "22  The author explained how machine translation h...   \n",
       "23  The paper contribute towards finding and enhan...   \n",
       "24  The contribution is the introduction of Soft-p...   \n",
       "25  The main contribution of the paper is to repre...   \n",
       "26  The authors propose techniques for controlling...   \n",
       "27  The author tried to create the smiles in which...   \n",
       "28  The single frame bias network model proposed b...   \n",
       "29  It helps in finding out ways to learn event de...   \n",
       "30  The authors introduce a Grounded Open Vocabula...   \n",
       "31  It introduces a novel causal framework for eva...   \n",
       "32  The paper proposes CMN, a novel method that le...   \n",
       "33  It introduces two text diversification approac...   \n",
       "34  Here the main contribution of the paper is int...   \n",
       "35  The main contribution is MuDA, where it evalua...   \n",
       "36  The contribution of the paper is a novel frame...   \n",
       "37  Here the LEXSYM promotes compositional general...   \n",
       "38  Existing datasets mostly annotated the samples...   \n",
       "39  Their contribution is a new method called CASN...   \n",
       "\n",
       "                                               Method  \\\n",
       "0   MUST (Multi-user Simulator Training). MUST add...   \n",
       "1   The authors benchmarked three powerful models ...   \n",
       "2   The authors use a method that evaluates the pe...   \n",
       "3   The authors use a method that combines persona...   \n",
       "4   The authors approach the problem with a mix of...   \n",
       "5   The authors use a method that evaluates the pe...   \n",
       "6   The authors use an Expectation-Maximization (E...   \n",
       "7   ACLM builds on BART and is optimized on a nove...   \n",
       "8   To establish a strong baseline on this challen...   \n",
       "9   The authors use a method that retrieves neighb...   \n",
       "10  The core idea of MIL-Decoding is to enhance th...   \n",
       "11  This paper's methodological approach combines ...   \n",
       "12  They propose two simple yet effective strategi...   \n",
       "13  Learning to map the contextual word embedding ...   \n",
       "14  They use recursive composition operations to c...   \n",
       "15  Figure 2 depicts the general LPT procedure. Th...   \n",
       "16  They use the same formulation of document leve...   \n",
       "17  In addition to increasing the accuracy of fact...   \n",
       "18  The approach put forth in this paper makes a s...   \n",
       "19  The work makes a substantial contribution to t...   \n",
       "20  The author used the method or framework of SPE...   \n",
       "21  The author used the Rule By Example framework ...   \n",
       "22  They use this dataset to evaluate the performa...   \n",
       "23  This paper includes APE model which has 3 comp...   \n",
       "24  The method used by the author is tailor framew...   \n",
       "25  The authors developed the Moral Norm Predictio...   \n",
       "26  The author used to create new method which is ...   \n",
       "27  The author used GPT-2 to create smiles and the...   \n",
       "28  Single Frame Bias Probing (SFBP) framework, wh...   \n",
       "29  The unsupervised loss encourages the model to ...   \n",
       "30  We develop World-to-Words (W2W) model with gro...   \n",
       "31  we use is the concept of the casual NLP, which...   \n",
       "32  Authors focused on are evaluate CMN on two dia...   \n",
       "33  Authors implemented two text diversification t...   \n",
       "34  Here the paper proposes a method called SMP, w...   \n",
       "35  The authors propose a method called PCXMI, a m...   \n",
       "36  The method involves causal intervention to rem...   \n",
       "37  The methods used in the paper are LEXSYM invol...   \n",
       "38  The proposed model, LFMIM, is a transformer-ba...   \n",
       "39  CASN estimates the difference between normal a...   \n",
       "\n",
       "                                              Dataset  \\\n",
       "0   didn't find information may have used personal...   \n",
       "1   The dataset used in this paper is SafeConv, wh...   \n",
       "2   The paper does not explicitly mention the data...   \n",
       "3   The paper does not explicitly mention the data...   \n",
       "4   The paper does not explicitly mention the data...   \n",
       "5   The authors demonstrate the effectiveness of t...   \n",
       "6   The paper does not explicitly mention the data...   \n",
       "7   The paper does not explicitly mention the data...   \n",
       "8   The dataset used in this paper is ARCADE, whic...   \n",
       "9   The paper does not explicitly mention the data...   \n",
       "10  This paper conduct experiments on two datasets...   \n",
       "11  They have created several datasets that have b...   \n",
       "12  Experiments on two OpenLTG tasks (i.e., storyt...   \n",
       "13  They have constructed three semantic spaces, t...   \n",
       "14  Hockenmaier and Steedman (2007) used a standar...   \n",
       "15  They formalize the lifelong UIE benchmark by u...   \n",
       "16  They use RAMS and WikiEvents, two popular docu...   \n",
       "17  A benchmark consisting of 11 datasets of 4 tas...   \n",
       "18  They use four widely used public standard data...   \n",
       "19  In their experiments, they make use of two pop...   \n",
       "20  Author has used different kinds of datasets fo...   \n",
       "21  The author has used 3 datasets for the analysi...   \n",
       "22  The authors created a dataset of sentences con...   \n",
       "23  The author used multiple datasets to train the...   \n",
       "24  author conducted experiments on the widely use...   \n",
       "25  The authors used 2kinds of datasets for the te...   \n",
       "26  The authors create a new dataset called LyriCo...   \n",
       "27  The authors created a dataset of Chinese simil...   \n",
       "28  The authors use multiple existing video-and-la...   \n",
       "29  The authors evaluate PANA on two event detecti...   \n",
       "30  No dataset is used, but image-text pairs with ...   \n",
       "31  Here we use the existing Math Word Problem dat...   \n",
       "32  Authors used two dialogue datasets, those are ...   \n",
       "33  In this no data set is used, as most of the in...   \n",
       "34  Here also no dataset is used as the input is T...   \n",
       "35  There is no dataset used, instead the authors ...   \n",
       "36  The authors used two benchmark datasets: Twitt...   \n",
       "37  The paper mentions that it uses various datase...   \n",
       "38  Authors created a new dataset called CHERMA da...   \n",
       "39  The researchers test CASN on three datasets co...   \n",
       "\n",
       "                                           Conclusion  \\\n",
       "0   that MUST leads to a more robust dialogue syst...   \n",
       "1   The experiments showed that the detected unsaf...   \n",
       "2   The method proposed by the authors improves de...   \n",
       "3   The experiments show that their model outperfo...   \n",
       "4   The authors’ ternary BART base achieves an R1 ...   \n",
       "5   The authors’ approach significantly improves u...   \n",
       "6   Theoretical analyses and extensive experiments...   \n",
       "7   The authors demonstrate the effectiveness of A...   \n",
       "8   The authors explore few-shot prompting strateg...   \n",
       "9   The authors’ proposed method achieved a speed-...   \n",
       "10  They have introduced MIL-Decoding, which can d...   \n",
       "11  The empirical evidence gathered in this work p...   \n",
       "12  This work investigates Open-LTG using NAR mode...   \n",
       "13  In this paper, they honed techniques for predi...   \n",
       "14  In this paper, they proposed a new way to buil...   \n",
       "15  They examine a lifelong learning paradigm for ...   \n",
       "16  In this paper, they investigate the input and ...   \n",
       "17  In this paper, they propose a weakly supervise...   \n",
       "18  They present the AMR-based Path Aggregation Re...   \n",
       "19  To counter substitution-based adversarial atta...   \n",
       "20  The author finds that the new approach of SPEE...   \n",
       "21  Author finds novel dual-encoder model architec...   \n",
       "22  The author used multiple platform to transalat...   \n",
       "23  The results of the finding s shows that the AP...   \n",
       "24  After overall testing , author identifies that...   \n",
       "25  The authors find that LLMs exhibit a significa...   \n",
       "26  The results demonstrate the importance of inco...   \n",
       "27  In the conclusion, author has been successful ...   \n",
       "28  The proposed critical sampling strategy during...   \n",
       "29  PANA effectively leverages the unlabeled insta...   \n",
       "30  W2W with grounding supervision outperforms mod...   \n",
       "31  Here in the paper, size of the LLM model doesn...   \n",
       "32  In this paper they found that CMN outperforms ...   \n",
       "33  The findings in the paper are diversification ...   \n",
       "34  From this paper we can say that SMP outperform...   \n",
       "35  From this paper we found that MT models aren’t...   \n",
       "36  The findings of the paper are that the CCD fra...   \n",
       "37  From this paper we found that LEXSYM improves ...   \n",
       "38  Further analysis shows that LFMIM's design cho...   \n",
       "39  CASN achieves impressive results, nearly perfe...   \n",
       "\n",
       "                                          Future work  \n",
       "0   we can do advances to the model used in this p...  \n",
       "1   the findings suggest that there is potential f...  \n",
       "2   the findings suggest that there is potential f...  \n",
       "3   the findings suggest that there is potential f...  \n",
       "4   the findings suggest that there is potential f...  \n",
       "5   the findings suggest that there is potential f...  \n",
       "6   the findings suggest that there is potential f...  \n",
       "7   The findings suggest that there is potential f...  \n",
       "8   The findings suggest that there is potential f...  \n",
       "9   The findings suggest that there is potential f...  \n",
       "10  In future week, they will explore how to balan...  \n",
       "11  The results of this article may be expanded up...  \n",
       "12  Paper’s primary concern focuses on acceleratin...  \n",
       "13  In this area could concentrate on various dire...  \n",
       "14  Subsequent research endeavors may delve deeper...  \n",
       "15  Subsequent research endeavors may delve deeper...  \n",
       "16  In future, they plan to adapt their method to ...  \n",
       "17  Subsequent research endeavours may centre on e...  \n",
       "18  In the future, APARN with higher quality AMRs ...  \n",
       "19  They are the first to consider the adversarial...  \n",
       "20  To take the advancements in this approach of S...  \n",
       "21  This rule based example model has only worked ...  \n",
       "22  The author suggest that the handling of pronou...  \n",
       "23  They also propose extending the APE model to o...  \n",
       "24  They also propose investigating more advanced ...  \n",
       "25  The authors suggest several directions for fut...  \n",
       "26  They also propose investigating techniques for...  \n",
       "27  The author has been successful in creating the...  \n",
       "28  The author expecting to use different kind of ...  \n",
       "29  The author propose investigating more advanced...  \n",
       "30  Authors develop language learning agents that ...  \n",
       "31  To extend the framework's applicability to lan...  \n",
       "32  It will involve 'disentanglement studies' to a...  \n",
       "33  In future, we will develop methods for trainin...  \n",
       "34  The future work of the paper is Investigating ...  \n",
       "35  In future use, they call for further developme...  \n",
       "36  Future work mentioned in the paper involves im...  \n",
       "37  The future work mentioned in the paper is to e...  \n",
       "38  Future work aims to find this optimal balance....  \n",
       "39  The future work could involve improving CASN's...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# Define function to extract insights based on headers\n",
    "def extract_insights(text):\n",
    "    insights = {}\n",
    "    headers = [\n",
    "        'Purpose', 'Contribution', 'Method', 'Dataset', 'Conclusion', 'Future work'\n",
    "    ]\n",
    "    \n",
    "    # Use regular expressions to extract content for each header\n",
    "    for header in headers:\n",
    "        pattern = fr'{header}: (.+?)(?={(\"|\".join(headers))}|$)'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            insights[header] = match.group(1).strip()\n",
    "        else:\n",
    "            insights[header] = None\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Apply function to each row in the DataFrame\n",
    "Manualdf = pd.concat([Manualdf, Manualdf['Manual_Insights'].apply(lambda x: pd.Series(extract_insights(x)))], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "Manualdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94353ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The future work could involve improving CASN's efficiency and generalizability.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Manualdf.iloc[39,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the data you want to save\n",
    "\n",
    "# Function to clean problematic characters\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        # Encode text to handle surrogate characters\n",
    "        cleaned_text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "        return cleaned_text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply clean_text function to all columns of DataFrame\n",
    "df_cleaned = Manualdf.applymap(clean_text)\n",
    "\n",
    "# Export cleaned DataFrame to JSON\n",
    "json_file_path = 'NewData_papers_upto_40.json'\n",
    "df_cleaned.to_json(json_file_path, orient='records', force_ascii=False)\n",
    "print(f\"Data saved to {json_file_path}\")\n",
    "\n",
    "\n",
    "# Specify the file path for saving the JSON file locally\n",
    "json_file_path = r'C:\\Users\\harsh\\Downloads\\NewData_papers_upto_40.json'\n",
    "\n",
    "# Export cleaned DataFrame to JSON with specified file path\n",
    "df_cleaned.to_json(json_file_path, orient='records', force_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {json_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
