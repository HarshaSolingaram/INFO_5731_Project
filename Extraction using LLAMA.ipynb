{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe767ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\harsh\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: bert-score 0.3.10 has a non-standard dependency specifier transformers>=3.0.0numpy. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of bert-score or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\harsh\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.25.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-1.25.1-py3-none-any.whl (312 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.25.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: bert-score 0.3.10 has a non-standard dependency specifier transformers>=3.0.0numpy. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of bert-score or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llamaapi in c:\\users\\harsh\\anaconda3\\lib\\site-packages (0.1.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from llamaapi) (3.9.3)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.6 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from llamaapi) (1.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.27.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from llamaapi) (2.31.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.5->llamaapi) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.27.1->llamaapi) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: bert-score 0.3.10 has a non-standard dependency specifier transformers>=3.0.0numpy. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of bert-score or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version is compatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install openai --upgrade\n",
    "!pip install llamaapi\n",
    "\n",
    "import json\n",
    "from llamaapi import LlamaAPI\n",
    "\n",
    "import openai\n",
    "from packaging import version\n",
    "\n",
    "required_version = version.parse(\"1.1.1\")\n",
    "current_version = version.parse(openai.__version__)\n",
    "\n",
    "if current_version < required_version:\n",
    "    raise ValueError(f\"Error: OpenAI version {openai.__version__}\"\n",
    "                     \" is less than the required version 1.1.1\")\n",
    "else:\n",
    "    print(\"OpenAI version is compatible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda9533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2f0bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Background</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Experiments</th>\n",
       "      <th>Related Work</th>\n",
       "      <th>Conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One Cannot Stand for Everyone! Leveraging Mult...</td>\n",
       "      <td>Abstract User simulators are agents designed t...</td>\n",
       "      <td>Background Dialogue system. Taskoriented dialo...</td>\n",
       "      <td>Introduction Taskoriented dialogue systems aim...</td>\n",
       "      <td>method called MUST adaptive that balances i) t...</td>\n",
       "      <td>experiments, we observed that the dialogue sys...</td>\n",
       "      <td></td>\n",
       "      <td>Conclusion In this paper, we propose a framewo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SafeConv: Explaining and Correcting Conversati...</td>\n",
       "      <td>Abstract One of the main challenges opendomain...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Safety of artificial intelligence...</td>\n",
       "      <td>method is onetime checking and rewriting—direc...</td>\n",
       "      <td>Experiments show that the detected unsafe beha...</td>\n",
       "      <td>Related Work Dialogue Safety Datasets Datasets...</td>\n",
       "      <td>Conclusion In this paper, we study how to expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Detecting and Mitigating Hallucinations in Mac...</td>\n",
       "      <td>Abstract While the problem of hallucinations i...</td>\n",
       "      <td>Background and Setting In this section, we des...</td>\n",
       "      <td>Introduction Hallucinations in machine transla...</td>\n",
       "      <td>method that evaluates the percentage of the so...</td>\n",
       "      <td>experiments.1 1 Introduction Hallucinations in...</td>\n",
       "      <td></td>\n",
       "      <td>Conclusions We start by asking how far we can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explainable Recommendation with Personalized R...</td>\n",
       "      <td>Abstract Explainable recommendation is a techn...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Recent years have witnessed a gro...</td>\n",
       "      <td>method has been employed to identify and selec...</td>\n",
       "      <td>experiments on three datasets show that our mo...</td>\n",
       "      <td>Related Work 2.1 Explainable Recommendation wi...</td>\n",
       "      <td>Conclusion In this paper, we propose a novel m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binary and Ternary Natural Language Generation</td>\n",
       "      <td>Abstract Ternary and binary neural networks en...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Generative pretrained transformer...</td>\n",
       "      <td>method to natural language generation tasks an...</td>\n",
       "      <td>Experiments In this section, we evaluate the e...</td>\n",
       "      <td>Related Work Quantization has long been studie...</td>\n",
       "      <td>Conclusion We have demonstrated high accuracy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Span-Selective Linear Attention Transformers f...</td>\n",
       "      <td>Abstract In schemaguided dialogue state tracki...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Dialogue State Tracking (DST) ref...</td>\n",
       "      <td>approach significantly improves upon existin...</td>\n",
       "      <td>experiments on the SchemaGuided Dialogue (SGD)...</td>\n",
       "      <td>Related Work Extractive DST. Following the tra...</td>\n",
       "      <td>Conclusion In this work we introduced SPLAT, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EMPre-training for Multi-party Dialogue Respon...</td>\n",
       "      <td>Abstract Dialogue response generation requires...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Inspired by the tremendous succes...</td>\n",
       "      <td>method. The official implementation of this pa...</td>\n",
       "      <td>experiments have justified the feasibility and...</td>\n",
       "      <td>Related Works 2.1 Pretraining for Response Ge...</td>\n",
       "      <td>Conclusion Most multiparty dialogue corpora ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACLM: A Selective-Denoising based Generative D...</td>\n",
       "      <td>Abstract Complex Named Entity Recognition (NER...</td>\n",
       "      <td>Background and Related Work Complex NER Backgr...</td>\n",
       "      <td>Introduction Named Entity Recognition (NER) is...</td>\n",
       "      <td>method (Zhou and Chen, 2021) (previously evalu...</td>\n",
       "      <td>experiments reveal that the performance of the...</td>\n",
       "      <td>Related Work Complex NER</td>\n",
       "      <td>Conclusion In this paper, we propose ACLM, a n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Natural Language to Code Generation in Interac...</td>\n",
       "      <td>Abstract Computational notebooks, such as Jupy...</td>\n",
       "      <td>background material and hints in tutorial note...</td>\n",
       "      <td>Introduction Data science is the process of ex...</td>\n",
       "      <td>Method Length All / pandas JuICe (Agashe et al...</td>\n",
       "      <td>Experiments Models We evaluate PACHINC Oand st...</td>\n",
       "      <td>Related Work Automating Data Science The amoun...</td>\n",
       "      <td>Conclusion In this paper we present ARCADE , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subset Retrieval Nearest Neighbor Machine Tran...</td>\n",
       "      <td>Abstract knearest-neighbor machine translation...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Neural machine translation (NMT) ...</td>\n",
       "      <td>method improves the translation performance of...</td>\n",
       "      <td>experiments on the WMT’19 Germanto-English, th...</td>\n",
       "      <td>Related Work The ﬁrst type of examplebased mac...</td>\n",
       "      <td>Conclusion In this paper, we proposed “Subset ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MIL-Decoding: Detoxifying Language Models at T...</td>\n",
       "      <td>Abstract Despite advances in large pretrained ...</td>\n",
       "      <td>Background 2.1 Multiple Instance Learning (MIL...</td>\n",
       "      <td>Introduction Trained on huge amount of text co...</td>\n",
       "      <td>method uses a MIL network to score the retriev...</td>\n",
       "      <td>experiments conditioned on two widelyused data...</td>\n",
       "      <td>Related Work Much closely related work has bee...</td>\n",
       "      <td>Conclusion We have introduced MILDecoding, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dependency resolution at the syntax-semantics ...</td>\n",
       "      <td>Abstract Using psycholinguistic and computatio...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Treating pretrained language mode...</td>\n",
       "      <td>method consists of comparing model probabiliti...</td>\n",
       "      <td>experiments we compare the ability of humans a...</td>\n",
       "      <td>Related work Targeted evaluation of LMs: Targe...</td>\n",
       "      <td>conclusions. Our findings are equally valuabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Open-ended Long Text Generation via Masked Lan...</td>\n",
       "      <td>Abstract Pretrained autoregressive (AR) langua...</td>\n",
       "      <td>background) and the target representation of t...</td>\n",
       "      <td>Introduction Pretrained language models (PLMs)...</td>\n",
       "      <td>method design (§ 3.3), where the model can gen...</td>\n",
       "      <td>Experiments on the storytelling and multiparag...</td>\n",
       "      <td>Related Work Long Text Generation Text generat...</td>\n",
       "      <td>Conclusion This paper explores OpenLTG with NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A Method for Studying Semantic Construal in Gr...</td>\n",
       "      <td>Abstract We study semantic construal in gramma...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction There are now several paradigms f...</td>\n",
       "      <td>method can probe the distributional meaning of...</td>\n",
       "      <td>experiments in this paper are available at htt...</td>\n",
       "      <td></td>\n",
       "      <td>conclusion aligns with expectations, given tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HolographicCCGParsing</td>\n",
       "      <td>Abstract We propose a method for formulating C...</td>\n",
       "      <td>Background and Related Work 2.1 Recursive Comp...</td>\n",
       "      <td>Introduction Combinatory Categorial Grammar (C...</td>\n",
       "      <td>method for formulating CCG as a recursive comp...</td>\n",
       "      <td>Experiments revealed that phraselevel dependen...</td>\n",
       "      <td>Related Work 2.1 Recursive Compositional Model...</td>\n",
       "      <td>Conclusion In this paper, we proposed a novel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Prompts Can Play Lottery Tickets Well: Achievi...</td>\n",
       "      <td>Abstract Thanks to the recent success of Pretr...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Information Extraction (IE) is on...</td>\n",
       "      <td>method namely Lottery Prompt Tuning (LPT). LPT...</td>\n",
       "      <td>experiments demonstrate that LPT consistently ...</td>\n",
       "      <td>Related Work Lifelong Learning Lifelong Learni...</td>\n",
       "      <td>Conclusions In this paper, we study a lifelon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Retrieve-and-Sample: Document-level Event Argu...</td>\n",
       "      <td>Abstract Recent studies have shown the effecti...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Transforming the large amounts of...</td>\n",
       "      <td>method) with underlines. The best results amon...</td>\n",
       "      <td>experiments on RAMS and WikiEvents, we demonst...</td>\n",
       "      <td>Related Work Documentlevel Event Argument Extr...</td>\n",
       "      <td>Conclusion In this paper, we explore how to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WeCheck: Strong Factual Consistency Checker vi...</td>\n",
       "      <td>Abstract A crucial issue of current text gener...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction The research of text generation h...</td>\n",
       "      <td>method. 2.1 Problem Definition Factual Consist...</td>\n",
       "      <td>experiments on various tasks demonstrate the s...</td>\n",
       "      <td>Related Work Factual Consistency Evaluation Re...</td>\n",
       "      <td>Conclusion In this paper, we propose a weakly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AMR-based Network for Aspect-based Sentiment A...</td>\n",
       "      <td>Abstract Aspectbased sentiment analysis (ABSA)...</td>\n",
       "      <td>background “none” relations to the precious ef...</td>\n",
       "      <td>Introduction Recent years have witnessed growi...</td>\n",
       "      <td>approach to explicitly utilize dependency ty...</td>\n",
       "      <td>experiments further verify the significance of...</td>\n",
       "      <td>Related Work Aspectbased Sentiment Analysis Tr...</td>\n",
       "      <td>Conclusion In this paper, we propose APARN , A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Text Adversarial Purification as Defense again...</td>\n",
       "      <td>Abstract Adversarial purification is a success...</td>\n",
       "      <td>Background of Adversarial Purification A class...</td>\n",
       "      <td>Introduction Adversarial examples (Goodfellow ...</td>\n",
       "      <td>method that focuses on defending against textu...</td>\n",
       "      <td>experiments, we prove that the proposed text a...</td>\n",
       "      <td>Related Work 2.1 Adversarial Attacks in NLP In...</td>\n",
       "      <td>Conclusion and Future Work In this paper, we i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SPEECH: Structured Prediction with Energy-Base...</td>\n",
       "      <td>Abstract Eventcentric structured prediction in...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Structured prediction (Taskar et ...</td>\n",
       "      <td>Methodology 3.1 Preliminaries For structured ...</td>\n",
       "      <td>Experiments on two uniﬁedannotated event datas...</td>\n",
       "      <td>Related Work EventCentric Structured Predictio...</td>\n",
       "      <td>Conclusion and Future Work In this paper, we p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Rule By Example: Harnessing Logical Rules for ...</td>\n",
       "      <td>Abstract Classic approaches to content moderat...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Content moderation is a major cha...</td>\n",
       "      <td>method for noisy supervision using rules. Whil...</td>\n",
       "      <td>experiments, we combine the output classes of ...</td>\n",
       "      <td>Related Work There has been active work on det...</td>\n",
       "      <td>Conclusion We introduce Rule By Example, an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What about “em”? How Commercial Machine Transl...</td>\n",
       "      <td>Abstract As 3rdperson pronoun usage shifts to ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Machine translation (MT) is one o...</td>\n",
       "      <td>approach to detect gender issues in realworl...</td>\n",
       "      <td>Experimental Setup Our overall setup consists...</td>\n",
       "      <td>Related Work We review works on gender bias in...</td>\n",
       "      <td>Conclusion In this work, we have investigated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What Is Overlap Knowledge in Event Argument Ex...</td>\n",
       "      <td>Abstract The EAE task extracts a structured ev...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Event extraction (EE) is a pivota...</td>\n",
       "      <td>Method As illustrated in Figure 2, APE learns ...</td>\n",
       "      <td>experiments show APE achieves new stateof-thea...</td>\n",
       "      <td>Related Works 5.1 Transfer Learning in EAE Ev...</td>\n",
       "      <td>Conclusion In this work, we first define the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tailor: A Soft-Prompt-Based Approach to Attrib...</td>\n",
       "      <td>Abstract Attributebased Controlled Text Genera...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Attributebased CTG (Zhang et al.,...</td>\n",
       "      <td>method introduces aMultiAttribute Prompt mask ...</td>\n",
       "      <td>Experiments demonstrate that, only requiring 0...</td>\n",
       "      <td>Related Work AttributeBased CTG focuses on gen...</td>\n",
       "      <td>Conclusions In this paper, we explore attribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>Abstract Moral norms vary across cultures. A r...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Moral norms vary from culture to ...</td>\n",
       "      <td>method to estimate moral values and found EPLM...</td>\n",
       "      <td>experiments in the previous section for differ...</td>\n",
       "      <td>Related work 2.1 Automated moral inference in ...</td>\n",
       "      <td>conclusion We investigated whether English pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Songs Across Borders: Singable and Controllabl...</td>\n",
       "      <td>Abstract The development of generaldomain neur...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction With the globalization of enterta...</td>\n",
       "      <td>method design. Particularly, the “Pentathlon P...</td>\n",
       "      <td>experiments to test three different prompt met...</td>\n",
       "      <td>Related Work Lyric/Poetry Translation. Designi...</td>\n",
       "      <td>Conclusion We discussed how to obtain singable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fantastic Expressions and Where to Find Them:C...</td>\n",
       "      <td>Abstract Similes occur in the creative context...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Similes are widelyused and stimul...</td>\n",
       "      <td>method ) for the given tenor , then incorporat...</td>\n",
       "      <td>Experiments In this section, we first experime...</td>\n",
       "      <td>Related Work Different from metaphor (Yu and W...</td>\n",
       "      <td>Conclusion In this paper, we introduce a new t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Revealing Single Frame Bias for Video-and-Lang...</td>\n",
       "      <td>Abstract Training an effective videoand-langua...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Video and language are the two pr...</td>\n",
       "      <td>method outperforms late fusion strategies and ...</td>\n",
       "      <td>Experiments 4.1 Downstream Task Setup Textto-V...</td>\n",
       "      <td>Related Work Vision and Language. Vision and l...</td>\n",
       "      <td>conclusion holds for both short 15second MSRVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Learning with Partial Annotations for Event De...</td>\n",
       "      <td>Abstract Event detection (ED) seeks to discove...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Deep learning models have shown i...</td>\n",
       "      <td>method, to reduce the risk of mistraining on f...</td>\n",
       "      <td>experiments, we conduct realworld annotation e...</td>\n",
       "      <td>Related Work ED and the Partial Annotation Iss...</td>\n",
       "      <td>Conclusion In this study, we investigate the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>World-to-Words: Grounded Open Vocabulary Acqui...</td>\n",
       "      <td>Abstract The ability to connect language units...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Language is learned through senso...</td>\n",
       "      <td></td>\n",
       "      <td>experiments and analysis, we demonstrate that ...</td>\n",
       "      <td>Related Work VisionLanguage Mapping Mapping pl...</td>\n",
       "      <td>Conclusion and Future Work The connection betw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A Causal Framework to Quantify the Robustness ...</td>\n",
       "      <td>Abstract We have recently witnessed a number o...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Many natural language understandi...</td>\n",
       "      <td>method in the model’s performance, we carry ou...</td>\n",
       "      <td>experiments we generate 500 intervention pairs...</td>\n",
       "      <td>Related Work Causal NLP. Causal inference aims...</td>\n",
       "      <td>Conclusion We developed a framework to disenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Evaluating Open-Domain Dialogues in Latent Spa...</td>\n",
       "      <td>Abstract The longstanding oneto-many issue of ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Opendomain dialogue generation is...</td>\n",
       "      <td>method compared with a wide range of baselines...</td>\n",
       "      <td>experiments on two open dialogue datasets. The...</td>\n",
       "      <td>Related Work Referencebased metrics. Reference...</td>\n",
       "      <td>Conclusions In this paper, we propose a novel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Increasing Diversity While Maintaining Accurac...</td>\n",
       "      <td>Abstract Large language models (LLMs) can be u...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Training custom natural language ...</td>\n",
       "      <td>Method As a generative LLM, we used the textda...</td>\n",
       "      <td>experiments. Corpus of Linguistic Acceptabilit...</td>\n",
       "      <td>Related Work 2.1 Text Data Generation for Mode...</td>\n",
       "      <td>Conclusion In this work, we investigate approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Pruning Pre-trained Language Models Without Fi...</td>\n",
       "      <td>Abstract To overcome the overparameterized pro...</td>\n",
       "      <td>Background Leta=Wx refer to a fullyconnected l...</td>\n",
       "      <td>Introduction Pretrained Language Models (PLMs)...</td>\n",
       "      <td>method by directly removing unimportant weight...</td>\n",
       "      <td>experiments at various sparsity levels show SM...</td>\n",
       "      <td>Related Work Compressing PLMs for transfer lea...</td>\n",
       "      <td>Conclusion In this paper, we propose a simple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>When Does Translation Require Context? A Data-...</td>\n",
       "      <td>Abstract Although proper handling of discourse...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction In order to properly translate di...</td>\n",
       "      <td>method to create a multilingual benchmark test...</td>\n",
       "      <td>experiments, we assessed that this model only ...</td>\n",
       "      <td>Related Work Several works have worked on meas...</td>\n",
       "      <td>Conclusions and Future Work We investigate ty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Causal Intervention and Counterfactual Reasoni...</td>\n",
       "      <td>Abstract Due to the rapid upgrade of social pl...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Fake news quietly sneaks into peo...</td>\n",
       "      <td>method used to infer outcomes under hypothetic...</td>\n",
       "      <td>experiments on two realworld benchmark dataset...</td>\n",
       "      <td>Related Work In this section, we review the re...</td>\n",
       "      <td>conclusion that fake news prefers to use loade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LexSym: Compositionality as Lexical Symmetry</td>\n",
       "      <td>Abstract In tasks like semantic parsing, instr...</td>\n",
       "      <td>Background &amp; Approach We begin with a discussi...</td>\n",
       "      <td>Introduction A central challenge in natural la...</td>\n",
       "      <td>method that improves compositional generalizat...</td>\n",
       "      <td>experiments use more flexible models. We are n...</td>\n",
       "      <td>related work is discussed in Sec. 6. 3 Composi...</td>\n",
       "      <td>Conclusion We have presented LEXSYM, a new dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Layer-wise Fusion with Modality Independence M...</td>\n",
       "      <td>Abstract Multimodal emotion recognition has ga...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction The goal of human emotion recogni...</td>\n",
       "      <td>approach utilizes separate supervisions for ...</td>\n",
       "      <td>Experiments and Analysis In this section, we f...</td>\n",
       "      <td>Related Works There are a large volume of rel...</td>\n",
       "      <td>conclusion can be drawn from Figure 1(c) that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CASN:Class-Aware Score Network for Textual Adv...</td>\n",
       "      <td>Abstract Adversarial detection aims to detect ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction It has already become a consensus...</td>\n",
       "      <td>method to implicitly model the data distributi...</td>\n",
       "      <td>experiments are also presented. 6.1 Detection ...</td>\n",
       "      <td>Related work 2.1 Textual Adversarial Attacks C...</td>\n",
       "      <td>Conclusion In this paper, we propose a nearlyp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0   One Cannot Stand for Everyone! Leveraging Mult...   \n",
       "1   SafeConv: Explaining and Correcting Conversati...   \n",
       "2   Detecting and Mitigating Hallucinations in Mac...   \n",
       "3   Explainable Recommendation with Personalized R...   \n",
       "4      Binary and Ternary Natural Language Generation   \n",
       "5   Span-Selective Linear Attention Transformers f...   \n",
       "6   EMPre-training for Multi-party Dialogue Respon...   \n",
       "7   ACLM: A Selective-Denoising based Generative D...   \n",
       "8   Natural Language to Code Generation in Interac...   \n",
       "9   Subset Retrieval Nearest Neighbor Machine Tran...   \n",
       "10  MIL-Decoding: Detoxifying Language Models at T...   \n",
       "11  Dependency resolution at the syntax-semantics ...   \n",
       "12  Open-ended Long Text Generation via Masked Lan...   \n",
       "13  A Method for Studying Semantic Construal in Gr...   \n",
       "14                              HolographicCCGParsing   \n",
       "15  Prompts Can Play Lottery Tickets Well: Achievi...   \n",
       "16  Retrieve-and-Sample: Document-level Event Argu...   \n",
       "17  WeCheck: Strong Factual Consistency Checker vi...   \n",
       "18  AMR-based Network for Aspect-based Sentiment A...   \n",
       "19  Text Adversarial Purification as Defense again...   \n",
       "20  SPEECH: Structured Prediction with Energy-Base...   \n",
       "21  Rule By Example: Harnessing Logical Rules for ...   \n",
       "22  What about “em”? How Commercial Machine Transl...   \n",
       "23  What Is Overlap Knowledge in Event Argument Ex...   \n",
       "24  Tailor: A Soft-Prompt-Based Approach to Attrib...   \n",
       "25  Knowledge of cultural moral norms in large lan...   \n",
       "26  Songs Across Borders: Singable and Controllabl...   \n",
       "27  Fantastic Expressions and Where to Find Them:C...   \n",
       "28  Revealing Single Frame Bias for Video-and-Lang...   \n",
       "29  Learning with Partial Annotations for Event De...   \n",
       "30  World-to-Words: Grounded Open Vocabulary Acqui...   \n",
       "31  A Causal Framework to Quantify the Robustness ...   \n",
       "32  Evaluating Open-Domain Dialogues in Latent Spa...   \n",
       "33  Increasing Diversity While Maintaining Accurac...   \n",
       "34  Pruning Pre-trained Language Models Without Fi...   \n",
       "35  When Does Translation Require Context? A Data-...   \n",
       "36  Causal Intervention and Counterfactual Reasoni...   \n",
       "37       LexSym: Compositionality as Lexical Symmetry   \n",
       "38  Layer-wise Fusion with Modality Independence M...   \n",
       "39  CASN:Class-Aware Score Network for Textual Adv...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Abstract User simulators are agents designed t...   \n",
       "1   Abstract One of the main challenges opendomain...   \n",
       "2   Abstract While the problem of hallucinations i...   \n",
       "3   Abstract Explainable recommendation is a techn...   \n",
       "4   Abstract Ternary and binary neural networks en...   \n",
       "5   Abstract In schemaguided dialogue state tracki...   \n",
       "6   Abstract Dialogue response generation requires...   \n",
       "7   Abstract Complex Named Entity Recognition (NER...   \n",
       "8   Abstract Computational notebooks, such as Jupy...   \n",
       "9   Abstract knearest-neighbor machine translation...   \n",
       "10  Abstract Despite advances in large pretrained ...   \n",
       "11  Abstract Using psycholinguistic and computatio...   \n",
       "12  Abstract Pretrained autoregressive (AR) langua...   \n",
       "13  Abstract We study semantic construal in gramma...   \n",
       "14  Abstract We propose a method for formulating C...   \n",
       "15  Abstract Thanks to the recent success of Pretr...   \n",
       "16  Abstract Recent studies have shown the effecti...   \n",
       "17  Abstract A crucial issue of current text gener...   \n",
       "18  Abstract Aspectbased sentiment analysis (ABSA)...   \n",
       "19  Abstract Adversarial purification is a success...   \n",
       "20  Abstract Eventcentric structured prediction in...   \n",
       "21  Abstract Classic approaches to content moderat...   \n",
       "22  Abstract As 3rdperson pronoun usage shifts to ...   \n",
       "23  Abstract The EAE task extracts a structured ev...   \n",
       "24  Abstract Attributebased Controlled Text Genera...   \n",
       "25  Abstract Moral norms vary across cultures. A r...   \n",
       "26  Abstract The development of generaldomain neur...   \n",
       "27  Abstract Similes occur in the creative context...   \n",
       "28  Abstract Training an effective videoand-langua...   \n",
       "29  Abstract Event detection (ED) seeks to discove...   \n",
       "30  Abstract The ability to connect language units...   \n",
       "31  Abstract We have recently witnessed a number o...   \n",
       "32  Abstract The longstanding oneto-many issue of ...   \n",
       "33  Abstract Large language models (LLMs) can be u...   \n",
       "34  Abstract To overcome the overparameterized pro...   \n",
       "35  Abstract Although proper handling of discourse...   \n",
       "36  Abstract Due to the rapid upgrade of social pl...   \n",
       "37  Abstract In tasks like semantic parsing, instr...   \n",
       "38  Abstract Multimodal emotion recognition has ga...   \n",
       "39  Abstract Adversarial detection aims to detect ...   \n",
       "\n",
       "                                           Background  \\\n",
       "0   Background Dialogue system. Taskoriented dialo...   \n",
       "1                                                       \n",
       "2   Background and Setting In this section, we des...   \n",
       "3                                                       \n",
       "4                                                       \n",
       "5                                                       \n",
       "6                                                       \n",
       "7   Background and Related Work Complex NER Backgr...   \n",
       "8   background material and hints in tutorial note...   \n",
       "9                                                       \n",
       "10  Background 2.1 Multiple Instance Learning (MIL...   \n",
       "11                                                      \n",
       "12  background) and the target representation of t...   \n",
       "13                                                      \n",
       "14  Background and Related Work 2.1 Recursive Comp...   \n",
       "15                                                      \n",
       "16                                                      \n",
       "17                                                      \n",
       "18  background “none” relations to the precious ef...   \n",
       "19  Background of Adversarial Purification A class...   \n",
       "20                                                      \n",
       "21                                                      \n",
       "22                                                      \n",
       "23                                                      \n",
       "24                                                      \n",
       "25                                                      \n",
       "26                                                      \n",
       "27                                                      \n",
       "28                                                      \n",
       "29                                                      \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                                      \n",
       "33                                                      \n",
       "34  Background Leta=Wx refer to a fullyconnected l...   \n",
       "35                                                      \n",
       "36                                                      \n",
       "37  Background & Approach We begin with a discussi...   \n",
       "38                                                      \n",
       "39                                                      \n",
       "\n",
       "                                         Introduction  \\\n",
       "0   Introduction Taskoriented dialogue systems aim...   \n",
       "1   Introduction Safety of artificial intelligence...   \n",
       "2   Introduction Hallucinations in machine transla...   \n",
       "3   Introduction Recent years have witnessed a gro...   \n",
       "4   Introduction Generative pretrained transformer...   \n",
       "5   Introduction Dialogue State Tracking (DST) ref...   \n",
       "6   Introduction Inspired by the tremendous succes...   \n",
       "7   Introduction Named Entity Recognition (NER) is...   \n",
       "8   Introduction Data science is the process of ex...   \n",
       "9   Introduction Neural machine translation (NMT) ...   \n",
       "10  Introduction Trained on huge amount of text co...   \n",
       "11  Introduction Treating pretrained language mode...   \n",
       "12  Introduction Pretrained language models (PLMs)...   \n",
       "13  Introduction There are now several paradigms f...   \n",
       "14  Introduction Combinatory Categorial Grammar (C...   \n",
       "15  Introduction Information Extraction (IE) is on...   \n",
       "16  Introduction Transforming the large amounts of...   \n",
       "17  Introduction The research of text generation h...   \n",
       "18  Introduction Recent years have witnessed growi...   \n",
       "19  Introduction Adversarial examples (Goodfellow ...   \n",
       "20  Introduction Structured prediction (Taskar et ...   \n",
       "21  Introduction Content moderation is a major cha...   \n",
       "22  Introduction Machine translation (MT) is one o...   \n",
       "23  Introduction Event extraction (EE) is a pivota...   \n",
       "24  Introduction Attributebased CTG (Zhang et al.,...   \n",
       "25  Introduction Moral norms vary from culture to ...   \n",
       "26  Introduction With the globalization of enterta...   \n",
       "27  Introduction Similes are widelyused and stimul...   \n",
       "28  Introduction Video and language are the two pr...   \n",
       "29  Introduction Deep learning models have shown i...   \n",
       "30  Introduction Language is learned through senso...   \n",
       "31  Introduction Many natural language understandi...   \n",
       "32  Introduction Opendomain dialogue generation is...   \n",
       "33  Introduction Training custom natural language ...   \n",
       "34  Introduction Pretrained Language Models (PLMs)...   \n",
       "35  Introduction In order to properly translate di...   \n",
       "36  Introduction Fake news quietly sneaks into peo...   \n",
       "37  Introduction A central challenge in natural la...   \n",
       "38  Introduction The goal of human emotion recogni...   \n",
       "39  Introduction It has already become a consensus...   \n",
       "\n",
       "                                          Methodology  \\\n",
       "0   method called MUST adaptive that balances i) t...   \n",
       "1   method is onetime checking and rewriting—direc...   \n",
       "2   method that evaluates the percentage of the so...   \n",
       "3   method has been employed to identify and selec...   \n",
       "4   method to natural language generation tasks an...   \n",
       "5     approach significantly improves upon existin...   \n",
       "6   method. The official implementation of this pa...   \n",
       "7   method (Zhou and Chen, 2021) (previously evalu...   \n",
       "8   Method Length All / pandas JuICe (Agashe et al...   \n",
       "9   method improves the translation performance of...   \n",
       "10  method uses a MIL network to score the retriev...   \n",
       "11  method consists of comparing model probabiliti...   \n",
       "12  method design (§ 3.3), where the model can gen...   \n",
       "13  method can probe the distributional meaning of...   \n",
       "14  method for formulating CCG as a recursive comp...   \n",
       "15  method namely Lottery Prompt Tuning (LPT). LPT...   \n",
       "16  method) with underlines. The best results amon...   \n",
       "17  method. 2.1 Problem Definition Factual Consist...   \n",
       "18    approach to explicitly utilize dependency ty...   \n",
       "19  method that focuses on defending against textu...   \n",
       "20   Methodology 3.1 Preliminaries For structured ...   \n",
       "21  method for noisy supervision using rules. Whil...   \n",
       "22    approach to detect gender issues in realworl...   \n",
       "23  Method As illustrated in Figure 2, APE learns ...   \n",
       "24  method introduces aMultiAttribute Prompt mask ...   \n",
       "25  method to estimate moral values and found EPLM...   \n",
       "26  method design. Particularly, the “Pentathlon P...   \n",
       "27  method ) for the given tenor , then incorporat...   \n",
       "28  method outperforms late fusion strategies and ...   \n",
       "29  method, to reduce the risk of mistraining on f...   \n",
       "30                                                      \n",
       "31  method in the model’s performance, we carry ou...   \n",
       "32  method compared with a wide range of baselines...   \n",
       "33  Method As a generative LLM, we used the textda...   \n",
       "34  method by directly removing unimportant weight...   \n",
       "35  method to create a multilingual benchmark test...   \n",
       "36  method used to infer outcomes under hypothetic...   \n",
       "37  method that improves compositional generalizat...   \n",
       "38    approach utilizes separate supervisions for ...   \n",
       "39  method to implicitly model the data distributi...   \n",
       "\n",
       "                                          Experiments  \\\n",
       "0   experiments, we observed that the dialogue sys...   \n",
       "1   Experiments show that the detected unsafe beha...   \n",
       "2   experiments.1 1 Introduction Hallucinations in...   \n",
       "3   experiments on three datasets show that our mo...   \n",
       "4   Experiments In this section, we evaluate the e...   \n",
       "5   experiments on the SchemaGuided Dialogue (SGD)...   \n",
       "6   experiments have justified the feasibility and...   \n",
       "7   experiments reveal that the performance of the...   \n",
       "8   Experiments Models We evaluate PACHINC Oand st...   \n",
       "9   experiments on the WMT’19 Germanto-English, th...   \n",
       "10  experiments conditioned on two widelyused data...   \n",
       "11  experiments we compare the ability of humans a...   \n",
       "12  Experiments on the storytelling and multiparag...   \n",
       "13  experiments in this paper are available at htt...   \n",
       "14  Experiments revealed that phraselevel dependen...   \n",
       "15  experiments demonstrate that LPT consistently ...   \n",
       "16  experiments on RAMS and WikiEvents, we demonst...   \n",
       "17  experiments on various tasks demonstrate the s...   \n",
       "18  experiments further verify the significance of...   \n",
       "19  experiments, we prove that the proposed text a...   \n",
       "20  Experiments on two uniﬁedannotated event datas...   \n",
       "21  experiments, we combine the output classes of ...   \n",
       "22   Experimental Setup Our overall setup consists...   \n",
       "23  experiments show APE achieves new stateof-thea...   \n",
       "24  Experiments demonstrate that, only requiring 0...   \n",
       "25  experiments in the previous section for differ...   \n",
       "26  experiments to test three different prompt met...   \n",
       "27  Experiments In this section, we first experime...   \n",
       "28  Experiments 4.1 Downstream Task Setup Textto-V...   \n",
       "29  experiments, we conduct realworld annotation e...   \n",
       "30  experiments and analysis, we demonstrate that ...   \n",
       "31  experiments we generate 500 intervention pairs...   \n",
       "32  experiments on two open dialogue datasets. The...   \n",
       "33  experiments. Corpus of Linguistic Acceptabilit...   \n",
       "34  experiments at various sparsity levels show SM...   \n",
       "35  experiments, we assessed that this model only ...   \n",
       "36  experiments on two realworld benchmark dataset...   \n",
       "37  experiments use more flexible models. We are n...   \n",
       "38  Experiments and Analysis In this section, we f...   \n",
       "39  experiments are also presented. 6.1 Detection ...   \n",
       "\n",
       "                                         Related Work  \\\n",
       "0                                                       \n",
       "1   Related Work Dialogue Safety Datasets Datasets...   \n",
       "2                                                       \n",
       "3   Related Work 2.1 Explainable Recommendation wi...   \n",
       "4   Related Work Quantization has long been studie...   \n",
       "5   Related Work Extractive DST. Following the tra...   \n",
       "6    Related Works 2.1 Pretraining for Response Ge...   \n",
       "7                           Related Work Complex NER    \n",
       "8   Related Work Automating Data Science The amoun...   \n",
       "9   Related Work The ﬁrst type of examplebased mac...   \n",
       "10  Related Work Much closely related work has bee...   \n",
       "11  Related work Targeted evaluation of LMs: Targe...   \n",
       "12  Related Work Long Text Generation Text generat...   \n",
       "13                                                      \n",
       "14  Related Work 2.1 Recursive Compositional Model...   \n",
       "15  Related Work Lifelong Learning Lifelong Learni...   \n",
       "16  Related Work Documentlevel Event Argument Extr...   \n",
       "17  Related Work Factual Consistency Evaluation Re...   \n",
       "18  Related Work Aspectbased Sentiment Analysis Tr...   \n",
       "19  Related Work 2.1 Adversarial Attacks in NLP In...   \n",
       "20  Related Work EventCentric Structured Predictio...   \n",
       "21  Related Work There has been active work on det...   \n",
       "22  Related Work We review works on gender bias in...   \n",
       "23   Related Works 5.1 Transfer Learning in EAE Ev...   \n",
       "24  Related Work AttributeBased CTG focuses on gen...   \n",
       "25  Related work 2.1 Automated moral inference in ...   \n",
       "26  Related Work Lyric/Poetry Translation. Designi...   \n",
       "27  Related Work Different from metaphor (Yu and W...   \n",
       "28  Related Work Vision and Language. Vision and l...   \n",
       "29  Related Work ED and the Partial Annotation Iss...   \n",
       "30  Related Work VisionLanguage Mapping Mapping pl...   \n",
       "31  Related Work Causal NLP. Causal inference aims...   \n",
       "32  Related Work Referencebased metrics. Reference...   \n",
       "33  Related Work 2.1 Text Data Generation for Mode...   \n",
       "34  Related Work Compressing PLMs for transfer lea...   \n",
       "35  Related Work Several works have worked on meas...   \n",
       "36  Related Work In this section, we review the re...   \n",
       "37  related work is discussed in Sec. 6. 3 Composi...   \n",
       "38   Related Works There are a large volume of rel...   \n",
       "39  Related work 2.1 Textual Adversarial Attacks C...   \n",
       "\n",
       "                                           Conclusion  \n",
       "0   Conclusion In this paper, we propose a framewo...  \n",
       "1   Conclusion In this paper, we study how to expl...  \n",
       "2    Conclusions We start by asking how far we can...  \n",
       "3   Conclusion In this paper, we propose a novel m...  \n",
       "4   Conclusion We have demonstrated high accuracy ...  \n",
       "5   Conclusion In this work we introduced SPLAT, a...  \n",
       "6   Conclusion Most multiparty dialogue corpora ar...  \n",
       "7   Conclusion In this paper, we propose ACLM, a n...  \n",
       "8   Conclusion In this paper we present ARCADE , a...  \n",
       "9   Conclusion In this paper, we proposed “Subset ...  \n",
       "10  Conclusion We have introduced MILDecoding, whi...  \n",
       "11   conclusions. Our findings are equally valuabl...  \n",
       "12  Conclusion This paper explores OpenLTG with NA...  \n",
       "13  conclusion aligns with expectations, given tha...  \n",
       "14     Conclusion In this paper, we proposed a novel   \n",
       "15   Conclusions In this paper, we study a lifelon...  \n",
       "16  Conclusion In this paper, we explore how to de...  \n",
       "17  Conclusion In this paper, we propose a weakly ...  \n",
       "18  Conclusion In this paper, we propose APARN , A...  \n",
       "19  Conclusion and Future Work In this paper, we i...  \n",
       "20  Conclusion and Future Work In this paper, we p...  \n",
       "21  Conclusion We introduce Rule By Example, an ex...  \n",
       "22  Conclusion In this work, we have investigated ...  \n",
       "23  Conclusion In this work, we first define the s...  \n",
       "24   Conclusions In this paper, we explore attribu...  \n",
       "25  conclusion We investigated whether English pre...  \n",
       "26  Conclusion We discussed how to obtain singable...  \n",
       "27  Conclusion In this paper, we introduce a new t...  \n",
       "28  conclusion holds for both short 15second MSRVT...  \n",
       "29  Conclusion In this study, we investigate the p...  \n",
       "30  Conclusion and Future Work The connection betw...  \n",
       "31  Conclusion We developed a framework to disenta...  \n",
       "32   Conclusions In this paper, we propose a novel...  \n",
       "33  Conclusion In this work, we investigate approa...  \n",
       "34  Conclusion In this paper, we propose a simple ...  \n",
       "35   Conclusions and Future Work We investigate ty...  \n",
       "36  conclusion that fake news prefers to use loade...  \n",
       "37  Conclusion We have presented LEXSYM, a new dat...  \n",
       "38  conclusion can be drawn from Figure 1(c) that ...  \n",
       "39  Conclusion In this paper, we propose a nearlyp...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the JSON file\n",
    "json_file_path = r'C:\\Users\\harsh\\Downloads\\NewData_papers_upto_40.json'\n",
    "\n",
    "# Load JSON file into a DataFrame\n",
    "df = pd.read_json(json_file_path, orient='records')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d27ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.__version__\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdeccd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "index\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anthropic\n",
    "\n",
    "\n",
    "\n",
    "#YOUR_ANTHROPIC_API_KEY = \"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\"\n",
    "\n",
    "YOUR_ANTHROPIC_API_KEY = \"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\"\n",
    "\n",
    "# Initialize the Anthropic client\n",
    "client = anthropic.Client(api_key=YOUR_ANTHROPIC_API_KEY)\n",
    "\n",
    "def extract_info_with_claude(category, df):\n",
    "    try:\n",
    "        # Define mappings of category to corresponding columns\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Introduction', 'Background'],\n",
    "            'contribution': ['Abstract', 'Conclusion'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        # Get the list of columns corresponding to the specified category\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            text = ' '.join([str(row[col]) for col in columns_to_search if row[col] is not None])\n",
    "            time.sleep(40)\n",
    "            if text:\n",
    "                prompt = f\"Extract the {category} of the paper:\\n\\n{text}\"\n",
    "                message = client.messages.create(\n",
    "                    model=\"claude-3-opus-20240229\",\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0,\n",
    "                    system=f\"i want to know {category} of the text\\n\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": f\"{text}\"\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                response_message = message.content\n",
    "                \n",
    "                # Function to extract text from TextBlock objects\n",
    "                def extract_text_from_textblock(textblock):\n",
    "                    return textblock.text\n",
    "\n",
    "                # Extract text content from each TextBlock in the list\n",
    "                extracted_texts = [extract_text_from_textblock(tb) for tb in response_message]\n",
    "\n",
    "                # Print the extracted text contents\n",
    "                for text in extracted_texts:\n",
    "                    print(\"index\")\n",
    "                \n",
    "                output = text\n",
    "                \n",
    "                #print(output)\n",
    "                df.loc[index, 'CLAUDE_ExtractedInfo'] = output\n",
    "\n",
    "        print(\"exit\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'contribution'\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "df['CLAUDE_ExtractedInfo'] = None\n",
    "\n",
    "# Apply the function to extract information based on the specified category\n",
    "df = extract_info_with_claude(category, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cdb4309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Background</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Experiments</th>\n",
       "      <th>Related Work</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>CLAUDE_ExtractedInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One Cannot Stand for Everyone! Leveraging Mult...</td>\n",
       "      <td>Abstract User simulators are agents designed t...</td>\n",
       "      <td>Background Dialogue system. Taskoriented dialo...</td>\n",
       "      <td>Introduction Taskoriented dialogue systems aim...</td>\n",
       "      <td>method called MUST adaptive that balances i) t...</td>\n",
       "      <td>experiments, we observed that the dialogue sys...</td>\n",
       "      <td></td>\n",
       "      <td>Conclusion In this paper, we propose a framewo...</td>\n",
       "      <td>Based on the abstract, the key contributions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SafeConv: Explaining and Correcting Conversati...</td>\n",
       "      <td>Abstract One of the main challenges opendomain...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Safety of artificial intelligence...</td>\n",
       "      <td>method is onetime checking and rewriting—direc...</td>\n",
       "      <td>Experiments show that the detected unsafe beha...</td>\n",
       "      <td>Related Work Dialogue Safety Datasets Datasets...</td>\n",
       "      <td>Conclusion In this paper, we study how to expl...</td>\n",
       "      <td>Based on the abstract, this paper makes the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Detecting and Mitigating Hallucinations in Mac...</td>\n",
       "      <td>Abstract While the problem of hallucinations i...</td>\n",
       "      <td>Background and Setting In this section, we des...</td>\n",
       "      <td>Introduction Hallucinations in machine transla...</td>\n",
       "      <td>method that evaluates the percentage of the so...</td>\n",
       "      <td>experiments.1 1 Introduction Hallucinations in...</td>\n",
       "      <td></td>\n",
       "      <td>Conclusions We start by asking how far we can...</td>\n",
       "      <td>Based on the conclusions section, the key poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explainable Recommendation with Personalized R...</td>\n",
       "      <td>Abstract Explainable recommendation is a techn...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Recent years have witnessed a gro...</td>\n",
       "      <td>method has been employed to identify and selec...</td>\n",
       "      <td>experiments on three datasets show that our mo...</td>\n",
       "      <td>Related Work 2.1 Explainable Recommendation wi...</td>\n",
       "      <td>Conclusion In this paper, we propose a novel m...</td>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Binary and Ternary Natural Language Generation</td>\n",
       "      <td>Abstract Ternary and binary neural networks en...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Generative pretrained transformer...</td>\n",
       "      <td>method to natural language generation tasks an...</td>\n",
       "      <td>Experiments In this section, we evaluate the e...</td>\n",
       "      <td>Related Work Quantization has long been studie...</td>\n",
       "      <td>Conclusion We have demonstrated high accuracy ...</td>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Span-Selective Linear Attention Transformers f...</td>\n",
       "      <td>Abstract In schemaguided dialogue state tracki...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Dialogue State Tracking (DST) ref...</td>\n",
       "      <td>approach significantly improves upon existin...</td>\n",
       "      <td>experiments on the SchemaGuided Dialogue (SGD)...</td>\n",
       "      <td>Related Work Extractive DST. Following the tra...</td>\n",
       "      <td>Conclusion In this work we introduced SPLAT, a...</td>\n",
       "      <td>Based on the abstract and conclusion, the key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EMPre-training for Multi-party Dialogue Respon...</td>\n",
       "      <td>Abstract Dialogue response generation requires...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Inspired by the tremendous succes...</td>\n",
       "      <td>method. The official implementation of this pa...</td>\n",
       "      <td>experiments have justified the feasibility and...</td>\n",
       "      <td>Related Works 2.1 Pretraining for Response Ge...</td>\n",
       "      <td>Conclusion Most multiparty dialogue corpora ar...</td>\n",
       "      <td>Here are the key contributions of this work ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACLM: A Selective-Denoising based Generative D...</td>\n",
       "      <td>Abstract Complex Named Entity Recognition (NER...</td>\n",
       "      <td>Background and Related Work Complex NER Backgr...</td>\n",
       "      <td>Introduction Named Entity Recognition (NER) is...</td>\n",
       "      <td>method (Zhou and Chen, 2021) (previously evalu...</td>\n",
       "      <td>experiments reveal that the performance of the...</td>\n",
       "      <td>Related Work Complex NER</td>\n",
       "      <td>Conclusion In this paper, we propose ACLM, a n...</td>\n",
       "      <td>The main contributions of this paper are:\\n\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Natural Language to Code Generation in Interac...</td>\n",
       "      <td>Abstract Computational notebooks, such as Jupy...</td>\n",
       "      <td>background material and hints in tutorial note...</td>\n",
       "      <td>Introduction Data science is the process of ex...</td>\n",
       "      <td>Method Length All / pandas JuICe (Agashe et al...</td>\n",
       "      <td>Experiments Models We evaluate PACHINC Oand st...</td>\n",
       "      <td>Related Work Automating Data Science The amoun...</td>\n",
       "      <td>Conclusion In this paper we present ARCADE , a...</td>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subset Retrieval Nearest Neighbor Machine Tran...</td>\n",
       "      <td>Abstract knearest-neighbor machine translation...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Neural machine translation (NMT) ...</td>\n",
       "      <td>method improves the translation performance of...</td>\n",
       "      <td>experiments on the WMT’19 Germanto-English, th...</td>\n",
       "      <td>Related Work The ﬁrst type of examplebased mac...</td>\n",
       "      <td>Conclusion In this paper, we proposed “Subset ...</td>\n",
       "      <td>The main contributions of this paper are:\\n\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MIL-Decoding: Detoxifying Language Models at T...</td>\n",
       "      <td>Abstract Despite advances in large pretrained ...</td>\n",
       "      <td>Background 2.1 Multiple Instance Learning (MIL...</td>\n",
       "      <td>Introduction Trained on huge amount of text co...</td>\n",
       "      <td>method uses a MIL network to score the retriev...</td>\n",
       "      <td>experiments conditioned on two widelyused data...</td>\n",
       "      <td>Related Work Much closely related work has bee...</td>\n",
       "      <td>Conclusion We have introduced MILDecoding, whi...</td>\n",
       "      <td>Based on my analysis of the abstract and concl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dependency resolution at the syntax-semantics ...</td>\n",
       "      <td>Abstract Using psycholinguistic and computatio...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Treating pretrained language mode...</td>\n",
       "      <td>method consists of comparing model probabiliti...</td>\n",
       "      <td>experiments we compare the ability of humans a...</td>\n",
       "      <td>Related work Targeted evaluation of LMs: Targe...</td>\n",
       "      <td>conclusions. Our findings are equally valuabl...</td>\n",
       "      <td>Based on my analysis, the key conclusions from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Open-ended Long Text Generation via Masked Lan...</td>\n",
       "      <td>Abstract Pretrained autoregressive (AR) langua...</td>\n",
       "      <td>background) and the target representation of t...</td>\n",
       "      <td>Introduction Pretrained language models (PLMs)...</td>\n",
       "      <td>method design (§ 3.3), where the model can gen...</td>\n",
       "      <td>Experiments on the storytelling and multiparag...</td>\n",
       "      <td>Related Work Long Text Generation Text generat...</td>\n",
       "      <td>Conclusion This paper explores OpenLTG with NA...</td>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A Method for Studying Semantic Construal in Gr...</td>\n",
       "      <td>Abstract We study semantic construal in gramma...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction There are now several paradigms f...</td>\n",
       "      <td>method can probe the distributional meaning of...</td>\n",
       "      <td>experiments in this paper are available at htt...</td>\n",
       "      <td></td>\n",
       "      <td>conclusion aligns with expectations, given tha...</td>\n",
       "      <td>Here are the key conclusions and contributions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HolographicCCGParsing</td>\n",
       "      <td>Abstract We propose a method for formulating C...</td>\n",
       "      <td>Background and Related Work 2.1 Recursive Comp...</td>\n",
       "      <td>Introduction Combinatory Categorial Grammar (C...</td>\n",
       "      <td>method for formulating CCG as a recursive comp...</td>\n",
       "      <td>Experiments revealed that phraselevel dependen...</td>\n",
       "      <td>Related Work 2.1 Recursive Compositional Model...</td>\n",
       "      <td>Conclusion In this paper, we proposed a novel</td>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Prompts Can Play Lottery Tickets Well: Achievi...</td>\n",
       "      <td>Abstract Thanks to the recent success of Pretr...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Information Extraction (IE) is on...</td>\n",
       "      <td>method namely Lottery Prompt Tuning (LPT). LPT...</td>\n",
       "      <td>experiments demonstrate that LPT consistently ...</td>\n",
       "      <td>Related Work Lifelong Learning Lifelong Learni...</td>\n",
       "      <td>Conclusions In this paper, we study a lifelon...</td>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Retrieve-and-Sample: Document-level Event Argu...</td>\n",
       "      <td>Abstract Recent studies have shown the effecti...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Transforming the large amounts of...</td>\n",
       "      <td>method) with underlines. The best results amon...</td>\n",
       "      <td>experiments on RAMS and WikiEvents, we demonst...</td>\n",
       "      <td>Related Work Documentlevel Event Argument Extr...</td>\n",
       "      <td>Conclusion In this paper, we explore how to de...</td>\n",
       "      <td>In summary, this paper makes the following key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WeCheck: Strong Factual Consistency Checker vi...</td>\n",
       "      <td>Abstract A crucial issue of current text gener...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction The research of text generation h...</td>\n",
       "      <td>method. 2.1 Problem Definition Factual Consist...</td>\n",
       "      <td>experiments on various tasks demonstrate the s...</td>\n",
       "      <td>Related Work Factual Consistency Evaluation Re...</td>\n",
       "      <td>Conclusion In this paper, we propose a weakly ...</td>\n",
       "      <td>Thank you for the detailed abstract on the WeC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AMR-based Network for Aspect-based Sentiment A...</td>\n",
       "      <td>Abstract Aspectbased sentiment analysis (ABSA)...</td>\n",
       "      <td>background “none” relations to the precious ef...</td>\n",
       "      <td>Introduction Recent years have witnessed growi...</td>\n",
       "      <td>approach to explicitly utilize dependency ty...</td>\n",
       "      <td>experiments further verify the significance of...</td>\n",
       "      <td>Related Work Aspectbased Sentiment Analysis Tr...</td>\n",
       "      <td>Conclusion In this paper, we propose APARN , A...</td>\n",
       "      <td>Based on the abstract, this paper makes the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Text Adversarial Purification as Defense again...</td>\n",
       "      <td>Abstract Adversarial purification is a success...</td>\n",
       "      <td>Background of Adversarial Purification A class...</td>\n",
       "      <td>Introduction Adversarial examples (Goodfellow ...</td>\n",
       "      <td>method that focuses on defending against textu...</td>\n",
       "      <td>experiments, we prove that the proposed text a...</td>\n",
       "      <td>Related Work 2.1 Adversarial Attacks in NLP In...</td>\n",
       "      <td>Conclusion and Future Work In this paper, we i...</td>\n",
       "      <td>Based on my understanding, the key contributio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SPEECH: Structured Prediction with Energy-Base...</td>\n",
       "      <td>Abstract Eventcentric structured prediction in...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Structured prediction (Taskar et ...</td>\n",
       "      <td>Methodology 3.1 Preliminaries For structured ...</td>\n",
       "      <td>Experiments on two uniﬁedannotated event datas...</td>\n",
       "      <td>Related Work EventCentric Structured Predictio...</td>\n",
       "      <td>Conclusion and Future Work In this paper, we p...</td>\n",
       "      <td>Based on my analysis of the abstract and concl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Rule By Example: Harnessing Logical Rules for ...</td>\n",
       "      <td>Abstract Classic approaches to content moderat...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Content moderation is a major cha...</td>\n",
       "      <td>method for noisy supervision using rules. Whil...</td>\n",
       "      <td>experiments, we combine the output classes of ...</td>\n",
       "      <td>Related Work There has been active work on det...</td>\n",
       "      <td>Conclusion We introduce Rule By Example, an ex...</td>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What about “em”? How Commercial Machine Transl...</td>\n",
       "      <td>Abstract As 3rdperson pronoun usage shifts to ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Machine translation (MT) is one o...</td>\n",
       "      <td>approach to detect gender issues in realworl...</td>\n",
       "      <td>Experimental Setup Our overall setup consists...</td>\n",
       "      <td>Related Work We review works on gender bias in...</td>\n",
       "      <td>Conclusion In this work, we have investigated ...</td>\n",
       "      <td>Based on my analysis, the key contributions of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What Is Overlap Knowledge in Event Argument Ex...</td>\n",
       "      <td>Abstract The EAE task extracts a structured ev...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Event extraction (EE) is a pivota...</td>\n",
       "      <td>Method As illustrated in Figure 2, APE learns ...</td>\n",
       "      <td>experiments show APE achieves new stateof-thea...</td>\n",
       "      <td>Related Works 5.1 Transfer Learning in EAE Ev...</td>\n",
       "      <td>Conclusion In this work, we first define the s...</td>\n",
       "      <td>Here are the key contributions of this work on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tailor: A Soft-Prompt-Based Approach to Attrib...</td>\n",
       "      <td>Abstract Attributebased Controlled Text Genera...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Attributebased CTG (Zhang et al.,...</td>\n",
       "      <td>method introduces aMultiAttribute Prompt mask ...</td>\n",
       "      <td>Experiments demonstrate that, only requiring 0...</td>\n",
       "      <td>Related Work AttributeBased CTG focuses on gen...</td>\n",
       "      <td>Conclusions In this paper, we explore attribu...</td>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Knowledge of cultural moral norms in large lan...</td>\n",
       "      <td>Abstract Moral norms vary across cultures. A r...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Moral norms vary from culture to ...</td>\n",
       "      <td>method to estimate moral values and found EPLM...</td>\n",
       "      <td>experiments in the previous section for differ...</td>\n",
       "      <td>Related work 2.1 Automated moral inference in ...</td>\n",
       "      <td>conclusion We investigated whether English pre...</td>\n",
       "      <td>Here are a few key points about the contributi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Songs Across Borders: Singable and Controllabl...</td>\n",
       "      <td>Abstract The development of generaldomain neur...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction With the globalization of enterta...</td>\n",
       "      <td>method design. Particularly, the “Pentathlon P...</td>\n",
       "      <td>experiments to test three different prompt met...</td>\n",
       "      <td>Related Work Lyric/Poetry Translation. Designi...</td>\n",
       "      <td>Conclusion We discussed how to obtain singable...</td>\n",
       "      <td>Here are the key contributions of this paper o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fantastic Expressions and Where to Find Them:C...</td>\n",
       "      <td>Abstract Similes occur in the creative context...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Similes are widelyused and stimul...</td>\n",
       "      <td>method ) for the given tenor , then incorporat...</td>\n",
       "      <td>Experiments In this section, we first experime...</td>\n",
       "      <td>Related Work Different from metaphor (Yu and W...</td>\n",
       "      <td>Conclusion In this paper, we introduce a new t...</td>\n",
       "      <td>Based on my understanding of the text, here ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Revealing Single Frame Bias for Video-and-Lang...</td>\n",
       "      <td>Abstract Training an effective videoand-langua...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Video and language are the two pr...</td>\n",
       "      <td>method outperforms late fusion strategies and ...</td>\n",
       "      <td>Experiments 4.1 Downstream Task Setup Textto-V...</td>\n",
       "      <td>Related Work Vision and Language. Vision and l...</td>\n",
       "      <td>conclusion holds for both short 15second MSRVT...</td>\n",
       "      <td>Based on my understanding of the abstract, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Learning with Partial Annotations for Event De...</td>\n",
       "      <td>Abstract Event detection (ED) seeks to discove...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Deep learning models have shown i...</td>\n",
       "      <td>method, to reduce the risk of mistraining on f...</td>\n",
       "      <td>experiments, we conduct realworld annotation e...</td>\n",
       "      <td>Related Work ED and the Partial Annotation Iss...</td>\n",
       "      <td>Conclusion In this study, we investigate the p...</td>\n",
       "      <td>Based on my understanding, this paper makes th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>World-to-Words: Grounded Open Vocabulary Acqui...</td>\n",
       "      <td>Abstract The ability to connect language units...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Language is learned through senso...</td>\n",
       "      <td></td>\n",
       "      <td>experiments and analysis, we demonstrate that ...</td>\n",
       "      <td>Related Work VisionLanguage Mapping Mapping pl...</td>\n",
       "      <td>Conclusion and Future Work The connection betw...</td>\n",
       "      <td>Here are the key contributions of this work on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A Causal Framework to Quantify the Robustness ...</td>\n",
       "      <td>Abstract We have recently witnessed a number o...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Many natural language understandi...</td>\n",
       "      <td>method in the model’s performance, we carry ou...</td>\n",
       "      <td>experiments we generate 500 intervention pairs...</td>\n",
       "      <td>Related Work Causal NLP. Causal inference aims...</td>\n",
       "      <td>Conclusion We developed a framework to disenta...</td>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Evaluating Open-Domain Dialogues in Latent Spa...</td>\n",
       "      <td>Abstract The longstanding oneto-many issue of ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Opendomain dialogue generation is...</td>\n",
       "      <td>method compared with a wide range of baselines...</td>\n",
       "      <td>experiments on two open dialogue datasets. The...</td>\n",
       "      <td>Related Work Referencebased metrics. Reference...</td>\n",
       "      <td>Conclusions In this paper, we propose a novel...</td>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Increasing Diversity While Maintaining Accurac...</td>\n",
       "      <td>Abstract Large language models (LLMs) can be u...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Training custom natural language ...</td>\n",
       "      <td>Method As a generative LLM, we used the textda...</td>\n",
       "      <td>experiments. Corpus of Linguistic Acceptabilit...</td>\n",
       "      <td>Related Work 2.1 Text Data Generation for Mode...</td>\n",
       "      <td>Conclusion In this work, we investigate approa...</td>\n",
       "      <td>Based on the abstract, the key contributions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Pruning Pre-trained Language Models Without Fi...</td>\n",
       "      <td>Abstract To overcome the overparameterized pro...</td>\n",
       "      <td>Background Leta=Wx refer to a fullyconnected l...</td>\n",
       "      <td>Introduction Pretrained Language Models (PLMs)...</td>\n",
       "      <td>method by directly removing unimportant weight...</td>\n",
       "      <td>experiments at various sparsity levels show SM...</td>\n",
       "      <td>Related Work Compressing PLMs for transfer lea...</td>\n",
       "      <td>Conclusion In this paper, we propose a simple ...</td>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>When Does Translation Require Context? A Data-...</td>\n",
       "      <td>Abstract Although proper handling of discourse...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction In order to properly translate di...</td>\n",
       "      <td>method to create a multilingual benchmark test...</td>\n",
       "      <td>experiments, we assessed that this model only ...</td>\n",
       "      <td>Related Work Several works have worked on meas...</td>\n",
       "      <td>Conclusions and Future Work We investigate ty...</td>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Causal Intervention and Counterfactual Reasoni...</td>\n",
       "      <td>Abstract Due to the rapid upgrade of social pl...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction Fake news quietly sneaks into peo...</td>\n",
       "      <td>method used to infer outcomes under hypothetic...</td>\n",
       "      <td>experiments on two realworld benchmark dataset...</td>\n",
       "      <td>Related Work In this section, we review the re...</td>\n",
       "      <td>conclusion that fake news prefers to use loade...</td>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LexSym: Compositionality as Lexical Symmetry</td>\n",
       "      <td>Abstract In tasks like semantic parsing, instr...</td>\n",
       "      <td>Background &amp; Approach We begin with a discussi...</td>\n",
       "      <td>Introduction A central challenge in natural la...</td>\n",
       "      <td>method that improves compositional generalizat...</td>\n",
       "      <td>experiments use more flexible models. We are n...</td>\n",
       "      <td>related work is discussed in Sec. 6. 3 Composi...</td>\n",
       "      <td>Conclusion We have presented LEXSYM, a new dat...</td>\n",
       "      <td>Here are the key points about the contribution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Layer-wise Fusion with Modality Independence M...</td>\n",
       "      <td>Abstract Multimodal emotion recognition has ga...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction The goal of human emotion recogni...</td>\n",
       "      <td>approach utilizes separate supervisions for ...</td>\n",
       "      <td>Experiments and Analysis In this section, we f...</td>\n",
       "      <td>Related Works There are a large volume of rel...</td>\n",
       "      <td>conclusion can be drawn from Figure 1(c) that ...</td>\n",
       "      <td>Here are the key contributions of this paper o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>CASN:Class-Aware Score Network for Textual Adv...</td>\n",
       "      <td>Abstract Adversarial detection aims to detect ...</td>\n",
       "      <td></td>\n",
       "      <td>Introduction It has already become a consensus...</td>\n",
       "      <td>method to implicitly model the data distributi...</td>\n",
       "      <td>experiments are also presented. 6.1 Detection ...</td>\n",
       "      <td>Related work 2.1 Textual Adversarial Attacks C...</td>\n",
       "      <td>Conclusion In this paper, we propose a nearlyp...</td>\n",
       "      <td>Based on the abstract, the main contributions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0   One Cannot Stand for Everyone! Leveraging Mult...   \n",
       "1   SafeConv: Explaining and Correcting Conversati...   \n",
       "2   Detecting and Mitigating Hallucinations in Mac...   \n",
       "3   Explainable Recommendation with Personalized R...   \n",
       "4      Binary and Ternary Natural Language Generation   \n",
       "5   Span-Selective Linear Attention Transformers f...   \n",
       "6   EMPre-training for Multi-party Dialogue Respon...   \n",
       "7   ACLM: A Selective-Denoising based Generative D...   \n",
       "8   Natural Language to Code Generation in Interac...   \n",
       "9   Subset Retrieval Nearest Neighbor Machine Tran...   \n",
       "10  MIL-Decoding: Detoxifying Language Models at T...   \n",
       "11  Dependency resolution at the syntax-semantics ...   \n",
       "12  Open-ended Long Text Generation via Masked Lan...   \n",
       "13  A Method for Studying Semantic Construal in Gr...   \n",
       "14                              HolographicCCGParsing   \n",
       "15  Prompts Can Play Lottery Tickets Well: Achievi...   \n",
       "16  Retrieve-and-Sample: Document-level Event Argu...   \n",
       "17  WeCheck: Strong Factual Consistency Checker vi...   \n",
       "18  AMR-based Network for Aspect-based Sentiment A...   \n",
       "19  Text Adversarial Purification as Defense again...   \n",
       "20  SPEECH: Structured Prediction with Energy-Base...   \n",
       "21  Rule By Example: Harnessing Logical Rules for ...   \n",
       "22  What about “em”? How Commercial Machine Transl...   \n",
       "23  What Is Overlap Knowledge in Event Argument Ex...   \n",
       "24  Tailor: A Soft-Prompt-Based Approach to Attrib...   \n",
       "25  Knowledge of cultural moral norms in large lan...   \n",
       "26  Songs Across Borders: Singable and Controllabl...   \n",
       "27  Fantastic Expressions and Where to Find Them:C...   \n",
       "28  Revealing Single Frame Bias for Video-and-Lang...   \n",
       "29  Learning with Partial Annotations for Event De...   \n",
       "30  World-to-Words: Grounded Open Vocabulary Acqui...   \n",
       "31  A Causal Framework to Quantify the Robustness ...   \n",
       "32  Evaluating Open-Domain Dialogues in Latent Spa...   \n",
       "33  Increasing Diversity While Maintaining Accurac...   \n",
       "34  Pruning Pre-trained Language Models Without Fi...   \n",
       "35  When Does Translation Require Context? A Data-...   \n",
       "36  Causal Intervention and Counterfactual Reasoni...   \n",
       "37       LexSym: Compositionality as Lexical Symmetry   \n",
       "38  Layer-wise Fusion with Modality Independence M...   \n",
       "39  CASN:Class-Aware Score Network for Textual Adv...   \n",
       "\n",
       "                                             Abstract  \\\n",
       "0   Abstract User simulators are agents designed t...   \n",
       "1   Abstract One of the main challenges opendomain...   \n",
       "2   Abstract While the problem of hallucinations i...   \n",
       "3   Abstract Explainable recommendation is a techn...   \n",
       "4   Abstract Ternary and binary neural networks en...   \n",
       "5   Abstract In schemaguided dialogue state tracki...   \n",
       "6   Abstract Dialogue response generation requires...   \n",
       "7   Abstract Complex Named Entity Recognition (NER...   \n",
       "8   Abstract Computational notebooks, such as Jupy...   \n",
       "9   Abstract knearest-neighbor machine translation...   \n",
       "10  Abstract Despite advances in large pretrained ...   \n",
       "11  Abstract Using psycholinguistic and computatio...   \n",
       "12  Abstract Pretrained autoregressive (AR) langua...   \n",
       "13  Abstract We study semantic construal in gramma...   \n",
       "14  Abstract We propose a method for formulating C...   \n",
       "15  Abstract Thanks to the recent success of Pretr...   \n",
       "16  Abstract Recent studies have shown the effecti...   \n",
       "17  Abstract A crucial issue of current text gener...   \n",
       "18  Abstract Aspectbased sentiment analysis (ABSA)...   \n",
       "19  Abstract Adversarial purification is a success...   \n",
       "20  Abstract Eventcentric structured prediction in...   \n",
       "21  Abstract Classic approaches to content moderat...   \n",
       "22  Abstract As 3rdperson pronoun usage shifts to ...   \n",
       "23  Abstract The EAE task extracts a structured ev...   \n",
       "24  Abstract Attributebased Controlled Text Genera...   \n",
       "25  Abstract Moral norms vary across cultures. A r...   \n",
       "26  Abstract The development of generaldomain neur...   \n",
       "27  Abstract Similes occur in the creative context...   \n",
       "28  Abstract Training an effective videoand-langua...   \n",
       "29  Abstract Event detection (ED) seeks to discove...   \n",
       "30  Abstract The ability to connect language units...   \n",
       "31  Abstract We have recently witnessed a number o...   \n",
       "32  Abstract The longstanding oneto-many issue of ...   \n",
       "33  Abstract Large language models (LLMs) can be u...   \n",
       "34  Abstract To overcome the overparameterized pro...   \n",
       "35  Abstract Although proper handling of discourse...   \n",
       "36  Abstract Due to the rapid upgrade of social pl...   \n",
       "37  Abstract In tasks like semantic parsing, instr...   \n",
       "38  Abstract Multimodal emotion recognition has ga...   \n",
       "39  Abstract Adversarial detection aims to detect ...   \n",
       "\n",
       "                                           Background  \\\n",
       "0   Background Dialogue system. Taskoriented dialo...   \n",
       "1                                                       \n",
       "2   Background and Setting In this section, we des...   \n",
       "3                                                       \n",
       "4                                                       \n",
       "5                                                       \n",
       "6                                                       \n",
       "7   Background and Related Work Complex NER Backgr...   \n",
       "8   background material and hints in tutorial note...   \n",
       "9                                                       \n",
       "10  Background 2.1 Multiple Instance Learning (MIL...   \n",
       "11                                                      \n",
       "12  background) and the target representation of t...   \n",
       "13                                                      \n",
       "14  Background and Related Work 2.1 Recursive Comp...   \n",
       "15                                                      \n",
       "16                                                      \n",
       "17                                                      \n",
       "18  background “none” relations to the precious ef...   \n",
       "19  Background of Adversarial Purification A class...   \n",
       "20                                                      \n",
       "21                                                      \n",
       "22                                                      \n",
       "23                                                      \n",
       "24                                                      \n",
       "25                                                      \n",
       "26                                                      \n",
       "27                                                      \n",
       "28                                                      \n",
       "29                                                      \n",
       "30                                                      \n",
       "31                                                      \n",
       "32                                                      \n",
       "33                                                      \n",
       "34  Background Leta=Wx refer to a fullyconnected l...   \n",
       "35                                                      \n",
       "36                                                      \n",
       "37  Background & Approach We begin with a discussi...   \n",
       "38                                                      \n",
       "39                                                      \n",
       "\n",
       "                                         Introduction  \\\n",
       "0   Introduction Taskoriented dialogue systems aim...   \n",
       "1   Introduction Safety of artificial intelligence...   \n",
       "2   Introduction Hallucinations in machine transla...   \n",
       "3   Introduction Recent years have witnessed a gro...   \n",
       "4   Introduction Generative pretrained transformer...   \n",
       "5   Introduction Dialogue State Tracking (DST) ref...   \n",
       "6   Introduction Inspired by the tremendous succes...   \n",
       "7   Introduction Named Entity Recognition (NER) is...   \n",
       "8   Introduction Data science is the process of ex...   \n",
       "9   Introduction Neural machine translation (NMT) ...   \n",
       "10  Introduction Trained on huge amount of text co...   \n",
       "11  Introduction Treating pretrained language mode...   \n",
       "12  Introduction Pretrained language models (PLMs)...   \n",
       "13  Introduction There are now several paradigms f...   \n",
       "14  Introduction Combinatory Categorial Grammar (C...   \n",
       "15  Introduction Information Extraction (IE) is on...   \n",
       "16  Introduction Transforming the large amounts of...   \n",
       "17  Introduction The research of text generation h...   \n",
       "18  Introduction Recent years have witnessed growi...   \n",
       "19  Introduction Adversarial examples (Goodfellow ...   \n",
       "20  Introduction Structured prediction (Taskar et ...   \n",
       "21  Introduction Content moderation is a major cha...   \n",
       "22  Introduction Machine translation (MT) is one o...   \n",
       "23  Introduction Event extraction (EE) is a pivota...   \n",
       "24  Introduction Attributebased CTG (Zhang et al.,...   \n",
       "25  Introduction Moral norms vary from culture to ...   \n",
       "26  Introduction With the globalization of enterta...   \n",
       "27  Introduction Similes are widelyused and stimul...   \n",
       "28  Introduction Video and language are the two pr...   \n",
       "29  Introduction Deep learning models have shown i...   \n",
       "30  Introduction Language is learned through senso...   \n",
       "31  Introduction Many natural language understandi...   \n",
       "32  Introduction Opendomain dialogue generation is...   \n",
       "33  Introduction Training custom natural language ...   \n",
       "34  Introduction Pretrained Language Models (PLMs)...   \n",
       "35  Introduction In order to properly translate di...   \n",
       "36  Introduction Fake news quietly sneaks into peo...   \n",
       "37  Introduction A central challenge in natural la...   \n",
       "38  Introduction The goal of human emotion recogni...   \n",
       "39  Introduction It has already become a consensus...   \n",
       "\n",
       "                                          Methodology  \\\n",
       "0   method called MUST adaptive that balances i) t...   \n",
       "1   method is onetime checking and rewriting—direc...   \n",
       "2   method that evaluates the percentage of the so...   \n",
       "3   method has been employed to identify and selec...   \n",
       "4   method to natural language generation tasks an...   \n",
       "5     approach significantly improves upon existin...   \n",
       "6   method. The official implementation of this pa...   \n",
       "7   method (Zhou and Chen, 2021) (previously evalu...   \n",
       "8   Method Length All / pandas JuICe (Agashe et al...   \n",
       "9   method improves the translation performance of...   \n",
       "10  method uses a MIL network to score the retriev...   \n",
       "11  method consists of comparing model probabiliti...   \n",
       "12  method design (§ 3.3), where the model can gen...   \n",
       "13  method can probe the distributional meaning of...   \n",
       "14  method for formulating CCG as a recursive comp...   \n",
       "15  method namely Lottery Prompt Tuning (LPT). LPT...   \n",
       "16  method) with underlines. The best results amon...   \n",
       "17  method. 2.1 Problem Definition Factual Consist...   \n",
       "18    approach to explicitly utilize dependency ty...   \n",
       "19  method that focuses on defending against textu...   \n",
       "20   Methodology 3.1 Preliminaries For structured ...   \n",
       "21  method for noisy supervision using rules. Whil...   \n",
       "22    approach to detect gender issues in realworl...   \n",
       "23  Method As illustrated in Figure 2, APE learns ...   \n",
       "24  method introduces aMultiAttribute Prompt mask ...   \n",
       "25  method to estimate moral values and found EPLM...   \n",
       "26  method design. Particularly, the “Pentathlon P...   \n",
       "27  method ) for the given tenor , then incorporat...   \n",
       "28  method outperforms late fusion strategies and ...   \n",
       "29  method, to reduce the risk of mistraining on f...   \n",
       "30                                                      \n",
       "31  method in the model’s performance, we carry ou...   \n",
       "32  method compared with a wide range of baselines...   \n",
       "33  Method As a generative LLM, we used the textda...   \n",
       "34  method by directly removing unimportant weight...   \n",
       "35  method to create a multilingual benchmark test...   \n",
       "36  method used to infer outcomes under hypothetic...   \n",
       "37  method that improves compositional generalizat...   \n",
       "38    approach utilizes separate supervisions for ...   \n",
       "39  method to implicitly model the data distributi...   \n",
       "\n",
       "                                          Experiments  \\\n",
       "0   experiments, we observed that the dialogue sys...   \n",
       "1   Experiments show that the detected unsafe beha...   \n",
       "2   experiments.1 1 Introduction Hallucinations in...   \n",
       "3   experiments on three datasets show that our mo...   \n",
       "4   Experiments In this section, we evaluate the e...   \n",
       "5   experiments on the SchemaGuided Dialogue (SGD)...   \n",
       "6   experiments have justified the feasibility and...   \n",
       "7   experiments reveal that the performance of the...   \n",
       "8   Experiments Models We evaluate PACHINC Oand st...   \n",
       "9   experiments on the WMT’19 Germanto-English, th...   \n",
       "10  experiments conditioned on two widelyused data...   \n",
       "11  experiments we compare the ability of humans a...   \n",
       "12  Experiments on the storytelling and multiparag...   \n",
       "13  experiments in this paper are available at htt...   \n",
       "14  Experiments revealed that phraselevel dependen...   \n",
       "15  experiments demonstrate that LPT consistently ...   \n",
       "16  experiments on RAMS and WikiEvents, we demonst...   \n",
       "17  experiments on various tasks demonstrate the s...   \n",
       "18  experiments further verify the significance of...   \n",
       "19  experiments, we prove that the proposed text a...   \n",
       "20  Experiments on two uniﬁedannotated event datas...   \n",
       "21  experiments, we combine the output classes of ...   \n",
       "22   Experimental Setup Our overall setup consists...   \n",
       "23  experiments show APE achieves new stateof-thea...   \n",
       "24  Experiments demonstrate that, only requiring 0...   \n",
       "25  experiments in the previous section for differ...   \n",
       "26  experiments to test three different prompt met...   \n",
       "27  Experiments In this section, we first experime...   \n",
       "28  Experiments 4.1 Downstream Task Setup Textto-V...   \n",
       "29  experiments, we conduct realworld annotation e...   \n",
       "30  experiments and analysis, we demonstrate that ...   \n",
       "31  experiments we generate 500 intervention pairs...   \n",
       "32  experiments on two open dialogue datasets. The...   \n",
       "33  experiments. Corpus of Linguistic Acceptabilit...   \n",
       "34  experiments at various sparsity levels show SM...   \n",
       "35  experiments, we assessed that this model only ...   \n",
       "36  experiments on two realworld benchmark dataset...   \n",
       "37  experiments use more flexible models. We are n...   \n",
       "38  Experiments and Analysis In this section, we f...   \n",
       "39  experiments are also presented. 6.1 Detection ...   \n",
       "\n",
       "                                         Related Work  \\\n",
       "0                                                       \n",
       "1   Related Work Dialogue Safety Datasets Datasets...   \n",
       "2                                                       \n",
       "3   Related Work 2.1 Explainable Recommendation wi...   \n",
       "4   Related Work Quantization has long been studie...   \n",
       "5   Related Work Extractive DST. Following the tra...   \n",
       "6    Related Works 2.1 Pretraining for Response Ge...   \n",
       "7                           Related Work Complex NER    \n",
       "8   Related Work Automating Data Science The amoun...   \n",
       "9   Related Work The ﬁrst type of examplebased mac...   \n",
       "10  Related Work Much closely related work has bee...   \n",
       "11  Related work Targeted evaluation of LMs: Targe...   \n",
       "12  Related Work Long Text Generation Text generat...   \n",
       "13                                                      \n",
       "14  Related Work 2.1 Recursive Compositional Model...   \n",
       "15  Related Work Lifelong Learning Lifelong Learni...   \n",
       "16  Related Work Documentlevel Event Argument Extr...   \n",
       "17  Related Work Factual Consistency Evaluation Re...   \n",
       "18  Related Work Aspectbased Sentiment Analysis Tr...   \n",
       "19  Related Work 2.1 Adversarial Attacks in NLP In...   \n",
       "20  Related Work EventCentric Structured Predictio...   \n",
       "21  Related Work There has been active work on det...   \n",
       "22  Related Work We review works on gender bias in...   \n",
       "23   Related Works 5.1 Transfer Learning in EAE Ev...   \n",
       "24  Related Work AttributeBased CTG focuses on gen...   \n",
       "25  Related work 2.1 Automated moral inference in ...   \n",
       "26  Related Work Lyric/Poetry Translation. Designi...   \n",
       "27  Related Work Different from metaphor (Yu and W...   \n",
       "28  Related Work Vision and Language. Vision and l...   \n",
       "29  Related Work ED and the Partial Annotation Iss...   \n",
       "30  Related Work VisionLanguage Mapping Mapping pl...   \n",
       "31  Related Work Causal NLP. Causal inference aims...   \n",
       "32  Related Work Referencebased metrics. Reference...   \n",
       "33  Related Work 2.1 Text Data Generation for Mode...   \n",
       "34  Related Work Compressing PLMs for transfer lea...   \n",
       "35  Related Work Several works have worked on meas...   \n",
       "36  Related Work In this section, we review the re...   \n",
       "37  related work is discussed in Sec. 6. 3 Composi...   \n",
       "38   Related Works There are a large volume of rel...   \n",
       "39  Related work 2.1 Textual Adversarial Attacks C...   \n",
       "\n",
       "                                           Conclusion  \\\n",
       "0   Conclusion In this paper, we propose a framewo...   \n",
       "1   Conclusion In this paper, we study how to expl...   \n",
       "2    Conclusions We start by asking how far we can...   \n",
       "3   Conclusion In this paper, we propose a novel m...   \n",
       "4   Conclusion We have demonstrated high accuracy ...   \n",
       "5   Conclusion In this work we introduced SPLAT, a...   \n",
       "6   Conclusion Most multiparty dialogue corpora ar...   \n",
       "7   Conclusion In this paper, we propose ACLM, a n...   \n",
       "8   Conclusion In this paper we present ARCADE , a...   \n",
       "9   Conclusion In this paper, we proposed “Subset ...   \n",
       "10  Conclusion We have introduced MILDecoding, whi...   \n",
       "11   conclusions. Our findings are equally valuabl...   \n",
       "12  Conclusion This paper explores OpenLTG with NA...   \n",
       "13  conclusion aligns with expectations, given tha...   \n",
       "14     Conclusion In this paper, we proposed a novel    \n",
       "15   Conclusions In this paper, we study a lifelon...   \n",
       "16  Conclusion In this paper, we explore how to de...   \n",
       "17  Conclusion In this paper, we propose a weakly ...   \n",
       "18  Conclusion In this paper, we propose APARN , A...   \n",
       "19  Conclusion and Future Work In this paper, we i...   \n",
       "20  Conclusion and Future Work In this paper, we p...   \n",
       "21  Conclusion We introduce Rule By Example, an ex...   \n",
       "22  Conclusion In this work, we have investigated ...   \n",
       "23  Conclusion In this work, we first define the s...   \n",
       "24   Conclusions In this paper, we explore attribu...   \n",
       "25  conclusion We investigated whether English pre...   \n",
       "26  Conclusion We discussed how to obtain singable...   \n",
       "27  Conclusion In this paper, we introduce a new t...   \n",
       "28  conclusion holds for both short 15second MSRVT...   \n",
       "29  Conclusion In this study, we investigate the p...   \n",
       "30  Conclusion and Future Work The connection betw...   \n",
       "31  Conclusion We developed a framework to disenta...   \n",
       "32   Conclusions In this paper, we propose a novel...   \n",
       "33  Conclusion In this work, we investigate approa...   \n",
       "34  Conclusion In this paper, we propose a simple ...   \n",
       "35   Conclusions and Future Work We investigate ty...   \n",
       "36  conclusion that fake news prefers to use loade...   \n",
       "37  Conclusion We have presented LEXSYM, a new dat...   \n",
       "38  conclusion can be drawn from Figure 1(c) that ...   \n",
       "39  Conclusion In this paper, we propose a nearlyp...   \n",
       "\n",
       "                                 CLAUDE_ExtractedInfo  \n",
       "0   Based on the abstract, the key contributions a...  \n",
       "1   Based on the abstract, this paper makes the fo...  \n",
       "2   Based on the conclusions section, the key poin...  \n",
       "3   Based on the abstract, the key contributions o...  \n",
       "4   The key contributions of this work are:\\n\\n1. ...  \n",
       "5   Based on the abstract and conclusion, the key ...  \n",
       "6   Here are the key contributions of this work ba...  \n",
       "7   The main contributions of this paper are:\\n\\n1...  \n",
       "8   Here are the key contributions of this paper:\\...  \n",
       "9   The main contributions of this paper are:\\n\\n1...  \n",
       "10  Based on my analysis of the abstract and concl...  \n",
       "11  Based on my analysis, the key conclusions from...  \n",
       "12  Here are the key contributions of this paper:\\...  \n",
       "13  Here are the key conclusions and contributions...  \n",
       "14  The key contributions of this paper are:\\n\\n1....  \n",
       "15  The key contributions of this paper are:\\n\\n1....  \n",
       "16  In summary, this paper makes the following key...  \n",
       "17  Thank you for the detailed abstract on the WeC...  \n",
       "18  Based on the abstract, this paper makes the fo...  \n",
       "19  Based on my understanding, the key contributio...  \n",
       "20  Based on my analysis of the abstract and concl...  \n",
       "21  Based on the abstract, the key contributions o...  \n",
       "22  Based on my analysis, the key contributions of...  \n",
       "23  Here are the key contributions of this work on...  \n",
       "24  The key contributions of this paper are:\\n\\n1....  \n",
       "25  Here are a few key points about the contributi...  \n",
       "26  Here are the key contributions of this paper o...  \n",
       "27  Based on my understanding of the text, here ar...  \n",
       "28  Based on my understanding of the abstract, the...  \n",
       "29  Based on my understanding, this paper makes th...  \n",
       "30  Here are the key contributions of this work on...  \n",
       "31  The key contributions of this work are:\\n\\n1. ...  \n",
       "32  Here are the key contributions of this paper:\\...  \n",
       "33  Based on the abstract, the key contributions a...  \n",
       "34  Based on the abstract, the key contributions o...  \n",
       "35  The key contributions of this work are:\\n\\n1. ...  \n",
       "36  Based on the abstract, the key contributions o...  \n",
       "37  Here are the key points about the contribution...  \n",
       "38  Here are the key contributions of this paper o...  \n",
       "39  Based on the abstract, the main contributions ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50320646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to Claude_Insights.json\n",
      "Data saved to C:\\Users\\harsh\\Downloads\\INFO_5731\\Claude_Insights.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame containing the data you want to save\n",
    "\n",
    "# Function to clean problematic characters\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        # Encode text to handle surrogate characters\n",
    "        cleaned_text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "        return cleaned_text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply clean_text function to all columns of DataFrame\n",
    "df_cleaned = df.applymap(clean_text)\n",
    "\n",
    "# Export cleaned DataFrame to JSON\n",
    "json_file_path = 'Claude_Insights.json'\n",
    "df_cleaned.to_json(json_file_path, orient='records', force_ascii=False)\n",
    "print(f\"Data saved to {json_file_path}\")\n",
    "\n",
    "\n",
    "# Specify the file path for saving the JSON file locally\n",
    "json_file_path = r'C:\\Users\\harsh\\Downloads\\INFO_5731\\Claude_Insights.json'\n",
    "\n",
    "# Export cleaned DataFrame to JSON with specified file path\n",
    "df_cleaned.to_json(json_file_path, orient='records', force_ascii=False)\n",
    "\n",
    "print(f\"Data saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736a75d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLAUDE_ExtractedInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on the abstract, the key contributions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Based on the abstract, this paper makes the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Based on the conclusions section, the key poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Based on the abstract and conclusion, the key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here are the key contributions of this work ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The main contributions of this paper are:\\n\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The main contributions of this paper are:\\n\\n1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Based on my analysis of the abstract and concl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Based on my analysis, the key conclusions from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Here are the key conclusions and contributions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>In summary, this paper makes the following key...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thank you for the detailed abstract on the WeC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Based on the abstract, this paper makes the fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Based on my understanding, the key contributio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Based on my analysis of the abstract and concl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Based on my analysis, the key contributions of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Here are the key contributions of this work on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The key contributions of this paper are:\\n\\n1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Here are a few key points about the contributi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Here are the key contributions of this paper o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Based on my understanding of the text, here ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Based on my understanding of the abstract, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Based on my understanding, this paper makes th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Here are the key contributions of this work on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Here are the key contributions of this paper:\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Based on the abstract, the key contributions a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The key contributions of this work are:\\n\\n1. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Based on the abstract, the key contributions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Here are the key points about the contribution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Here are the key contributions of this paper o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Based on the abstract, the main contributions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 CLAUDE_ExtractedInfo\n",
       "0   Based on the abstract, the key contributions a...\n",
       "1   Based on the abstract, this paper makes the fo...\n",
       "2   Based on the conclusions section, the key poin...\n",
       "3   Based on the abstract, the key contributions o...\n",
       "4   The key contributions of this work are:\\n\\n1. ...\n",
       "5   Based on the abstract and conclusion, the key ...\n",
       "6   Here are the key contributions of this work ba...\n",
       "7   The main contributions of this paper are:\\n\\n1...\n",
       "8   Here are the key contributions of this paper:\\...\n",
       "9   The main contributions of this paper are:\\n\\n1...\n",
       "10  Based on my analysis of the abstract and concl...\n",
       "11  Based on my analysis, the key conclusions from...\n",
       "12  Here are the key contributions of this paper:\\...\n",
       "13  Here are the key conclusions and contributions...\n",
       "14  The key contributions of this paper are:\\n\\n1....\n",
       "15  The key contributions of this paper are:\\n\\n1....\n",
       "16  In summary, this paper makes the following key...\n",
       "17  Thank you for the detailed abstract on the WeC...\n",
       "18  Based on the abstract, this paper makes the fo...\n",
       "19  Based on my understanding, the key contributio...\n",
       "20  Based on my analysis of the abstract and concl...\n",
       "21  Based on the abstract, the key contributions o...\n",
       "22  Based on my analysis, the key contributions of...\n",
       "23  Here are the key contributions of this work on...\n",
       "24  The key contributions of this paper are:\\n\\n1....\n",
       "25  Here are a few key points about the contributi...\n",
       "26  Here are the key contributions of this paper o...\n",
       "27  Based on my understanding of the text, here ar...\n",
       "28  Based on my understanding of the abstract, the...\n",
       "29  Based on my understanding, this paper makes th...\n",
       "30  Here are the key contributions of this work on...\n",
       "31  The key contributions of this work are:\\n\\n1. ...\n",
       "32  Here are the key contributions of this paper:\\...\n",
       "33  Based on the abstract, the key contributions a...\n",
       "34  Based on the abstract, the key contributions o...\n",
       "35  The key contributions of this work are:\\n\\n1. ...\n",
       "36  Based on the abstract, the key contributions o...\n",
       "37  Here are the key points about the contribution...\n",
       "38  Here are the key contributions of this paper o...\n",
       "39  Based on the abstract, the main contributions ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "newdf = pd.DataFrame()\n",
    "newdf['CLAUDE_ExtractedInfo'] = df['CLAUDE_ExtractedInfo']\n",
    "\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd36e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80893a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b70150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126b761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9e63a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "cell_type = type(newdf.at[0, 'CLAUDE_ExtractedInfo'])\n",
    "print(cell_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7ecfa",
   "metadata": {},
   "source": [
    "##  END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07de0dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract User simulators are agents designed to imitate human users; recent advances have found that Taskoriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a suboptimal ToD system if it is tailored to only one ad hoc user simulator, since human users can behave differently. In this paper, we propose a framework called MUST1to optimize ToD systems via leveraging Multiple UserSimula Tors. The main challenges of implementing the MUST are 1) how to adaptively determine which user simulator to interact with the ToD system at each optimization step, since the ToD system might be overfitted to some specific user simulators, and simultaneously underfitted to some others; 2) how to avoid catastrophic forgetting of the adaption for a simulator that is not selected for several consecutive optimization steps. To tackle these challenges, we formulate MUST as a Multiarmed bandits (MAB) problem and provide a method called MUST adaptive that balances i) the boosting adaption for adaptive interactions between different user simulators and the ToD system andii) the uniform adaption to avoid the catastrophic forgetting issue. With both automatic evaluations and human evaluations, our experimental results on MultiWOZ show that the dialogue system trained by MUST achieves a better performance than those trained by a single user simulator. It also has a better generalization ability when testing with unseen user simulators. 1 Introduction Taskoriented dialogue systems aim to help users accomplish their various tasks (e.g., restaurant reservations) through natural language conversations. Training taskoriented dialogue systems in 1The code is available at https://github.com/ kiseliu/must .supervised learning approaches often requires a large amount of expertlabeled dialogues, however collecting these dialogues is usually expensive and timeconsuming. Moreover, even with a large amount of dialogue data, some dialogue states may not be explored sufficiently for dialogue systems2 (Li et al., 2016b). To this end, many researchers try to build user simulators to mimic human users for generating reasonable and natural conversations. By using a user simulator and sampling user goals, we can train the dialogue system from scratch with reinforcement learning (RL) algorithms. Previous works tend to design better user simulator models (Schatzmann et al., 2007; Asri et al., 2016; Gur et al., 2018; Kreyssig et al., 2018; Lin et al., 2021). Especially, Shi et al. (2019) builds various user simulators and analyzes the behavior of each user simulator in the popular restaurant search task from MultiWOZ (Budzianowski et al., 2018). In real scenarios, dialogue systems need to face various types of users. A single ad hoc user simulator can only represent one or a group of users, while other users might be underrepresented. Instead of choosing the bestperforming one from many dialogue systems trained by different single user simulators, we believe that it is worth trying to train a dialogue system by leveraging all user simulators simultaneously. In this paper, we propose a framework called MUST to utilize Multiple UserSimula Tors simultaneously to obtain a better system agent. There exist several simple ways to implement the MUST framework, including a merging strategy, a continual reinforcement learning (CRL) strategy, and a uniform adaption strategy, namely MUST merging , MUST CRL, and MUST uniform respectively (See §3.2). However, none of them could effectively tackle the challenges: 1) how to efficiently leverage multiple user simulators to train the dialogue 2We use the dialogue systems to refer to the taskoriented dialogue systems for simplicity in this paper.1system since the system might be easily overfitted to some specific user simulators and simultaneously underfitted to some others, and 2) it should avoid a catastrophic forgetting issue. To tackle them effectively, we first formulate the problem as a Multiarmed bandits (MAB) problem (Auer et al., 2002); similar to the exploitation vs exploration tradeoff, specifying multiple user simulators should trade off a boosting adaption (tackling challenge 1) and a uniform adaption (tackling challenge 2), see §4.1 for more details. Then we implement a new method called MUST adaptive to utilize an adaptivelyupdated distribution among all user simulators to sample them when training the dialogue system in the RL training. Our contributions are threefold: (1) To the best of our knowledge, our proposed MUST is the first developed work to improve the dialogue system by using multiple user simulators simultaneously; (2) We design several ways to implement the MUST. Especially, we formulate MUST as a Multiarmed bandits (MAB) problem, based on which we provide a novel method MUST adaptive ; and (3) The results show that dialogue systems trained with MUST consistently outperform those trained with a single user simulator through automatic and human evaluations, showing its potential for robustness to the diversity of user simulators. Importantly, it significantly improves the performance of the dialogue system tested on outof-domain evaluation. Moreover, our results show that our method MUST adaptive can efficiently leverage multiple user simulators to train the dialogue system in terms of convergence speed. 2 Background Dialogue system. Taskoriented dialogue systems aim to help users accomplish various tasks such as restaurant reservations through natural language conversations. Researchers usually divide the taskoriented dialogue systems into four modules (Wen et al., 2017; Ham et al., 2020; Peng et al., 2021): Natural Language Understanding (NLU) (Liu and Lane, 2016) that first comprehends user’s intents and extracts the slotsvalues pairs, Dialog State Tracker (DST) (Williams et al., 2013) that tracks the values of slots, Dialog Policy Learning (POL) (Peng et al., 2017, 2018) that decides the dialog actions, and Natural Language Generation (NLG) (Wen et al., 2015; Peng et al., 2020) that translates the dialog actions into a naturallanguageform. The DST module and the POL module usually are collectively referred to as the dialogue manager (DM) (Chen et al., 2017). These different modules can be trained independently or jointly in an endto-end manner (Wen et al., 2017; Liu and Lane, 2018; Ham et al., 2020; Peng et al., 2021). User simulator. The user simulator is also an agent but plays a user role. Different from dialogue systems, the user agent has a goal describing a target entity (e.g., a restaurant at a specific location) and should express its goal completely in an organized way by interacting with the system agent (Takanobu et al., 2020). Therefore, besides the modules of NLU, DM, and NLG like dialogue systems, the user agent should have another module called Goal Generator (Kreyssig et al., 2018), which is responsible for generating the user’s goal. Building a user simulator could usually use an agendabased approach (Schatzmann et al., 2007; Schatzmann and Young, 2009) designing handcrafted rules to mimic user behaviors or a modelbased approach such as neural networks (Asri et al., 2016; Kreyssig et al., 2018; Gur et al., 2018) learned on a corpus of dialogues. Training dialogue systems with a user simulator. To start a dialogue, a user agent will have an initial goal from its Goal Generator and then expresses its goal in natural languages. However, users’ goals are invisible to the system agent. Then the system agent tends to gradually understand the users’ utterances, query the database to find entities, and provide useful information to accomplish users’ task. When the database result returned by the system agent is empty, the user agent should learn to compromise and change its goal with the help of Goal Generator. When the dialogue ends, the user simulator will reward the system agent according to if it accomplishes the task. Then we could use the reward to update the system agent with RL algorithms (Tseng et al., 2021). 3 MUST: a Framework to Leverage Multiple User SimulaTors 3.1 Motivations to Use Multiple Simulators User simulators behave differently. Shi et al. (2019) implement six user simulators (AgenT, AgenR, AgenG, RNNT, RNNR, RNN3) with both 3Here we rename the user simulators of SLT, SLR, and SLE in Shi et al. (2019) as RNNT, RNNR, RNN for emphasizing the model structure of their DM modules.2(a) Success rates of different systems. (b) Dialog act distributions of different user simulators. Figure 1: (a) is the heat map on the success rates of system agents tested by different user simulators on 200 dialogues. (b) shows the dialog act distributions of Agendabased User Simulators (ABUS) and Neural networksbased User Simulators (NUS) provided by Shi et al. (2019). There exist seven user dialog acts annotated in the restaurant search task from MultiWOZ, as shown on the Yaxis. agendabased methods and neural networksbased methods on the popular restaurant search task from MultiWOZ (Budzianowski et al., 2018). From their experiments, we observed that the dialogue systems trained by different user simulators vary in their performances (i.e., the success rates tested by the same user simulators). For example, when interacting with the user simulator of AgenT, the success rates of the system agents trained by Agendabased user simulators (i.e., AgenT, AgenR, AgenG) are much higher than those of the system agents trained by RNNbased user simulators (i.e., RNNT, RNNR, RNN), see Fig. 1(a). The reason might be that these user simulators (i.e., with either handcrafted rules or datadriven learning in their DM modules) have different user dialog act distributions4(see Fig. 1(b)) which determine the dialogue state space explored by the dialogue system. One cannot stand for everyone. Users might behave differently, one could design different user simulators with specific user dialog act distributions, see Shi et al. (2019). A single user simulator learned on a taskoriented dialogue corpus can just represent one or a group of users, while the dialogue system needs to accomplish tasks from various human users in real scenarios. We argue that it is beneficial to utilize all different user simulators to train the dialogue system. By leveraging multiple user simulators that have different user dialog act distributions, the dialogue systems can explore a larger dialogue state space, which might 4The dialogue policy learning module is essential in both dialogue systems and user simulators. A policy module corresponds to a dialog act distribution since it decides to take which dialog act to respond to the current dialogue state. The user dialog act distribution behind a user simulator determines the diversity of the dialogue state space explored by dialogue systems; therefore it might affect the system performances.improve the ability of the learned dialogue system. 3.2 Some Preliminary Proposals for MUST We propose a framework called MUST, the core idea of which is to train a better dialogue system by leveraging Multiple UserSimula Tors simultaneously. There are several simple ways to implement our MUST, including a merging strategy (MUST merging ), aContinual Reinforcement Learningstrategy (MUST CRL), and a uniform adaption strategy (MUST uniform ). (I) MUST merging first samples some dialogues from each user simulator and the corresponding dialogue system trained by this simulator. Then it combines the collected dialogues to train a new user simulator for ensembling different user dialog act distributions. Finally, it uses this new user simulator to train the dialogue system with RL. (II) MUST CRL5treats each user simulator as an independent RL environment. It moves the trained system agent to another one (i.e., let the system agent interact with another user simulator) if the system has converged in the current environment. (III) MUST uniform allows the system agent have chances to interact with all user simulators simultaneously. Different from MUST CRL, MUST uniform puts all user simulators in a single RL environment and adopts the simplest way to specify different user simulators to train the dialogue system, which is to pick a user simulator among all user simulators with a uniform distribution for each iteration in the RL training. 5Continual Reinforcement Learning (CRL) (Khetarpal et al., 2020) is a sequential learning paradigm for training an agent with RL algorithms.3dynamic avoiding forgettingefficiencyadaption catastrophic MUST merging × × × MUST CRL × × × MUST uniform × ✓ × MUST adaptive ✓ ✓ ✓ Table 1: The comparison of different strategies for leveraging multiple user simulators. Challenges to leverage multiple user simulators. It is difficult to adaptively adjust weights of user simulators during training in MUST merging . Since the proportions of dialogues from each user simulator are fixed in MUST merging , user simulators might be welladapted and others might not. The MUST CRL strategy has a problem of catastrophic forgetting (Khetarpal et al., 2020) and would be sensitive to the order of different user agents interacting with the dialogue system, which might result in obtaining a suboptimal dialogue system. As Shi et al. (2019) shows, the system agents trained by different user simulators have different convergence speeds and converged performances. Namely, the system agent might be easily fitted to some user simulators but might be hardly fitted to others. A uniform distribution for the simulator selection under MUST uniform will result in inefficient training, since it would be unnecessary to assign the many training costs for easilyadapted user simulators. Overall, the challenging problems under MUST are 1) how to efficiently leverage multiple user simulators to train the system agent, and 2) avoiding the catastrophic forgetting issue. 4 MUST as a MAB Problem To tackle the challenges in MUST, we first formulate MUST as a Multiarmed bandit (MAB) problem, see §4.1. In §4.2, we propose a method called MUST adaptive to use an adaptivelyupdated distribution to replace the uniform distribution under the MUST uniform for accelerating the MUST training. We briefly compare these different implementations of MUST in Tab. 1. 4.1 Formulating MUST as a MAB Problem Adaptively specifying user simulators to train dialogue systems reminds us of a similar concept in machine learning, called boosting (Zhou, 2012). From a boosting point of view, one should increase the weights of weaklyperforming data examples and decrease the weights for wellperforming ones.In MUST, we accordingly assume that it should reduce the interactions between the dialogue system and those user simulators that the system has performed well; and meanwhile increase the interactions between the system and other user simulators that the system performs poorly. We refer to this strategy as boosting adaption . Meanwhile, we should also give some chances to all user simulators to relieve the catastrophic forgetting issue. We refer to this as uniform adaption . Such a tradeoff between boosting adaption and uniform adaption is similar to the the exploitation vs exploration tradeoff existing in the Multiarmed bandit (MAB) problem (Auer et al., 2002). Here, we interpret MUST as a MAB problem. We treat each user simulator as an arm. Suppose there are Karms (simulators), and each arm ihas a fixed but unknown reward distribution Riwith an expectation µi. At each time step t= 1,2, ..., T , one must choose one of these Karms. We denote the arm pulled at time step tasit∈ {1, ..., K}. After pulling an arm, it receives a reward xitdrawn from the arm’s underlying reward distribution. The decision maker’s objective is to maximize the cumulative expected reward over the time horizon T/summationdisplay t=1E[xit] =T/summationdisplay t=1µit. (1) In MUST, the reward received in each armpulling step refers to the possible performance gain of the dialogue system after it interacts with a selected user simulator. A significant difference between the standard MAB problem and MUST is that the reward expectation of a user simulator (arm) in MUST is not static; it changes over time. For example, by consecutively interacting with the same user simulator, the performance gain (reward) of the system will decay since the system might be in saturation or overfitting to this simulator. Moreover, the performance gain of the system after interacting with a simulator might increase if the simulator has not been selected for a period. To deal with this difference , we should tailor the solution of MAB to the MUST framework. 4.2 Training with MUST adaptive To solve this MAB problem in MUST, we implement a method called MUST adaptive with a twophase procedure, as presented in Algorithm 1. MUST adaptive specifies user simulators in a4Algorithm 1: Implement MUST adaptive with the modified UCB1 algorithm Input: K fixed User simulators U={U1, U2,···UK}and the values of hyperparameters Twarmup, T, e, d, τ ; 1Initialization : randomly initialize System agent S; 2Initialization : initialize the simulator sampling distribution pas a uniform distribution. 3(1)Warmup phase: 4fort= 0, ..., T warmup−1do 5 sample a simulator UjinUw.r.t. the distribution p; 6 synthesize a new dialogue using the system agent Sand the sampled Uj; 7 use the reward obtained for the dialogue to update Swith a RL algorithm; 8(2)Adaptive phase: 9fort= 0, ..., T−1do 10 ift%e== 0 then 11 forj= 1, ..., K do 12 evaluate the performance i.e. the success rate ¯xjof the agent Sby letting it interact dtimes with the simulator Uj; 13 update pbased on these success rates {¯x1, ...,¯xK}(see Eq. 2, Eq. 3, and Eq. 4); 14 else 15 sample a simulator UjinUw.r.t. the distribution p; 16 synthesizing a new dialogue using the system agent Sand the sampled Uj; 17 use the reward obtained for the dialogue to update Swith a RL algorithm; Output: The learned dialogue system S. uniform distribution, similar to the UCB16algorithm, to train the dialogue system Sin the first Twarmup steps (i.e., in the warmup phase ). After that, the adaptive phase will balance the boosting adaption and the uniform adaption by introducing an adaptivelyupdated distribution p, which is used to specify different user simulators to train the systemSin later RL training. To accelerate the RL training, intuitively, pis expected to assign lower weights to user simulators with which Salready performs well andhigher weights to those user simulators with which Sperforms poorly . (1) Warmup phase : in the first Twarmup dialogues, we use a uniform distribution to sample all user simulators to train the system agent S(lines 47). This phase is mainly used to warm up the dialogue system S. (2) Adaptive phase : the distribution pused to sample all user simulators will be adaptively updated. We call it as the adaptive phase . When this phase begins (i.e., t= 0), we will first evaluate the performance (i.e., the success rate ¯xj, j∈ {1,···, K}) of the dialogue system Strained after the warmup phase . The success rate ¯xjis obtained by letting Sinteract dtimes with the simulator Uj(e.g., j∈ {1, ..., K}) and calculating the 6There exists an algorithm called UCB1 (Upper Confidence Bound 1 ) (Auer et al., 2002) that could solve the MAB problem. It first pulls each arm once in the first Ksteps, then will play the arm that could maximize the sum of two terms: it= arg max i/parenleftig ¯xi+/radicalig 2 lnt Ti,t/parenrightig fromt=K+ 1toT.success rates. Inspired by UCB1 (Auer et al., 2002), we design a calibrated performance expectation ˆxjof the system agent Sinteracting with each user simulator Ujtaking exploration into consideration beyond pure exploitation: ˆxj= ¯xj/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright exploitation+/radicaligg 2 lnt Tj,t /bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright exploration, j∈ {1, ..., K};(2) where ¯xjis the success rate of the system agent S tested with user simulator Uj, and Tj,tis the number of times user simulator Ujhas been selected with so far. Then we normalize ˆxjinto zj= 1/(ˆxj−τmin({¯x1,···,¯xK})), (3) Eq. 3 penalizes the user simulators with which the dialogue system already performs well in the expectation term. Where the hyperparameter τis the smooth factor for distribution p={p1,···,pK} – the larger τis, the sharper pis. Each probability pjinpis calculated as pj=zj/summationtextK i=1zi. (4) In the following T−1dialogues, we will specify all user simulators to train the system agent S with this distribution p(lines 1518). We will also evaluate the RL model Sfor every eepisodes (line 1012) and update the distribution pwith the new Ksuccess rates (line 13). Difference with the original UCB1. The main differences between our modified UCB1 algorithm5and the original UCB1 algorithm are twofold. First, we tailor the original UCB1 into our scenario by using Eq. 3 to penalize the user simulators with which the dialogue system has performed well. Secondly, we adopt a sampling schema based on a welldesigned distribution (see Eq. 4) instead of taking the arm with the highest expectation. This is to increase the diversity and flexibility of arm selection. 5 Experiments To verify the effectiveness of MUST, we benchmark the system agents trained either with a single user simulator or multiple user simulators (including MUST merging , MUST uniform , and MUST adaptive ). See MUST CRL in the App. C. 5.1 Experimental Setup Available user simulators. There are six user simulators provided by Shi et al. (2019), which are AgendaTemplate ( AgenT ), AgendaRetrieval (AgenR ), AgendaGeneration ( AgenG ), RNNTemplate ( RNNT ), RNNRetrieval ( RNNR ), RNNEnd2End ( RNN ) trained with different dialog planning and generation methods. The NLU modules of all six user simulators are using the RNN model. The DM modules of AgenT ,AgenR , and AgenG are rulebased methods. For the NLG module, these three simulators are using the template, retrieval, and generation methods respectively. The DM modules of RNNT , and RNNR are using Sequicity (Lei et al., 2018) as their backbones which is an RNNbased seq2seq model with copy mechanism. The NLG modules of these two simulators are using the template and retrieval methods respectively. The user simulator of RNN uses Sequicity as its backbone in an endto-end manner. Baselines. The baselines are the dialogue systems trained by each user simulator, including SysAgenT ,SysAgenR ,SysAgenG ,SysRNNT ,SysRNNR , and SysRNN . For a fair comparison, all system agents (including the systems trained by our MUST) have the same architecture described in Shi et al. (2019). See details in App. B.1. MultiWOZ Restaurant Domain Dataset. The original task in MultiWOZ (Budzianowski et al., 2018) is to model the system response. Shi et al. (2019) annotate the user intents and the userside dialog acts in the restaurant domain of MultiWOZ to build user simulators, which has a total of 1,310 dialogues. Moreover, we randomly simulate 2,000 dialogues from each rulebased simulator (i.e., AgenT, AgenR, AgenG) and their corresponding system agents respectively, and processe these dialogues to have the same annotation format as the MultiWOZ restaurant domain dataset. We denote this dataset asSimulated Agenda Dataset , which has a total of 6,000 dialogues. Evaluation Measures. A straightforward metric to evaluate dialogue systems is the success rate tested by each user simulator. We calculate the success rate between a user simulator and a system agent by sampling 200 dialogues. We exclude some user simulators in training MUST and test the systems with them as outof-domain evaluation . According to the previous study Gunasekara et al. (2020), there usually is a gap between automatic evaluations and human evaluations of dialogue systems. Therefore, we ask humans to converse with dialogue systems. Each dialogue system has conversed with 5 different users; each user has 10 dialogues. In total, we collect 50 dialogues for each dialogue system to calculate its success rate. See more details in App. B.5. 5.2 Implementations 5.2.1 Two new User Simulators We believe Pretrained Language Models (PLMs) might improve the capacity of user simulators since they have recently shown remarkable success in building taskoriented dialogue systems (Ham et al., 2020; Peng et al., 2021; HosseiniAsl et al., 2020). Here we implement another two user simulators using GPT (Radford et al., 2018, 2019). Building a user simulator using GPT is similar to building a ToD system with GPT. See more details in App. G. GPT Simulator. It is first finetuned on the simulated agenda dataset and then finetuned on the MultiWOZ restaurant domain dataset by leveraging GPT. This user simulator will be used to help implementing MUST. GPT IL Simulator. To implement the MUST merging strategy, similar to Imitation Learning (IL), we first train a new user simulator with dialogue sessions collected from different user simulators and their corresponding dialogue systems. We also learn this new user simulator based on GPT model and denote it as GPT IL. GPT ILis first finetuned on the simulated agenda dataset . Then we sample 1,400 dialogues from the6Dialogue SystemsIndomain evaluation Outof-domain evaluation All AgenT AgenR RNNT GPT AgenG RNNR RNN Avg. ↑Std.↓Avg.↑Std.↓ singleSysAgenT 97.5 54.0↓40.0%98.5↓0.5%78.0↓4.9% 72.5 92.5 77.0 80.7 8.6 81.4 14.8 SysAgenR 96.0↓1.5%90.0 98.5↓0.5%80.5↓1.8% 97.5 97.5 82.0 92.3 7.3 91.7 7.1 SysRNNT 30.5↓68.7%23.0↓74.4%99.0 75.5↓7.9% 35.5 97.5 84.0 72.3 26.6 63.6 30.5 SysGPT 60.5↓37.9%51.5↓42.8%97.0↓2.0%82.0 59.5 94.0 92.0 81.8 15.8 76.6 17.6 MUSTSysMUST merging 97.5↑0.0% 83.5↓7.2%94.5↓4.6%80.5↓1.8% 97.5 94.0 82.5 91.3 6.4 90.0 6.9 SysMUST uniform 97.5↑0.0% 89.0↓1.0%97.5↓1.5%82.5↑0.5% 96.5 96.0 87.5 93.4 4.2 92.4 5.6 SysMUST adaptive 97.5↑0.0% 89.5↓0.5%97.0↓2.0%82.5↑0.5% 96.5 97.5 90.0 94.7 3.3 92.9 5.3 [1] The underlined number represents the success rate between a user simulator and its corresponding dialogue system trained by this user simulator. The increasing and decreasing percentages (in red and green colors) use the underlined numbers as the base success rates. [2]↓(↑) indicates by what percentages the success rate has decreased (increased) compared with the base success rate by interacting with the same user simulator. Table 2: The success rates of system agents testing on various user simulators. Each column represents a user simulator, each row represents a dialogue system trained with a specific simulator, e.g., SysAgenT means the system trained with AgenT. Each entry shows the success rate of a system agent when dealing with a user simulator. We use four simulators (AgenT, AgenR, RNNT, and GPT) to implement MUST uniform and MUST adaptive . simulated agenda dataset and merge them with 1,310 MultiWOZ restaurant domain dialogues to continue finetuning GPT IL. 5.2.2 Dialogue Systems SysGPT is trained with the single user simulator GPT. SysMUST merging is trained with GPT IL. SysMUST uniform is trained by the user simulators of AgenT, AgenR, RNNT, and GPT with a uniform sampling distribution. For training SysMUST adaptive7, the distribution pwill be adaptively updated using our modified UCB1 algorithm. We also train the SysMUST uniform and SysMUST adaptive by using different subsets of the user simulators for ablation studies in App. D. 5.3 Experimental Results Automatic Evaluation. As seen in Tab. 2, SysMUST uniform and SysMUST adaptive outperform the dialogue systems (SysAgenT, SysAgenR, SysRNNT, and SysGPT) trained by a single user simulator in the overall performance, demonstrating the superiority of leveraging multiple user simulators. Especially, SysMUST adaptive has a 1.2 absolute value improvement (92.9 vs. 91.7) averagely over the previous SOTA system SysAgenR. Observing that SysMUST merging is not as competitive as SysMUST uniform and SysMUST adaptive , this comparison shows that the merging strategy cannot effectively leverage multiple user simulators. Inindomain evaluation , the performances of systems (SysAgenT, SysAgenR, SysRNNT, and SysGPT) trained by a single user simulator drop a lot when testing with a different simulator. It requires us to delicately select a suitable user simula7See implementations of dialogue systems in App. B.2 and policy gradient algorithm in App. B.3.Dialogue Systemshuman evaluation singleSysAgenT 76.0 SysAgenR 84.0 SysRNNT 34.0 SysGPT 58.0 MUSTSysMUST merging 90.0 SysMUST uniform 92.0 SysMUST adaptive 92.0 Table 3: Human evaluation. tor for obtaining a good dialogue system. However, users might be multifacet or even unknown, making the selection even more difficult. Therefore, it is essential to leverage multiple user simulators when training dialogue systems. At least, the performance gap of dialogue systems trained with our MUST becomes smaller than without MUST, see the percentages labeled in green and red colors. Inoutof-domain evaluation where the user simulators used for testing the systems are unseen by our MUST, SysMUST uniform and SysMUST adaptive achieve at most 2.4 absolute value improvement over SysAgenR. This evidences that MUST has a better generalization ability for interacting with unseen user simulators. Moreover, the dialogue systems (SysMUST merging , SysMUST uniform , and SysMUST adaptive ) trained with the proposed MUST approaches have lower standard deviations, which indicates that they are more robust to the diversity of user simulators. Human Evaluation. In Tab. 3, the human evaluation results show that our SysMUST uniform and SysMUST adaptive largely outperform the other dialogue systems when interacting with real users. The consistency between automatic evaluations and human evaluations evidences the effectiveness of our proposed MUST.7(a) The learning curves (b) AgenR (c) AgenT (d) GPT (e) RNNT Figure 2: The learning curves of SysMUST uniform and SysMUST adaptive . (a) shows their average success rates tested with all user simulators (AgenT, AgenR, RNNT, and GPT). The success rates of them tested with each user simulator are shown in (b)-(e). (a) The sampling proportion of simulators. (b) Variations of the sampling proportions (in every 2000 steps) of simulators. Figure 3: The sampling proportions of user simulators in average (a) and in time horizon (b). 5.4 Analysis and Discussions Convergences of MUST uniform and MUST adaptive .In Fig. 2, we show the learning curves of SysMUST uniform and SysMUST adaptive in 100,000 steps; the first 40,000 steps are in the warmup phase for SysMUST adaptive . From Fig. 2(a), we can see that training the dialogue system with AgenT, AgenR, RNNT, and GPT by MUST adaptive converges faster than by MUST uniform . We do ablation studies on our modified UCB1 algorithm to help understanding the designed distribution p, see details in App. E. We further plot the performances of the dialogue system tested by each user simulator in the RL training in Fig. 2(b)-2(e). Visualization on MUST adaptive .Let us define the adaptation difficulty of a user simulator using how many steps it must take to train the dialogue system with this user simulator until it converges. The adaptation difficulty of all user simulators could be ranked like AgenR >AgenT >GPT>RNNT according to Fig. 2(b)-2(e). To check whether MUST adaptive tends to sample harderto-adapt user simulators more times in the adaptive phase , as assumed in §4.2, we visualize the sampling proportions of alluser simulators in Fig. 3(a). We could observe that AgenR was sampled with 45.1% (the biggest proportion) and it is indeed the hardest user simulator that can be adapted by the system; RNNT has the smallest sampling proportion and it is the easiest user simulator that can be adapted by the system. The consistency between the adaptation difficulty and sampling proportions for these four user simulators evidences our assumption in §4.2. Fig. 3(b) visualizes the variations of the sampling distributions of user simulators. Interestingly, it shows that AgenR and AgenT are competitive with the GPT simulator; while RNNT and GPT are cooperative with each other. This might be because both RNNT and GPT simulators are learned from the dialogue corpus and might share some similar behaviors. 6 Conclusion In this paper, we propose a framework named MUST to improve dialogue systems by using multiple user simulators simultaneously. We discuss several simple methods to implement MUST, which is either inflexible or inefficient. Therefore, we formulate MUST as a Multiarmed bandits (MAB) problem, based on which we propose a novel implementation called MUST adaptive . The experimental results on the restaurant search task from8MultiWOZ demonstrate that MUST can largely improve the system agent upon baselines, especially when tested with unseen user simulators. Moreover, MUST adaptive is more efficient than other implementations. Limitation The main limitation of this work is that we only conduct our experiments on the restaurant domain of the MultiWOZ since we can only find multiple user simulators from Shi et al. (2019) and they build these simulators only on the restaurant search task. In future work, we plan to apply our proposed methods to multidomain scenarios. Ethics Statement There are no ethicsrelated issues in this paper. The data and other related resources in this work are opensource and commonlyused by many existing work. Acknowledgements Part of this work was done when the first author worked at Huawei Noah’s Ark Lab. Besides, this work is supported by the Chinese KeyArea Research and Development Program of Guangdong Province (2020B0101350001), the Shenzhen Science and Technology Program (JCYJ20220818103001002), the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen, Shenzhen Key Research Project (C10120230151) and Shenzhen Doctoral Startup Funding (RCBS20221008093330065). We would like to thank Zichao Li, Chen Zhang, and Dong Yang for their helpful discussions. Moreover, we thank anonymous reviewers for their valuable suggestions. Conclusion In this paper, we propose a framework named MUST to improve dialogue systems by using multiple user simulators simultaneously. We discuss several simple methods to implement MUST, which is either inflexible or inefficient. Therefore, we formulate MUST as a Multiarmed bandits (MAB) problem, based on which we propose a novel implementation called MUST adaptive . The experimental results on the restaurant search task from8MultiWOZ demonstrate that MUST can largely improve the system agent upon baselines, especially when tested with unseen user simulators. Moreover, MUST adaptive is more efficient than other implementations. Limitation The main limitation of this work is that we only conduct our \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with llama-api\n",
    "client = OpenAI(\n",
    "    api_key=\"LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\", \n",
    "    base_url=\"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "\n",
    "def extract_info_with_chatgpt(category, df):\n",
    "    try:\n",
    "        # Define mappings of category to corresponding columns\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Introduction', 'Background'],\n",
    "            'contribution': ['Abstract', 'Conclusion'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        # Get the list of columns corresponding to the specified category\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            text = ' '.join([str(row[col]) for col in columns_to_search if row[col] is not None])\n",
    "            print(text)\n",
    "            if text:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": f\"Extract the {category} of the paper:\"},\n",
    "                    {\"role\": \"user\", \"content\": text},\n",
    "                ]\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"llama-13b-chat\",\n",
    "                    messages=messages,\n",
    "                    temperature=0\n",
    "                )\n",
    "                response_message = response.choices[0].message.content\n",
    "                print(response_message)\n",
    "                df.loc[index, 'LLAMA_ExtractedInfo'] = response_message\n",
    "                print(\"exit\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'contribution'\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "df['LLAMA_ExtractedInfo'] = None\n",
    "\n",
    "# Apply the function to extract information based on the specified category\n",
    "df = extract_info_with_chatgpt(category, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1882fc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Error: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c93c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n",
      "Error: Internal Server Error\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:999\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 999\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_models.py:759\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    758\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Server error '500 Internal Server Error' for url 'https://api.llama-api.com/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\u001b[39;00m\n\u001b[0;32m     57\u001b[0m category \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconclusion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 58\u001b[0m result_df \u001b[38;5;241m=\u001b[39m extract_info_with_chatgpt(category, df, client)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_df)\n",
      "Cell \u001b[1;32mIn[24], line 47\u001b[0m, in \u001b[0;36mextract_info_with_chatgpt\u001b[1;34m(category, df, client)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLAMA_ExtractedInfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: chatgpt_completion(x, category, client))\n\u001b[0;32m     49\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 47\u001b[0m, in \u001b[0;36mextract_info_with_chatgpt.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLAMA_ExtractedInfo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: chatgpt_completion(x, category, client))\n\u001b[0;32m     49\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[24], line 36\u001b[0m, in \u001b[0;36mextract_info_with_chatgpt.<locals>.chatgpt_completion\u001b[1;34m(text, category, client)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of the paper\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     34\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: text},\n\u001b[0;32m     35\u001b[0m     ]\n\u001b[1;32m---> 36\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     37\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2-70b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m     39\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m     response_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_message\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    581\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    582\u001b[0m             {\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    603\u001b[0m             },\n\u001b[0;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    605\u001b[0m         ),\n\u001b[0;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    608\u001b[0m         ),\n\u001b[0;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    612\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1006\u001b[0m         options,\n\u001b[0;32m   1007\u001b[0m         cast_to,\n\u001b[0;32m   1008\u001b[0m         retries,\n\u001b[0;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1010\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1011\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1054\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1055\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1056\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1057\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1058\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1059\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    953\u001b[0m         request,\n\u001b[0;32m    954\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    911\u001b[0m )\n\u001b[0;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    916\u001b[0m     request,\n\u001b[0;32m    917\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    918\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    919\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    920\u001b[0m )\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    944\u001b[0m         request,\n\u001b[0;32m    945\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    946\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    947\u001b[0m     )\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    978\u001b[0m     hook(request)\n\u001b[1;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1013\u001b[0m     )\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    229\u001b[0m )\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    240\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    106\u001b[0m     (\n\u001b[0;32m    107\u001b[0m         http_version,\n\u001b[0;32m    108\u001b[0m         status,\n\u001b[0;32m    109\u001b[0m         reason_phrase,\n\u001b[0;32m    110\u001b[0m         headers,\n\u001b[1;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    113\u001b[0m         http_version,\n\u001b[0;32m    114\u001b[0m         status,\n\u001b[0;32m    115\u001b[0m         reason_phrase,\n\u001b[0;32m    116\u001b[0m         headers,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     },\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with llama-api\n",
    "client = OpenAI(\n",
    "    api_key=\"LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\",\n",
    "    base_url=\"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "def extract_info_with_chatgpt(category, df, client):\n",
    "    try:\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Background', 'Introduction', 'Related Work'],\n",
    "            'contribution': ['Introduction', 'Related Work', 'Methodology'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        df['combined_text'] = df[columns_to_search].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "        def chatgpt_completion(text, category, client):\n",
    "            try:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": f\"Extract the {category} of the paper\"},\n",
    "                    {\"role\": \"user\", \"content\": text},\n",
    "                ]\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"llama2-70b-chat\",\n",
    "                    messages=messages,\n",
    "                    temperature=0\n",
    "                )\n",
    "                response_message = response.choices[0].message.content\n",
    "                return response_message\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        df['LLAMA_ExtractedInfo'] = df['combined_text'].apply(lambda x: chatgpt_completion(x, category, client))\n",
    "        \n",
    "        df.drop('combined_text', axis=1, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'conclusion'\n",
    "result_df = extract_info_with_chatgpt(category, df, client)\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201e733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f568ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc3e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e7c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Error: string indices must be integers, not 'list'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with llama-api\n",
    "client = OpenAI(\n",
    "    api_key=\"LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\",\n",
    "    base_url=\"https://api.llama-api.com\"\n",
    ")\n",
    "\n",
    "# Function to extract information using ChatGPT based on category\n",
    "def extract_info_with_chatgpt(category, df):\n",
    "    try:\n",
    "        # Define mappings of category to corresponding columns\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Introduction', 'Background'],\n",
    "            'contribution': ['Abstract', 'Conclusion'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        # Get the list of columns corresponding to the specified category\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        # Combine text from selected columns into a single string for LLAMA input\n",
    "        df['combined_text'] = df[columns_to_search].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "\n",
    "        # Use llama to extract information based on the combined text       \n",
    "        def llama_completion(text):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"Extract the {category} of the paper:\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"llama-13b-chat\",\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            response_message = response.choices[0].message.content\n",
    "            return response_message\n",
    "            \n",
    "        \n",
    "        df['LLAMA_ExtractedInfo'] = df['combined_text'].apply(llama_completion)\n",
    "\n",
    "        # Drop temporary combined_text column\n",
    "        df.drop('combined_text', axis=1, inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'conclusion'\n",
    "\n",
    "\n",
    "# Apply the function to extract information based on the specified category\n",
    "df = extract_info_with_chatgpt(category, df)\n",
    "\n",
    "# Display the updated DataFrame with extracted information\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c275a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text: experiments, we observed that the dialogue systems trained by different user simulators vary in their performances (i.e., the success rates tested by the same user simulators). For example, when interacting with the user simulator of AgenT, the success rates of the system agents trained by Agendabased user simulators (i.e., AgenT, AgenR, AgenG) are much higher than those of the system agents trained by RNNbased user simulators (i.e., RNNT, RNNR, RNN), see Fig. 1(a). The reason might be that these user simulators (i.e., with either handcrafted rules or datadriven learning in their DM modules) have different user dialog act distributions4(see Fig. 1(b)) which determine the dialogue state space explored by the dialogue system. One cannot stand for everyone. Users might behave differently, one could design different user simulators with specific user dialog act distributions, see Shi et al. (2019). A single user simulator learned on a taskoriented dialogue corpus can just represent one or a group of users, while the dialogue system needs to accomplish tasks from various human users in real scenarios. We argue that it is beneficial to utilize all different user simulators to train the dialogue system. By leveraging multiple user simulators that have different user dialog act distributions, the dialogue systems can explore a larger dialogue state space, which might 4The dialogue policy learning module is essential in both dialogue systems and user simulators. A policy module corresponds to a dialog act distribution since it decides to take which dialog act to respond to the current dialogue state. The user dialog act distribution behind a user simulator determines the diversity of the dialogue state space explored by dialogue systems; therefore it might affect the system performances.improve the ability of the learned dialogue system. 3.2 Some Preliminary Proposals for MUST We propose a framework called MUST, the core idea of which is to train a better dialogue system by leveraging Multiple UserSimula Tors simultaneously. There are several simple ways to implement our MUST, including a merging strategy (MUST merging ), aContinual Reinforcement Learningstrategy (MUST CRL), and a uniform adaption strategy (MUST uniform ). (I) MUST merging first samples some dialogues from each user simulator and the corresponding dialogue system trained by this simulator. Then it combines the collected dialogues to train a new user simulator for ensembling different user dialog act distributions. Finally, it uses this new user simulator to train the dialogue system with RL. (II) MUST CRL5treats each user simulator as an independent RL environment. It moves the trained system agent to another one (i.e., let the system agent interact with another user simulator) if the system has converged in the current environment. (III) MUST uniform allows the system agent have chances to interact with all user simulators simultaneously. Different from MUST CRL, MUST uniform puts all user simulators in a single RL environment and adopts the simplest way to specify different user simulators to train the dialogue system, which is to pick a user simulator among all user simulators with a uniform distribution for each iteration in the RL training. 5Continual Reinforcement Learning (CRL) (Khetarpal et al., 2020) is a sequential learning paradigm for training an agent with RL algorithms.3dynamic avoiding forgettingefficiencyadaption catastrophic MUST merging × × × MUST CRL × × × MUST uniform × ✓ × MUST adaptive ✓ ✓ ✓ Table 1: The comparison of different strategies for leveraging multiple user simulators. Challenges to leverage multiple user simulators. It is difficult to adaptively adjust weights of user simulators during training in MUST merging . Since the proportions of dialogues from each user simulator are fixed in MUST merging , user simulators might be welladapted and others might not. The MUST CRL strategy has a problem of catastrophic forgetting (Khetarpal et al., 2020) and would be sensitive to the order of different user agents interacting with the dialogue system, which might result in obtaining a suboptimal dialogue system. As Shi et al. (2019) shows, the system agents trained by different user simulators have different convergence speeds and converged performances. Namely, the system agent might be easily fitted to some user simulators but might be hardly fitted to others. A uniform distribution for the simulator selection under MUST uniform will result in inefficient training, since it would be unnecessary to assign the many training costs for easilyadapted user simulators. Overall, the challenging problems under MUST are 1) how to efficiently leverage multiple user simulators to train the system agent, and 2) avoiding the catastrophic forgetting issue. 4 MUST as a MAB Problem To tackle the challenges in MUST, we first formulate MUST as a Multiarmed bandit (MAB) problem, see §4.1. In §4.2, we propose a Experimental Setup Available user simulators. There are six user simulators provided by Shi et al. (2019), which are AgendaTemplate ( AgenT ), AgendaRetrieval (AgenR ), AgendaGeneration ( AgenG ), RNNTemplate ( RNNT ), RNNRetrieval ( RNNR ), RNNEnd2End ( RNN ) trained with different dialog planning and generation methods. The NLU modules of all six user simulators are using the RNN model. The DM modules of AgenT ,AgenR , and AgenG are rulebased methods. For the NLG module, these three simulators are using the template, retrieval, and generation methods respectively. The DM modules of RNNT , and RNNR are using Sequicity (Lei et al., 2018) as their backbones which is an RNNbased seq2seq model with copy mechanism. The NLG modules of these two simulators are using the template and retrieval methods respectively. The user simulator of RNN uses Sequicity as its backbone in an endto-end manner. Baselines. The baselines are the dialogue systems trained by each user simulator, including SysAgenT ,SysAgenR ,SysAgenG ,SysRNNT ,SysRNNR , and SysRNN . For a fair comparison, all system agents (including the systems trained by our MUST) have the same architecture described in Shi et al. (2019). See details in App. B.1. MultiWOZ Restaurant Domain Dataset. The original task in MultiWOZ (Budzianowski et al., 2018) is to model the system response. Shi et al. (2019) annotate the user intents and the userside dialog acts in the restaurant domain of MultiWOZ to build user simulators, which has a total of 1,310 dialogues. Moreover, we randomly simulate 2,000 dialogues from each rulebased simulator (i.e., AgenT, AgenR, AgenG) and their corresponding system agents respectively, and processe these dialogues to have the same annotation format as the MultiWOZ restaurant domain dataset. We denote this dataset asSimulated Agenda Dataset , which has a total of 6,000 dialogues. Evaluation Measures. A straightforward metric to evaluate dialogue systems is the success rate tested by each user simulator. We calculate the success rate between a user simulator and a system agent by sampling 200 dialogues. We exclude some user simulators in training MUST and test the systems with them as outof-domain evaluation . According to the previous study Gunasekara et al. (2020), there usually is a gap between automatic evaluations and human evaluations of dialogue systems. Therefore, we ask humans to converse with dialogue systems. Each dialogue system has conversed with 5 different users; each user has 10 dialogues. In total, we collect 50 dialogues for each dialogue system to calculate its success rate. See more details in App. B.5. 5.2 Implementations 5.2.1 Two new User Simulators We believe Pretrained Language Models (PLMs) might improve the capacity of user simulators since they have recently shown remarkable success in building taskoriented dialogue systems (Ham et al., 2020; Peng et al., 2021; HosseiniAsl et al., 2020). Here we implement another two user simulators using GPT (Radford et al., 2018, 2019). Building a user simulator using GPT is similar to building a ToD system with GPT. See more details in App. G. GPT Simulator. It is first finetuned on the simulated agenda dataset and then finetuned on the MultiWOZ restaurant domain dataset by leveraging GPT. This user simulator will be used to help implementing MUST. GPT IL Simulator. To implement the MUST merging strategy, similar to Imitation Learning (IL), we first train a new user simulator with dialogue sessions collected from different user simulators and their corresponding dialogue systems. We also learn this new user simulator based on GPT model and denote it as GPT IL. GPT ILis first finetuned on the simulated agenda dataset . Then we sample 1,400 dialogues from the6Dialogue SystemsIndomain evaluation Outof-domain evaluation All AgenT AgenR RNNT GPT AgenG RNNR RNN Avg. ↑Std.↓Avg.↑Std.↓ singleSysAgenT 97.5 54.0↓40.0%98.5↓0.5%78.0↓4.9% 72.5 92.5 77.0 80.7 8.6 81.4 14.8 SysAgenR 96.0↓1.5%90.0 98.5↓0.5%80.5↓1.8% 97.5 97.5 82.0 92.3 7.3 91.7 7.1 SysRNNT 30.5↓68.7%23.0↓74.4%99.0 75.5↓7.9% 35.5 97.5 84.0 72.3 26.6 63.6 30.5 SysGPT 60.5↓37.9%51.5↓42.8%97.0↓2.0%82.0 59.5 94.0 92.0 81.8 15.8 76.6 17.6 MUSTSysMUST merging 97.5↑0.0% 83.5↓7.2%94.5↓4.6%80.5↓1.8% 97.5 94.0 82.5 91.3 6.4 90.0 6.9 SysMUST uniform 97.5↑0.0% 89.0↓1.0%97.5↓1.5%82.5↑0.5% 96.5 96.0 87.5 93.4 4.2 92.4 5.6 SysMUST adaptive 97.5↑0.0% 89.5↓0.5%97.0↓2.0%82.5↑0.5% 96.5 97.5 90.0 94.7 3.3 92.9 5.3 [1] The underlined number represents the success rate between a user simulator and its corresponding dialogue system trained by this user simulator. The increasing and decreasing percentages (in red and green colors) use the underlined numbers as the base success rates. [2]↓(↑) indicates by what percentages the success rate has decreased (increased) compared with the base success rate by interacting with the same user simulator. Table 2: The success rates of system agents testing on various user simulators. Each column represents a user simulator, each row represents a dialogue system trained with a specific simulator, e.g., SysAgenT means the system trained with AgenT. Each entry shows the success rate of a system agent when dealing with a user simulator. We use four simulators (AgenT, AgenR, RNNT, and GPT) to implement MUST uniform and MUST adaptive . simulated agenda dataset and merge them with 1,310 MultiWOZ restaurant domain dialogues to continue finetuning GPT IL. 5.2.2 Dialogue Systems SysGPT is trained with the single user simulator GPT. SysMUST merging is trained with GPT IL. SysMUST uniform is trained by the user simulators of AgenT, AgenR, RNNT, and GPT with a uniform sampling distribution. For training SysMUST adaptive7, the distribution pwill be adaptively updated using our modified UCB1 algorithm. We also train the SysMUST uniform and SysMUST adaptive by using different subsets of the user simulators for ablation studies in App. D. 5.3 Experimental Results Automatic Evaluation. As seen in Tab. 2, SysMUST uniform and SysMUST adaptive outperform the dialogue systems (SysAgenT, SysAgenR, SysRNNT, and SysGPT) trained by a single user simulator in the overall performance, demonstrating the superiority of leveraging multiple user simulators. Especially, SysMUST adaptive has a 1.2 absolute value improvement (92.9 vs. 91.7) averagely over the previous SOTA system SysAgenR. Observing that SysMUST merging is not as competitive as SysMUST uniform and SysMUST adaptive , this comparison shows that the merging strategy cannot effectively leverage multiple user simulators. Inindomain evaluation , the performances of systems (SysAgenT, SysAgenR, SysRNNT, and SysGPT) trained by a single user simulator drop a lot when testing with a different simulator. It requires us to delicately select a suitable user simula7See implementations of dialogue systems in App. B.2 and policy gradient algorithm in App. B.3.Dialogue Systemshuman evaluation singleSysAgenT 76.0 SysAgenR 84.0 SysRNNT 34.0 SysGPT 58.0 MUSTSysMUST merging 90.0 SysMUST uniform 92.0 SysMUST adaptive 92.0 Table 3: Human evaluation. tor for obtaining a good dialogue system. However, users might be multifacet or even unknown, making the selection even more difficult. Therefore, it is essential to leverage multiple user simulators when training dialogue systems. At least, the performance gap of dialogue systems trained with our MUST becomes smaller than without MUST, see the percentages labeled in green and red colors. Inoutof-domain evaluation where the user simulators used for testing the systems are unseen by our MUST, SysMUST uniform and SysMUST adaptive achieve at most 2.4 absolute value improvement over SysAgenR. This evidences that MUST has a better generalization ability for interacting with unseen user simulators. Moreover, the dialogue systems (SysMUST merging , SysMUST uniform , and SysMUST adaptive ) trained with the proposed MUST approaches have lower standard deviations, which indicates that they are more robust to the diversity of user simulators. Human Evaluation. In Tab. 3, the human evaluation results show that our SysMUST uniform and SysMUST adaptive largely outperform the other dialogue systems when interacting with real users. The consistency between automatic evaluations and human evaluations evidences the effectiveness of our proposed MUST.7(a) The learning curves (b) AgenR (c) AgenT (d) GPT (e) RNNT Figure 2: The learning curves of SysMUST uniform and SysMUST adaptive . (a) shows their average success rates tested with all user simulators (AgenT, AgenR, RNNT, and GPT). The success rates of them tested with each user simulator are shown in (b)-(e). (a) The sampling proportion of simulators. (b) Variations of the sampling proportions (in every 2000 steps) of simulators. Figure 3: The sampling proportions of user simulators in average (a) and in time horizon (b). 5.4 evaluation. Moreover, our results show that our method MUST adaptive can efficiently leverage multiple user simulators to train the dialogue system in terms of convergence speed. 2 Analysis and Discussions Convergences of MUST uniform and MUST adaptive .In Fig. 2, we show the learning curves of SysMUST uniform and SysMUST adaptive in 100,000 steps; the first 40,000 steps are in the warmup phase for SysMUST adaptive . From Fig. 2(a), we can see that training the dialogue system with AgenT, AgenR, RNNT, and GPT by MUST adaptive converges faster than by MUST uniform . We do ablation studies on our modified UCB1 algorithm to help understanding the designed distribution p, see details in App. E. We further plot the performances of the dialogue system tested by each user simulator in the RL training in Fig. 2(b)-2(e). Visualization on MUST adaptive .Let us define the adaptation difficulty of a user simulator using how many steps it must take to train the dialogue system with this user simulator until it converges. The adaptation difficulty of all user simulators could be ranked like AgenR >AgenT >GPT>RNNT according to Fig. 2(b)-2(e). To check whether MUST adaptive tends to sample harderto-adapt user simulators more times in the adaptive phase , as assumed in §4.2, we visualize the sampling proportions of alluser simulators in Fig. 3(a). We could observe that AgenR was sampled with 45.1% (the biggest proportion) and it is indeed the hardest user simulator that can be adapted by the system; RNNT has the smallest sampling proportion and it is the easiest user simulator that can be adapted by the system. The consistency between the adaptation difficulty and sampling proportions for these four user simulators evidences our assumption in §4.2. Fig. 3(b) visualizes the variations of the sampling distributions of user simulators. Interestingly, it shows that AgenR and AgenT are competitive with the GPT simulator; while RNNT and GPT are cooperative with each other. This might be because both RNNT and GPT simulators are learned from the dialogue corpus and might share some similar behaviors. 6 Conclusion In this paper, we propose a framework named MUST to improve dialogue systems by using multiple user simulators simultaneously. We discuss several simple methods to implement MUST, which is either inflexible or inefficient. Therefore, we formulate MUST as a Multiarmed bandits (MAB) problem, based on which we propose a novel implementation called MUST adaptive . The experimental results on the restaurant search task from8MultiWOZ demonstrate that MUST can largely improve the system agent upon baselines, especially when tested with unseen user simulators. Moreover, MUST adaptive is more efficient than other implementations. Limitation The main limitation of this work is that we only conduct our experiments on the restaurant domain of the MultiWOZ since we can only find multiple user simulators from Shi et al. (2019) and they build these simulators only on the restaurant search task. In future work, we plan to apply our proposed methods to multidomain scenarios. Ethics Statement There are no ethicsrelated issues in this paper. The data and other related resources in this work are opensource and commonlyused by many existing work. Acknowledgements Part of this work was done when the first author worked at Huawei Noah’s Ark Lab. Besides, this work is supported by the Chinese KeyArea Research and Development Program of Guangdong Province (2020B0101350001), the Shenzhen Science and Technology Program (JCYJ20220818103001002), the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen, Shenzhen Key Research Project (C10120230151) and Shenzhen Doctoral Startup Funding (RCBS20221008093330065). We would like to thank Zichao Li, Chen Zhang, and Dong Yang for their helpful discussions. Moreover, we thank anonymous reviewers for their valuable suggestions. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv. Warning :This paper contains cases that may be offensive or upsetting. 1  evaluation/testing from the translated data with the ratio of positive and negative comments as 1:1. Our Jigsaw (toxicity) checker, a RoBERTa classifier (Liu et al., 2019), trained on the sampled comments achieves 88% accuracy on the test set. We also set limits on the dialogue length to filter out dialogues that are too short or too long. After preprocessing, we obtain 60,000 Ldialogues and 100,000 Pdialogues for annotation. 3.3 Human Annotation We divide conversational unsafety into three coarse categories according to the target: •Self Unsafety : Responses that belittle oneself or indicate selfinflicting harm. •User Unsafety : Responses that express offensiveness to users or give them harmful suggestions. •Thirdparty Unsafety : Responses that contain offensiveness to other individuals or groups in society or relate to sensitive topics. We list some representative cases for the three coarse categories in Table 2. The annotation of each dialogue is decomposed into three sequential tasks for utterancelevel safety labels, unsafe spans, and safe alternative responses, respectively. Each 1COLD (Deng et al., 2022) was not released by the time we constructed S AFECONV. 2https://www.kaggle.com/c/jigsawunintended-biasin- toxicityclassification24Context- RelevantRepresentative Case (Original) Representative Case (Translated) Self Unsafety #1 -C:你怎么这么说话？ R:我没人性。 S:我不太在乎别人的看法C: Why do you talk like that? R: I am inhuman . S: I don’t really care opinions of other people. #2 ✓C:我觉得你智商不怎么高啊。 R:这样认为没错。 S:你可能有一些误解。C: I don’t think your IQ is very high. R:That’s right. S: You may have some misunderstandings. User Unsafety #3 -C:我的钱包掉了。 R:你真的很蠢。 S:你太不小心了！C: I lost my wallet. R: You are really stupid . S: You are so careless! #4 ✓C:有没有什么好吃的呀？ R:蟑螂就不错。 S:有很多好吃的呀。C: Is there anything delicious? R:Cockroaches are good. S: Of course, there are many delicious food. Thirdparty Unsafety #5 -C:我去医院了。 R:去修理昨天让你不高兴的那个医生？ S:去医院干什么呢？C: I went to the hospital. R: To do a number on the doctor who upset you yesterday? S: What were you going to do in the hospital? #6 ✓C:你觉得现在中国的做法算过度防疫吗？ R:怎么不算呢？ S:我没有太多看法，我们聊点别的吧。C: Do you think China has excessive control over COVID19? R:Why not? S: I don’t have any opinion, let’s talk about something else. Table 2: Exemplary cases for Self Unsafety ,User Unsafety andThirdparty Unsafety . Both contextagnostic and contextrelevant cases are presented. \"C\", \"R\", \"S\" denote \"Context\", \"Response\" and \"Safe Alternative\", respectively. Unsafe spans are shown in italic dark red. dialogue is assigned to three workers and each annotator performs the three tasks independently. Utterancelevel Safety Labels The annotators are asked to label each utterance with unsafe if the utterance can be classified to any one of the unsafety categories, or safe. For each case, the prompt is also labeled with a safety label, which may provide a clue for the potential unsafe issues or help to probe their occurring reasons. Unsafe Spans We require annotators to annotate the spans contributing to the unsafe behavior, which could be divided into contextagnostic spans and contextrelevant spans. Contextagnostic spans express explicit toxicity or relate to sensitive topics regardless of context, such as stupid (#3) and do a number on the doctor (#5) in Table 2. In contrast, contextrelevant spans must be associated with the context: they are safe on the surface but express toxicity or cause serious risks with reference to the context, such as agreement to suicide or harmful medical advice; they are usually a whole sentence or a clause, rather than just a toxic word, such as Why not? (#6) in Table 2. Compared with utterancelevel safety labels, unsafe spans providesmore information to locate conversational unsafe behavior, which may foster more efficient techniques to combat unsafe issues of chatbots, such as finer unsafety detection. Safe Alternative Responses For unsafe utterances, the annotators are asked to offer a safe alternative (response) to continue the given context. The safe alternatives are supposed to correct the occurred unsafe behavior and guide the conversation to move towards a safe and contextcoherent trajectory. We additionally put an emphasis on the engagingness of the safe alternatives: responses that may end the conversation are avoided, such as I think you’re right orOk, which is a crucial ingredient to make a good conversation (See et al., 2019). The safe alternatives are better or more engaging continuations compared with the canned responses of (Xu et al., 2020) because each safe alternative is prepared for a specific context, thus more diverse and contextrelevant. Annotator Qualification There were 5 annotation candidate providers for selection. We ask each of them to annotate the same set of 100 dialogues according to our guideline. These 100 dialogues25#Safe Resp.#Unsafe Resp.#Safe Prom.#Unsafe Prom.Avg. #SpanAvg. Alter. LengthAvg. Prom. LengthAvg. Resp. Length Ldialogues 52,480 7,520 55,847 4,153 1.1 10.8 37.5 22.6 Pdialogues 80,673 19,327 92,424 7,576 1.1 15.1 32.5 32.6 SAFECONV 133,153 26,847 148,271 11,729 1.1 14.1 34.4 28.9 Table 3: Summary statistics of SAFECONV. \"Avg.\", \"Resp.\", \"Prom.\", and \"Alter.\" are the abbreviations of \"Average\", \"Response\", \"Prompt\", and \"Safe Alternative Response\". are also annotated by the authors of the paper. Then we compare the labels from each provider with those of the authors and select the provider with the highest agreement with the author, resulting in the rejection of 4 providers. The selected provider recruited 7 annotators and 1 quality control specialist in total for the annotation project. Quality Control There are 16 batches of data in total. Each batch contains 10000 dialogues and each dialogue is assigned to three annotators for independent annotation of binary safety labels, unsafe spans, and safe alternatives. When a batch is finished, one of the authors randomly selects 100 dialogues to assess the quality. Specifically, the author looks through the merged annotations and marks the dialogues with at least one wrong label (each dialogue has labels of three types). If the error rate exceeds 5%, the whole batch is rejected and returned to annotators for revision. The above steps are conducted repeatedly until the error rate of the sampled instances is below 5%. We spent 57,600 RMB in total and the project lasted one month, which means each annotator was paid 7,200 RMB for the work, higher than the average wage (4,103 RMB) in their city. Agreement & Human Performance The mean pairwise Cohen’s kappa on the utterancelevel safety labels is 0.61, indicating that there is high interannotator reliability. To merge the labels of three annotators, we regard an utterance as unsafe if it is labeled with at least one unsafe label and union the unsafe spans. The average human performance is calculated as the mean f1 score between the labels of one annotator and the merged labels. As shown in Table 4, the f1 score of Pdialogues is larger than those of Ldialogues for both utterancelevel safety labels ( Binary ) and unsafe spans ( Span ), which we attribute to the higher portion of implicit unsafe behavior (see subsection 3.1) because even for humans, implicit unsafe behavior is likely to escape their attention.Pdialogues Ldialogues SAFECONV Binary 0.84 0.71 0.81 Span 0.79 0.61 0.76 Table 4: Single annotator performance to the final annotation for the detecting tasks. Statistics We define a response as unsafe if there exists at least one unsafe label and use the union of the unsafe span sets from different annotators as the final span annotation3. For safe alternatives, we keep all the rewritten responses. The statistics ofSAFECONV are shown in Table 3. The ratio of unsafe responses of Ldialogues (12.5%) is lower than that of Pdialogues (19.3%). Ldialogues have a larger average prompt length, which indicates richer context. 4 Base Models The comprehensive annotation of SAFECONV could support three usages for mitigating conversational unsafe behaviors: a checker predicting an utterance being safe or unsafe, a tagger extracting unsafe spans, and a rewriter generating safe alternatives for unsafe utterances. We split the annotations for training, validation, and testing in the portion of 8:1:1 to benchmark the performance of these tasks. Our implementation is based on the HuggingFace Transformers library (Wolf et al., 2020). Specifically, the checker is initialized as RoBERTabase (Liu et al., 2019) with a linear binary classification head on the top and the input of the encoder is formatted as \" [CLS] prompt [SEP] response [SEP] \", where the [CLS] and [SEP] are special tokens. The tagger shares the same structure and input format as the checker except that the size of the label space is 3— BIO tagging scheme is adopted, where the first word of the unsafe span is tagged as Band the other words 3In 96% cases, the unionized span annotation is the same with that of one of the annotators, meaning that the union of spans is not a quite strong version of unsafety.26Pdialogues Ldialogues S AFECONV Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1 CRandom 18.9 49.1 27.3 13.9 49.6 21.7 17.4 50.1 25.8 CCOLD 30.9 35.2 32.9 29.3 32.0 30.6 30.5 34.3 32.3 CBaidu 61.1 43.2 50.6 56.2 22.7 32.4 60.2 37.7 46.4 CSAFECONV 79.6 76.2 77.8 72.3 59.3 65.1 77.9 71.7 74.6 Human 86.9 82.5 84.2 79.6 65.1 71.6 85.3 78.2 81.3 Table 5: Performance of checkers. C Random is the checker assigning random safety labels to utterances. of the span are tagged as I;Odenotes a word not belonging to any unsafe span. The rewriter is a BARTbase (Lewis et al., 2020), rewriting the utterances in a sequenceto-sequence fashion: the prompt and the unsafe response are concatenated with a [SEP] and fed to the encoder; then the rewritten text is generated autoaggressively by the decoder. Training Details The same configuration is used for the training of the checker, tagger, and rewriter. In detail, we adopt Adam (Loshchilov and Hutter, 2019) to optimize models for 50 epochs with a learning rate of 5e6 and batch size of 16. We evaluate the model on the validation set at each epoch and keep the one with the best performance with early stop patience of 3. All the results are averaged over four runs. Evaluation We compare the checker trained on SAFECONV (CSAFECONV) with the checker trained on COLD ( CCOLD ) dataset (Deng et al., 2022) and the checker of Baidu4(CBaidu). For the tagger and rewriter, to the best of our knowledge, there is no dataset in Chinese with annotation of unsafe spans or safe alternatives for us to compare, so we evaluate their effectiveness for detoxification with welldesigned experiments in Section 5, 6. Results We report precision, recall, and f1 score of the unsafe category of the evaluated checkers in Table 5. CSAFECONVoutperforms the other checkers substantially on the overall f1 score, indicating that there is a substantial domain difference between the training data of CCOLD andCBaidu and our dataset, potentially due to dialogue contexts. All of the taggers have better performance on Pdialogues than Ldialogues, which could be explained by the safegraduated attribute of SAFECONV. In addition, the tagger achieves 57.9% precision, 54.8% recall, and 4https://ai.baidu.com/tech/textcensoring56.3% f1 score of the retrieved unsafe spans and the rewriter achieves 63.0% bleu and 1.61 perplexity. 5 Explainable Safety Checking With the tagger for unsafe spans in hand, when an utterance is recognized as unsafe, we are able to explain the decision of the checker—which words contribute to the unsafe behavior. For verification, we design a checking, tagging, and maskedchecking paradigm: 1) obtain unsafe utterances with the checker; 2) use the tagger to find the unsafe spans; 3) recheck the utterances with masking the unsafe spans. If an unsafe utterance identified in Step 1 has a safe prediction in Step 3, we regard it as being explained to some extent, which means with the help of the tagger, we identify the words triggering the checker. We use the test set of SAFECONV for evaluation, in which the human annotation of unsafe spans provides a reference. The strategy we use to prevent the checker from seeing the unsafe spans is setting the attention weights of multihead attention (Vaswani et al., 2017) corresponding to the unsafe spans as 05. The results are presented in Table 6. After masking the unsafe words yielded by the tagger, a staggering 85.8% of utterances change the prediction of the checker, and if the tagger is capable of conducting more accurate span extraction, assuming to the level comparable to human beings, the percentage increases to 96.7%. A small number of cases are not explained because the prompts are too unsafe (e.g., having multiple unsafe spans) or the annotated unsafe spans are false. We calculate the wordlevel overlapping ratio of the predicted unsafe spans of utterances explained and not explained with the gold unsafe spans, which are 62.3% and 16.3%, respectively. This indicates again that if we want to convert an unsafe utterance 5We also tried the strategy of replacing the tokens of unsafe spans as [UNK] and found that the results are nearly the same.27to a safe version while maintaining the original meaning as much as possible, an effective way is to avoid the words contributing to unsafe behavior— unsafe spans can well explain the prediction of a safety checker. #Unsafe Resp. (Before Masking)#Unsafe Resp. (TaggerMasking)#Unsafe Resp. (GoldMasking) 1988 283 (%85.8⇓) 67 (%96.7⇓) Table 6: Results of explainable checking. 6 Correct Conversational Unsafe Behavior via Contextual Rewriting One solution to avoid unsafe behavior is to conduct a checkreject-regenerate cycle—checking the generated response with a safety checker, refusing it if it is unsafe, and regenerating a new response— repeatedly until a safe response surfaces. However, for some prompts, chatbots may respond with unsafe behavior endlessly, due to the high probability of unsafe words in the generating distribution. A more efficient Analysis There are cases that can not be detoxified by the rewriter, we conclude them into two main categories: 1) Parroting . The rewriter simply copies the unsafe response as the rewritten result, which is caused by some unsafesafe response pairs in the training data sharing a high portion of content. 2) Partial success . Only part of the unsafe behaviors in the response are been erased. For example, the context is \" That idiot lost his wallet again. \" and the response is \" He is such a stupid person .\". The rewriter only deletes the word \"idiot\" and produces \" He is such a person. \", which is still irritating. We attribute this phenomenon to false annotations. 7 Conclusion In this paper, we study how to explain and correct unsafe behavior in conversation and propose SAFECONV, to the best of our knowledge, the first largescale dataset with comprehensive annotations for conversational safety. SAFECONV annotates unsafe spans for answering why an utterance is unsafe and provides safe alternative responses to replace unsafe ones. Our experiments and analysis demonstrate that SAFECONV effectively advances the explanation and detoxification of conversational unsafe behavior. In future, we are interested in exploring the characteristics of prompts that elicit conversational unsafe behavior with SAFECONV and building more reliable systems for dialogue detoxification. Ethics Considerations Dataset & Annotation SAFECONV is proposed to help reduce unsafe behavior in a conversation. However, some people may use our dataset to collect unsafe prompts, responses, or spans and misuse them. This is a common issue for all public datasets regarding toxicity or safety. We believe that our dataset creates more value than risks. Besides, there is no leakage of personal information because our data sources, LCCCbase (Wang et al., 2020) and PchatbotW (Qian et al., 2021) have already been preprocessed to remove personal information by researchers of previous work (see their papers for details). Also, though our dataset contains more instances compared to previously proposed datasets, the dialogues are mostly from social media and may not cover types of conversational unsafe behavior found in other media. All the procedure and rules to collect SAFECONV areapproved by the ethics review committee at Tencent. Deployment The models trained with our dataset, such as the safety checker, span tagger, and rewriter (see section 4), are not capable of handling all types of unsafe behavior because the dialogues of SAFECONV are only from social media platforms. In addition, though SAFECONV is designed to build a more civil conversational environment, there may exist wrong usages of the dataset, such as training a rewriter that converts safe responses to unsafe ones and using the trained safety checker or span tagger to gather unsafe expression for misconduct. SAFECONV is available to the public under a usage agreement for research and related purposes only and we urge people interested to use it ethically. Limitations For the dataset, although we adopt several methods to assure a high quality of the dataset, mislabeled data still exist due to the subjectivity of the annotators. For example, annotators may have different opinions on whether to regard 屁民(shitizen ) as unsafe because 屁民(shitizen ) is a rare word in Chinese and could be both derogatory and selfdeprecating humorously in most cases. Moreover, our dataset is in Chinese. Directly translating SAFECONV to other languages with translation tools may induce erroneous labels due to syntactic and cultural differences between languages. We call for endeavors to fix it, such as annotating similar datasets in other languages or improving translation strategies. For the experiments, firstly, in Section 6, we evaluate the performance of the rewriter based on chatbots of restricted sizes. However, there are large chatbots that we do not include in the evaluation due to the limitation of computing resources, such as EV AxLarge with up to 2.8B parameters, on which the detoxifying results will lead to more comprehensive results. Secondly, as shown in Table 8, the overall contextual coherence and informativeness of the responses from current stateof-theart chatbots in Chinese are still not satisfying. EvaluatingSAFECONV on more powerful chatbots based on large language models is worth exploring in the future. Acknowledgements We thank the anonymous reviewers for their valuable comments.30 experiments.1 1 Introduction Hallucinations in machine translation (MT) are cases when the model generates output that is partially or fully unrelated to the source sentence. While generally this phenomenon is not frequent and has low impact on corpuslevel automatic metrics, the impact of hallucinations on user experience can be rather dramatic. For example, if a translation system generates The staff were very friendly and helpful in response to an input sentence about e.g.a marvelous view from the window , a user is unlikely to trust this system in future. 1https://github.com/facebookresearch/stopes/tree/ main/demo/alti/detecting_hallucinationsWhile the problem of hallucinations is known, addressing it remains challenging. Firstly, hallucinations are very rare. This is why previous work mostly resorted to settings where models are encouraged to hallucinate, by e.g. artificially perturbing source sentence (Lee et al., 2019; Raunak et al., 2021), adding specific types of noise to the training data (Raunak et al., 2021), working under domain shift (Wang and Sennrich, 2020; Müller et al., 2020), among others (Zhou et al., 2021). Secondly, hallucinations are hard to identify with automatic metrics. Often, hallucinations were defined as translations with low quality according to some metric such as adjusted BLEU or chrF (Lee et al., 2019; Raunak et al., 2021; Müller and Sennrich, 2021) or translations satisfying some heuristic condition (Berard et al., 2019; Raunak et al., 2021). Overall, it is not clear whether proposed methods detect naturally occurring hallucinations well. Recently, when revisiting previous work in a relatively clean setting, Guerreiro et al. (2022) found that existing detection methods fall short and the standard sequence logprobability is the most informative. To show this, the authors gathered a large dataset with professional annotations of translations that, according to 10 previously proposed methods, are likely to be hallucinations. This data (hallucinations along with the model that generated them) made it possible to first, evaluate the performance of various detection methods and second, to work on alleviating hallucinations at test time. For the latter, the idea is “detectthen-rewrite”: after flagging a translation as likely to be pathological, generate several alternative hypotheses and pick the best one relying on some measure. So far, the best realization of this general framework uses sequence logprobability – SeqLogprob – for detection, Monte Carlo dropout (Gal and Ghahramani, 2016) to generate several alternative translation hypotheses, and COMETQE to pick the final candidate (see Guerreiro et al. (2022) for the details).36We use the same test bed and substantially improve previous results. Regarding hallucination detection, we view the observation that SeqLogprob outperforms previous (specifically targeted to hallucinations) methods as follows: internal model characteristics may contain much more information than we expect . Therefore, before developing or using external models and measures, we ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, since hallucinations are translations that are “detached” from the source, low source contribution should be able to identify hallucinations. Despite the fact that understanding hallucinations was one of the motivations behind the first method evaluating relative source and target contributions, both existing methods only looked at highly artificial hallucinations (V oita et al., 2021; Ferrando et al., 2022). We propose to use ALTI+ by Ferrando et al. (2022), the method that aggregates layerwise tokens attributions, for both hallucination detection and reranking in the “detectthen-rewrite” pipeline. For detection of the most severe hallucinations, it is twice more accurate than SeqLogprob. For reranking, it performs on par with the previous best COMETQE . All in all, we improve the overall pipeline results by relying on internal model characteristics alone. When allowing external tools, previous work mostly focused on different ways to automatically evaluate quality of a translation example, either with stringbased methods or neural quality estimation systems. This idea (the better we estimate translation quality, the better we are at detecting hallucinations) is natural: hallucinations are lowquality translations in the first place. However, implementing this idea in practice is challenging: even stateof-theart quality estimation system substantially fails (Guerreiro et al., 2022). We hypothesize that instead of targeting quality evaluation, it might be beneficial to use models trained with a rather different objective. Indeed, as we show in this paper, similarity between the source and a translation estimated via crosslingual sentence embeddings outperforms the best internal method. Apart from crosslingual sentence similarity (which is expected to be sensitive to highly incorrect translations), we find that crosslingual natural language inference models (less anticipated in the context ofmachine translation) also perform quite well. To the best of our knowledge, we are the first to apply these models for hallucination detection. Overall, we show that: •by using only the model’s inner workings, we ◦detect the most severe type of hallucinations with twice better precision; ◦alleviate hallucinations at test time with results on par with the best previous method that relies on an external model; •models focused on semantic similarity of sentences detect all types of hallucinations with precision 80% higher than previous methods. 2  evaluation, it might be beneficial to use models trained with a rather different objective. Indeed, as we show in this paper, similarity between the source and a translation estimated via crosslingual sentence embeddings outperforms the best internal method. Apart from crosslingual sentence similarity (which is expected to be sensitive to highly incorrect translations), we find that crosslingual natural language inference models (less anticipated in the context ofmachine translation) also perform quite well. To the best of our knowledge, we are the first to apply these models for hallucination detection. Overall, we show that: •by using only the model’s inner workings, we ◦detect the most severe type of hallucinations with twice better precision; ◦alleviate hallucinations at test time with results on par with the best previous method that relies on an external model; •models focused on semantic similarity of sentences detect all types of hallucinations with precision 80% higher than previous methods. 2 analysis, the model was trained on the remaining 2/3 of the dataset. We use the model released by Guerreiro et al. (2022) that has been used to generate the hallucinations we analyze. 2.2 Hallucination Dataset The hallucination dataset released by Guerreiro et al. (2022) contains finegrained manual annotations of 3415 Germanto-English translations generated by the model above. These translations are chosen from a set of 1.8M translations of heldout data as the ones that are likely to be pathological. The criteria used to flag the translations include 10 methods ranging from previously proposed heuristics (Lee et al., 2019; Berard et al., 2019; Raunak et al., 2021) to quality estimation models (Rei et al., 2020b) and uncertainty detectors (Fomicheva et al., 2020; Zerva et al., 2021; Guerreiro et al., 2022).37Figure 1: Taxonomy of translation types (based on the dataset by Guerreiro et al. (2022)). The taxonomy of translation pathologies in the dataset is shown in Figure 1. Here, hallucinations are defined as severe translation errors that are detached from the source. These can be either oscillatory (i.e. contain erroneous repetitions of words and phrases) or largely fluent. The latter is further split by severity of an error into fully detached (the whole content is not supported by the source) and strongly, but not fully, detached (significant proportion of output is not supported by the source).2Additionally, the annotated data contains translation errors that are deemed not detached from the source (Figure 1). Overall, 323 examples are judged to be hallucinations, 1044 are less severe translation errors and the rest are correct translations. Note that so far, there is no “canonical” hallucination taxonomy and previous work used various, mostly overlapping, definitions (Lee et al., 2019; Raunak et al., 2021; Zhou et al., 2021; Ji et al., 2022; Raunak et al., 2022; Guerreiro et al., 2022). We follow the taxonomy by Guerreiro et al. (2022) for consistency with the dataset and the evaluation framework we use and because this taxonomy is general enough for our purposes. 3 Hallucination Detection Methods Generally, methods for handling hallucinations can be either internal , i.e. using only information coming from the translation model itself, or external , i.e. using auxiliary models. In addition to these, we also consider “oracles” relying on reference translation. Note that these cannot be used in preventive settings when references are not available; here we use them only for analysis. 2Guerreiro et al. (2022) mention that oscillatory hallucinations can also be either fully or strongly detached, but they do not divide this category into smaller groups because the overall number of such translations is rather small.3.1 ReferenceBased Oracles Following previous work (Müller and Sennrich, 2021; Guerreiro et al., 2022), we use: •chrF : character ngram F score of the translation with respect to the reference. We use the CHRF++ version that also takes into account word unigrams and bigrams (Popovi ´c, 2017); •COMET : a neural quality estimation metric by Rei et al. (2020a) which was shown to be the stateof-theart referencebased experiments on three datasets show that our model outperforms stateof-theart baselines (for example, 3.4% improvement in prediction and 15.8% improvement in explanation for TripAdvisor). 1  Evaluation Metrics For rating prediction, in order to evaluate the recommendation performance, we employ two commonly used indicators: Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), which measure the deviation between the predicted ratings rand the ground truth ratings r∗. For generated text, we adopt a variety of indicators that consider the quality of the generated text from different levels.BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BERTscore (Reimers and Gurevych, 2019) are commonly used metrics in natural language generation tasks. BLEUN (N=1,4) mainly counts on the Ngrams. R2P, R2R, R2F, RLP, RLR and RLF denote Precision, Recall and F1 of ROUGE2 and ROUGEL. BERTS represents similarity scores using contextual embeddings to calculate. They are employed to objectively evaluate the similarity between the generated text and the targeted content. 5.3 Baseline Methods 5.3.1 Prediction The performance in terms of accuracy of rating prediction is compared with two types of methods: Machine Learning and Deep Learning: •Deep learning models: NARRE (Chen et al., 2018) is a popular type of neural network for textbased tasks. PETER (Li et al., 2021b) and NRT (Li et al., 2017) are deep learning models that use review text for prediction and explanation at the same time. •Factorization methods: PMF (Salakhutdinov andMnih, 2007) is a matrix factorization analysis of prediction and explanation tasks points mean very satisfied, and 1 point means very bad. Table 5 reports the human evaluation results. Kappa (Li et al., 2019) is an indicator for measuring classification accuracy. Results demonstrate that our model outperforms the other three methods on fluency and Kappa metrics. 5.6 Accuracy of Prediction The evaluation result of prediction accuracy is shown in Table 2. As we can see, it shows that our Experiments In this section, we evaluate the effectiveness of our lowbit quantization scheme for natural language generative model on text summarization benchmarks: CNN/DailyMail (Nallapati et al., 2016) and XSUM (Narayan et al., 2018). We additionally experiment on the machine translation task with mBART on WMT16 EnglishRomanian (EnRo) dataset (Bojar et al., 2016a). 3.1 Experimental settings We follow recent work (Li et al., 2022) in training the quantized network with initialization and knowledge distillation from a fullprecision pretrained model. Specifically, we use the BARTbase (Lewis et al., 2019) as our fullprecision baseline for summarization tasks and mBARTlarge (Liu et al., 2020a) for the translation task. We train the quantized models for 20 epochs on 8 GPUs with a batch size of 128 and a learning rate of 2.5e4 for 8bit activation models and 5e4 for binary and ternary activation models. 3.2 Summarization For the summarization task, we adopt the following benchmarks: The XSUM dataset (Narayan et al., 2018) consists of 226k documents sampled from the online news website of BBC, together with short, one sentence summaries. Since the summaries are very short, abstractive methods tend to do better on this dataset.69Table 1: Comparison of quantization methods for text summarization on XSUM and CNN/DailyMail benchmarks. We use the “EW-A (#bits)” notation referring to the number of bits of embeddings, weights and activations, (specifically, 1denotes binary, 2denotes ternary). The results of QuantBart, DQBART and BlockPruning are quoted from their paper. Additionally, we implement the algorithm developed in BinaryBert, BiBert and TernaryBert to the BART model and report the results, denoted with∗. We use the rouge- {1,2,L}as evaluation metrics. XSUM CNN/DailyMail  evaluation metrics. XSUM CNN/DailyMail analysis In language generation tasks, the error compounding issue in the recursive decoder generation process will largely amplify the quantization error or even lead to divergent results, and thus is an harsh factor to test the robustness of a quantization experiments on the SchemaGuided Dialogue (SGD) (Rastogi et al., 2020), SGDX (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. SchemaGuided Dialogue. Unlike other taskoriented dialogue datasets which assume a single, fixed ontology at training and test time the SGD dataset includes new and unseen slots and services in the test set. This allows us to not only measure DST performance but also zeroshot generalization to unseen services. The dataset includes natural language descriptions for all intents and slots in its schema. We follow the standard evaluation setting and data split suggested by the authors. SGDX. The SGDX benchmark is an extension of the SGD dataset which provides five additional schema variants of different linguistic styles which increasingly diverge in style from the original schema with v1being most similar and v5least similar. We can evaluate our model’s robustness to variations in schema descriptions by training our model on SGD and comparing evaluation results using the different included schema variants. MultiWOZ. The MultiWOZ dataset is set of humanhuman dialogues collected in the Wizardof-OZ setup. Unlike in SGD the ontology is fixed and there are no unseen services at test time. There are multiple updated versions of the original MultiWOZ dataset (Budzianowski et al., 2018): MultiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds coreference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology. We therefore choose the MultiWOZ 2.2 dataset for our experiments. We follow the standard evaluation setting and data split.813.2 Evaluation Metrics In line with prior work (Rastogi et al., 2020) we evaluate our Experimental Setup We describe our experimental setup including datasets used for pretraining and evaluation, implementation details, baselines and evaluation metrics in detail below. 3.1 Benchmark Datasets We conduct experiments on the SchemaGuided Dialogue (SGD) (Rastogi et al., 2020), SGDX (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. SchemaGuided Dialogue. Unlike other taskoriented dialogue datasets which assume a single, fixed ontology at training and test time the SGD dataset includes new and unseen slots and services in the test set. This allows us to not only measure DST performance but also zeroshot generalization to unseen services. The dataset includes natural language descriptions for all intents and slots in its schema. We follow the standard evaluation setting and data split suggested by the authors. SGDX. The SGDX benchmark is an extension of the SGD dataset which provides five additional schema variants of different linguistic styles which increasingly diverge in style from the original schema with v1being most similar and v5least similar. We can evaluate our model’s robustness to variations in schema descriptions by training our model on SGD and comparing evaluation results using the different included schema variants. MultiWOZ. The MultiWOZ dataset is set of humanhuman dialogues collected in the Wizardof-OZ setup. Unlike in SGD the ontology is fixed and there are no unseen services at test time. There are multiple updated versions of the original MultiWOZ dataset (Budzianowski et al., 2018): MultiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds coreference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology. We therefore choose the MultiWOZ 2.2 dataset for our experiments. We follow the standard evaluation setting and data split.813.2 Evaluation Metrics In line with prior work (Rastogi et al., 2020) we evaluate our evaluation, implementation details, baselines and evaluation metrics in detail below. 3.1 Benchmark Datasets We conduct experiments on the SchemaGuided Dialogue (SGD) (Rastogi et al., 2020), SGDX (Lee et al., 2022) and MultiWOZ 2.2 (Zang et al., 2020) datasets. SchemaGuided Dialogue. Unlike other taskoriented dialogue datasets which assume a single, fixed ontology at training and test time the SGD dataset includes new and unseen slots and services in the test set. This allows us to not only measure DST performance but also zeroshot generalization to unseen services. The dataset includes natural language descriptions for all intents and slots in its schema. We follow the standard evaluation setting and data split suggested by the authors. SGDX. The SGDX benchmark is an extension of the SGD dataset which provides five additional schema variants of different linguistic styles which increasingly diverge in style from the original schema with v1being most similar and v5least similar. We can evaluate our model’s robustness to variations in schema descriptions by training our model on SGD and comparing evaluation results using the different included schema variants. MultiWOZ. The MultiWOZ dataset is set of humanhuman dialogues collected in the Wizardof-OZ setup. Unlike in SGD the ontology is fixed and there are no unseen services at test time. There are multiple updated versions of the original MultiWOZ dataset (Budzianowski et al., 2018): MultiWOZ 2.1 (Eric et al., 2020) and MultiWOZ 2.2 (Zang et al., 2020) fix annotation errors of previous versions, MultiWOZ 2.3 (Han et al., 2021) is based on version 2.1 and adds coreference annotations, MultiWOZ 2.4 (Ye et al., 2022) is also based on version 2.1 and includes test set corrections. However, MultiWOZ 2.2 is the only version of the dataset which includes a fully defined schema matching the ontology. We therefore choose the MultiWOZ 2.2 dataset for our experiments. We follow the standard evaluation setting and data split.813.2 Evaluation Metrics In line with prior work (Rastogi et al., 2020) we evaluate our analysis on JGA. We find that SPLAT base outperforms most similarlysized models including D3ST base and large and that SPLAT largeperforms better than all models aside from the more than 30 ×larger D3ST XXL. The notable exceptions to this are AGDST and DaP (ind). AGDST large achieves performance that is similar to SPLAT large using a generative experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at https://github.com/EricLee8/MPDRG . 1 Introduction Inspired by the tremendous success in pretraining large language models (PLMs) in general domains (Devlin et al., 2019; Clark et al., 2020; Radford et al., 2018), efforts have been made to train PLMs for dialogue response generation (Zhang et al., 2020; Bao et al., 2020; Chen et al., 2022). However, they constrain the dialogues to be either twoparty, or sequential structured (i.e. each utterance replies directly to its previous utterance). Different from them, a multiparty dialogue can involve multiple interlocutors, where each interlocutor can reply to ∗Corresponding author. This paper was partially supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011).any preceding utterances, making the response relations of the dialogue treestructured and much more complicated (Zhang et al., 2018; Le et al., 2019; Shi and Huang, 2019; Wang et al., 2020). Besides, the speaker and addressee of a response utterance should be specified before it is generated in multiparty scenario, making the annotated data for multiparty dialogue response generation (MPDRG) less available. Figure 1 illustrates an example of MPDRG task taken from the Ubuntu IRC benchmark (Hu et al., 2019). The upper part shows the treestructured addressee relations of the dialogue, where the arrows point from addressees to speakers, and different colors represent different interlocutors. The middle part displays the content of the dialogue history, where U7is the response to be generated. The addressee ( U6) and the speaker (#4) of it are given, and the content of this response is the target of our model. The lower part gives the human response, which is also called the ground truth reference. Previous works on MPDRG finetune generative PLMs on small multiparty dialogue datasets with explicit addressee annotations. They utilize the response annotations to form a treestructured response graph, then encode the dialogue history using either homogeneous or heterogeneous Graph Neural Networks (GNNs) (Hu et al., 2019; Gu et al., 2022). Nevertheless, none of them make attempts to pretrain a response generation model for multiparty dialogues due to the lack of largescale corpora with annotated addressee labels. To solve the aforementioned problem of data scarcity, we propose an EM approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Specifically, we treat the addressee of each utterance in the dialogue history as a discrete latent variable z. During the Esteps, given the current dialogue history ctand the the response utterance rt, we92model the distribution of the current addressee ztas p(zt|ct, rt;θ), where θis the current model parameters. During the Msteps, we sample (ct, rt, zt) triplets from distribution p(zt|ct, rt;θ)and optimize the generative model p(rt|ct, zt;θ)on these samples. With the iteration number increasing, the accuracy of latent variable prediction and the quality of generated responses will grow together. It is worth noting that during these iterations, annotated addressee labels are not required, which makes it possible to leverage the huge amount of multiparty dialogue corpora without addressee labels. We provide theoretical analyses to prove the feasibility of our EM method, and conduct experiments on the Ubuntu IRC benchmark, which is used in previous works (Hu et al., 2019; Gu et al., 2022). The contributions of our work can be summarized as the following three folds: •To the best of our knowledge, we are the first to study the pretraining of multiparty dialogue response generation, which is much more challenging and complicated than twoparty dialogues. •We put forward an EM approach to alleviate the scarcity of multiparty dialogue data with addressee labels, making it possible to pretrain a model with huge amount of unlabeled corpora. •We provide theoretical analyses to prove the feasibility of our EM pretraining method, and experimental results on the Ubuntu IRC benchmark show our pretrained model achieves stateof-theart performance compared with previous works. 2 Related Works 2.1 Pretraining for Response Generation In recent years, researchers have gradually drawn their attention from retrievalbased dialogue systems to generationbased ones. Thanks to the huge amount of twoparty dialogue corpora, various PLMs for twoparty dialogue response generation have been proposed. Zhang et al. (2020) propose DialoGPT, which utilizes the sequential response chains in the Reddit Corpus to pretrain an autoregressive response generation model based on the architecture of GPT (Radford et al., 2018). Different from their work, which focuses on sequential dialogue history, our work aims to solve the case where the agent can respond to any previous utterance in a treestructured dialogue history. Bao et al. (2020) propose PLATO, which models the conversational intents as Kdiscrete latent U1:[Speaker #1:Irunacomp, with 256 mbof ram, canIruncompiz orberyl init?] U2:[Speaker #2:I t ’sthegraphics card that is important .] U3:[Speaker #3:I’malittle fuzzy oncommand lineparams forthat, isitthrough iwconfig ?] U4:[Speaker #1:Can’t Irunitwithout one?] U5:[Speaker #4:That didn’t tellmemuch. ] U6:[Speaker #5:Use the“rdesktop” which isa “terminal server client” intheinternet menu. ] U7:[Speaker #4:________________________ ]U4 U1 U6 U2U5 U3U7Addressee Relations: Dialogue History: U7:[Speaker #4:Erm, how doIrunrdesktop?]Human Response:Figure 1: An example of multiparty dialogue response generation task, better view in color. variables, then utilizes response selection, bagof- words prediction, and language modeling objectives to train the model. DialogVED (Chen et al., 2022) further extends the discrete latent variables to continuous ones, and models them with a multivariable Gaussian distribution. It utilizes KL divergence reduction to optimize the parameters of the latent distribution and applies masked language modeling, response generation, and bagof-words prediction to train the whole model. PLATO and DialogVED focus on twoparty conversations, and the conversational intents they put forward have no corresponding concepts of actual entities (e.g., intent to argue, intent to end a conversation, and so on). Distinct from their works, we lay emphasis on multiparty dialogues, and the latent variables of our method have actual meanings: variable zt=j indicates that the addressee of the response at the tthturn is the jthutterance. 2.2 Multiparty Dialog Response Generation Several previous works have studied the MPDRG task. Hu et al. (2019) extract a subset of the Ubuntu Dialogue Corpus (Lowe et al., 2015) with explicit addressee labels to construct the Ubuntu IRC benchmark, where they propose a Graph Structured Neural Network (GSN) for dialogue modeling. Specifically, they first treat each utterance93… …… … … … S1:U1[SEP] Sj:Uj[SEP] St1:Ut1[SEP] St:… … Embedding LayersWord Embeddings:Positional Embeddings:Addressee Embeddings: Input Sequence:Generative Pre -trained Language Models (BART) Ut ct zt rtBayesian NetworkFigure 2: The overview of our model architecture. The left part shows how we incorporate the addressee information into response generation by adding addressee embeddings. The right part illustrates a Bayesian Network of how a response is generated given the current dialogue history ctand the addressee zt. of a dialogue as a node, and the addressee relations as edges to construct a dialogue graph, then make use of GNNs to encode the dialogue history. Finally, they adopt a Gated Recurrent Unit (GRU) with cross attention as the decoder to generate responses. Gu et al. (2022) put forward HeterMPC, which models the dialogue history as a heterogeneous graph. In detail, they first design six types of edges: reply and repliedby, address and addressedby, speak and spokenby, among two kinds of nodes: interlocutor nodes and utterance nodes, and then encode the dialogue history using Transformers (Vaswani et al., 2017) together with heterogeneous GNNs. Finally, they utilize a Transformer Decoder to generate responses. Instead of finetuning models on a small dataset with annotated addressee labels as these existing work did, our work focuses on the utilization of large unlabeled corpora to pretrain a response generation model for multiparty dialogues. 3 Methodology To design a model for multiparty dialogue response generation and make it compatible with the EM training algorithm, there are two important things to consider: how to model p(rt|ct, zt;θ) in the maximization step, and how to compute p(zt|ct, rt;θ)in the expectation step. In this section, we will first address these two problems, then mathematically derive the feasibility of our EM pretraining algorithm. 3.1 Task Formulation Given an input sequence of the dialogue history and the speaker of the response at time step t,X= {S1: U1[SEP]S 2: U2[SEP] . . .St1: Ut1[SEP]S t:}, together with the addressee of the response zt=j, our goal is to train a model that can generatean response Y= U t. Here each Siis the name of the speaker at time step i, which is represented as Speaker # Silike those in Figure 1. Ui={wi1, wi2, . . . , w ini}is the content of the ith utterance with niwords. zt=jrepresents that St speaks to Sj, who utters Uj, and [SEP] is a special token that indicates the end of a dialogue turn. 3.2 Addressee Modeling In this section, we answer the first question: how to model p(rt|ct, zt;θ), or in other words, how to incorporate the addressee information zt=jinto the process of generating a response rt. We design a straightforward method that adds addressee embeddings to the positional encodings and word embeddings, before they are further encoded by a PLM. The left part of Figure 2 illustrates this method, where we use an embedding lookup table with 2entries to indicate whether a word belongs to the addressee utterance or not. Specifically, if a word is in the addressee utterance, it will get its addressee embedding from entry 1, otherwise from entry 0. Since addressee modeling is not the key contribution of this work, we just adopt the most straightforward and effective way. In our experiments, we use BART (Lewis et al., 2020) as the backbone PLM, following previous works (Gu et al., 2022). Due to the page limit, the proverbial architecture of Transformer and BART are omitted here. 3.3 Latent Variable Prediction In this section, we answer the second question: how to compute p(zt|ct, rt;θ)in the expectation step, or in other words, how to predict the distribution of the unlabeled addressee zt, given the current dialogue context ct, response rt, under parameters θ. The solution to this question is essentially the most94important part of our method since it delicately solves the problem of data scarcity in MPDRG. Let’s consider what humans will do to participate in a multiparty conversation. First, we will read the dialogue history ct, then choose an addressee ztto reply. Once ctandztare determined, we will utter a response according to the content of the whole dialogue and the addressee utterance. The right part of Figure 2 gives the Bayesian Network of the above process, where the joint distribution of(ct, zt, rt)can be factorized as: p(c, z, r ) =p(c)·p(z|c)·p(r|c, z) (1) Here we omit the subscript tand model parameters θfor simplicity. Given Eq. (1), p(z|c, r;θ)can be derived as: p(z|c, r) =p(c, z, r ) p(c, r) =p(c)·p(z|c)·p(r|c, z) p(c)·p(r|c) =p(z|c)·p(r|c, z) p(r|c)(2) We assume that the probability of choosing any previous utterance as the addressee is the same given the current dialogue history, which means p(z|c)obeys a uniform distribution. Meanwhile, the denominator p(r|c)is independent of z, leaving only the term p(r|c, z). Now, we can induce that: p(z|c, r)∝p(r|c, z) (3) Therefore, for each zi, i= 1,2, . . . , t −1, we have: p(zi|c, r) =p(r|c, zi)/summationtextt−1 j=1p(r|c, zj)(4) In practice, we can use the generative model p(rt|ct, zt;θ)to compute the probability distribution of p(zt|ct, rt;θ)by Eq. (4). 3.4 ExpectationMaximization Process Figure 3 illustrates the overview of our EM training process. During the Esteps, we compute the probability distribution of the latent variable (the addressee z). During the Msteps, we sample (c, r, z ) triplets from this distribution and optimize the generative model by standard training algorithms. The Expectation Step is to compute the conditional distribution of the latent variable zt, given the observed data (ct, rt)and the current model …c, r z1 c, r z2 c, r zt1Generative Model c, r zj…p(r | c, z1) p(r | c, z2) p(r | c, zj) p(r | c, zt1)NormalizationDistribution of 𝑝𝑝 𝑧𝑧𝑐𝑐,𝑟𝑟) … … z1z2 zj zt1 (c, r, zj)Sample (c, r, zj) triplets from 𝑝𝑝𝑧𝑧𝑐𝑐,𝑟𝑟)Training…… The Maximization StepThe Expectation StepFigure 3: The overview of the EM process, where the expectation steps and maximization steps are performed alternately and iteratively. parameters θ, where Eq. (4) gives a reasonable approximation of this value. Specifically, for a sample (ct, rt), with the model parameters θfixed, we first calculate the unnormalized probability of each of the ith(i < t ) utterance being the addressee: p(rt|ct, zi t;θ)using Eq. (3), then normalize them to get the conditional distribution of ztusing Eq. (4). Once P(zt|ct, rt;θ)is obtained, we sample (ct, rt, zt)triplets from this distribution, which is further used in the maximization step. The Maximization Step is analogical to the normal training process. Given the sampled {(ck t, rk t, zk t)}N k=1triplets, where Nis the total number of samples, our goal is to minimize the autoregressive language modeling loss: LG=−N/summationdisplay k=1nk/summationdisplay i=1logp/parenleftig wk i|wk <i, ck t, zk t;θ/parenrightig (5) where wk iis the ithword in the response of the kth sample: rk t={wk i}ni i=1, andniis the length of this response. Compared with the vanilla EM algorithm , there are several differences in our implementations. First of all, we do not use the initial model to generate the training data for the first round of the maximization step. Instead, we utilize the discourse parser provided by Shi and Huang (2019) to predict the addressee of each utterance in the unlabeled corpus to get a coarse initial training dataset. The reason for this initialization method is that the initialization of training data (or model parameters) is vital to the EM method, which helps it converge to a better point. Second, rather than sampling ztfrom its conditional distribution, we adopt a hard EM approach which takes the value zi twith highest probability as the predicted label, where i= arg max ip(zi t|ct, rt;θ). This hard EM95approach is proved as more effective to boost the performance (Min et al., 2019). Finally, to ensure the quality of the generated training data in the maximization step, we set a hyperparameter α∈[0,1] to control the proportion of training data that is actually used. Specifically, we first rank the prediction confidence of each zk taccording to the value ofp(zk t|ck t, rk t;θ), then pick the top α×Nsamples with the highest confidence scores. In our experiments, αis dynamically set to ensure the addressee prediction accuracy of the selected samples is over 80% in an annotated validation set. 3.5 Proof of Feasibility In a multiparty dialogue corpus without annotated addressee labels, a usual solution to train a response generation model is to maximize the marginal loglikelihood (or incomplete loglikelihood) over all possible addressees: ℓ(c, r;θ) = log p(r |c;θ) = log/summationdisplay ip(r,zi|c;θ) (6) However, this objective is hard to optimize since the distribution of zis hard to obtain. Here, we define an expected complete loglikelihood where our estimation of p(zt|ct, rt;θ)can come to rescue: ˆℓ(c, r;θ) =q(zi)/summationdisplay ilog p(r ,zi|c;θ) q(z) =p(zt|ct, rt;θ)(7) Our new objective now becomes maximizing the expected complete loglikelihood. The relation between ℓandˆℓcan be derived as follows: ℓ(c, r;θ) = log/summationdisplay ip(r,zi|c;θ) = log/summationdisplay iq(zi)·p(r,zi|c;θ) q(zi) ≥/summationdisplay iq(zi)·logp(r,zi|c;θ) q(zi) =/summationdisplay iq(zi)·log p(r ,zi|c;θ) −/summationdisplay iq(zi)·log q(z i) =ˆℓ(c, r;θ) +Hq(z)(8) where the third line is derived from the Jensen Inequality , andHq(z)is the entropy of the distribution of z. Since Hq(z)≥0, we can derive that ˆℓ(c, r;θ)≤ℓ(c, r;θ), which means ˆℓis the lowerbound of ℓ. By maximizing the lower bound ˆℓ, we can indirectly maximize ℓ, which is originally hard to optimize. Another important observation is hat ˆℓ=ℓif and only if q(z) =p(zt|ct, rt;θ), which is exactly what we calculate during the Esteps in Eq. (7). Though the derivation of the posterior distribution of zis not exact since we assume uniform prior in Eq. (2), it is still much closer to the real distribution compared to random q(z). It is worth noting that the global optimal point is not guaranteed to be reached by this algorithm, and it depends heavily on the initialization of model parameters or the training data for the first round of the maximization step. This explains the reason why we utilize a discourse parser to get a coarse initial training dataset instead of using the expectation step at the first iteration in Section 3.4. 4 Experiments In this section, we first introduce the datasets to pretrain and evaluate our model, then present the experimental results and comparisons with previous methods. 4.1 Datasets and Experimental Setups For pretraining, we adopt the second version of Ubuntu Dialogue Corpus (Lowe et al., 2015), which contains no annotated addressee labels. The original dataset contains 1M dialogues for training, and0.5M dialogues for validation and testing, respectively. Dialogues that contain less than 4turns, or have overlap with the dataset for the downstream task (the Ubuntu IRC benchmark, Hu et al. 2019), are excluded from the pretraining data. After filtering, we eventually get a pretraining dataset that contains 764,373 dialogues. For finetuning, we follow previous works (Hu et al., 2019; Gu et al., 2022) to adopt the Ubuntu IRC benchmark, which is constructed by extracting all utterances with response addressees indicated by the “@\" symbol in the Ubuntu Dialogue Corpus. In total, this dataset consists of 311,725 dialogues for training, and 5,000 dialogues for validation and testing, respectively. It is worth noting that this dataset contains addressee labels for every single utterance in the dialogue history, which are utilized by previous methods, yet not by ours. For both pretraining and finetuning, BART (Lewis et al., 2020) is used as the backbone model. Before pretraining, we initialize the pretrained weights from BARTbase. During the process of96Model BLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGEL GPT2 (Radford et al., 2018) 10.37 3 .60 1 .66 0 .93 4 .01 9 .53 GSN (Hu et al., 2019) 10.23 3 .57 1 .70 0 .97 4 .10 9 .91 HeterMPC BART (Gu et al., 2022) 12.26 4 .80 2 .42 1 .49 4 .94 11 .20 BART (Lewis et al., 2020) 11.25 4 .02 1 .78 0 .95 4 .46 9 .90 Pretraining Only (PO) 11.78 4 .67 2 .38 1 .41 4 .98 11 .19 Finetuning Only (FO) 11.47 5 .11 2 .98 2 .11 5 .23 11 .31 Pretraining + Finetuning (PF) 12 .31 5 .39 3 .34 2 .45 5 .52 11 .71 FO + ReplyChain 9.11 3 .52 1 .99 1 .35 4 .32 9 .36 PO w/o EM 10.03 3 .90 2 .03 1 .18 4 .56 9 .66 PF w/o EM 11.39 5 .04 3 .02 2 .15 5 .27 11 .20 Denoising + Finetuning 11.49 5 .08 3 .02 2 .13 5 .25 11 .28 Table 1: Results on the Ubuntu IRC benchmark, where the upper part presents models of previous works, the middle part shows our backbone model BART together with our method under different settings, and the lower part shows the ablation studies. pretraining, we evaluate our model on the validation set of the Ubuntu IRC benchmark, and the best checkpoint is saved for the finetuning process. 4.2 Baseline Models and Evaluation Metrics Table 1 shows the results of our method and previous models, where GPT2, GSN, and HeterMPC (Radford et al., 2018; Hu et al., 2019; Gu et al., 2022) are introduced in section 2.1 and 2.2, respectively. BART is a sequenceto-sequence model with encoderdecoder Transformer architecture and is trained using denoising objectives. Following Hu et al. (2019), we also adopt BLEU1 to BLEU4, METEOR, and ROUGEL as the automatic evaluation metrics, which can be calculated using the pycocoevalcap package. Besides automatic evaluation, human evaluation is also conducted and will be introduced in Section 4.4. 4.3 Automatic Evaluation Results Let’s firstly focus on the upper and middle part of Table 1, where we present the results of previous models and our methods. Three settings of our method based on BART are experimented with: pretraining only (PO), finetuning only (FO), and pretraining-finetuning (PF). Results of PO are obtained by directly using the pretrained model to generate the response for each dialogue. FO means the checkpoint of BART is directly finetuned on the Ubuntu IRC benchmark without pretraining. PF follows a pretraining-finetuning paradigm, where the best checkpoint of the pretraining process is further finetuned on the downstream dataset. Three observations can be seen from the table. First of all, solely pretraining with our proposed EM method with unlabeled corpus is alreadyModel Score Kappa Best (%) Human References 2.20 0 .56 28 .00 BART 1.68 0 .45 8 .00 HeterMPC BART 1.88 0 .48 8 .00 Ours (PF) 1.92 0.47 28 .00 Table 2: Human evaluation results, where Score is the average score and Best means the ratio of each system being the best response. able to achieve comparable results with the previous stateof-theart (SOTA) models. It is surprising since the pretraining requires no annotated addressee labels, while previous models not merely utilize the addressee information of the response utterance, but also make use of the addressee labels of the dialogue history to form a response graph. Second, finetuning our model on the downstream dataset with the ground truth addressee labels yields better results compared with pretraining only. Since it uses the ground truth addressee labels of responses, the results of it can be regarded as an upper bound of what the EM training can achieve. Besides, FO outperforms the previous SOTA model by large margins with even simpler architecture and fewer annotations (without addressee labels in the dialogue history), demonstrating the effectiveness of our proposed addressee embeddings. Finally, by further finetuning the pretrained checkpoint with the ground truth addressee labels, we achieve the best performance on all metrics, which shows the transferability of our pretrained model. 4.4 Human Evaluation Results For human evaluation, we recruit a team with 8 members who have at least a Bachelor’s degree in97Computer Science and are familiar with Ubuntu and Linux. We randomly sample 100examples from the testing set, then ask the team members to score each prediction and select the best one. The quality scores are considered in terms of three independent aspects: 1) relevance, 2) fluency and 3) informativeness. They are scored from 03 and the average values were reported. The evaluation results are shown in Table 2, where our model (Pretraining + Finetuning) constantly outperforms vanilla BART and the previous SOTA model HeterMPC BART . We also report the Fleiss’s Kappa to indicate the agreement between annotators. Besides, the ratio of our predictions being the best response is the same as that of human responses, demonstrating the high quality of the generated responses of our model. 5 Analysis In order to get more insights into the proposed EM pretraining method, we dive deeper into it by conducting extensive analyses. 5.1 Ablation Study We conduct ablation studies to investigate the contribution of our different designs, whose results are tabulated in the lower part of Table 1. Firstly, let’s focus on the first line of the lower part. To study whether other utterances that are not in the reply chain of the current addressee can help to generate a better response, we extract the reply train by traversing from the current leave utterance (the response) up to the root node (the first utterance), then train a model by inputting this chain only. We see a large performance drop on all metrics in this setting, demonstrating the significance of the side information provided by the whole context. Second, let’s pay attention to the second and third lines of the lower part. In order to study the effect of the EM pretraining process, which is the key contribution of our work, we remove this process and pretrain a model using only the addressee labels obtained from the discourse parser (i.e. the initial training data used in the first iteration of our EM approach). A sharp performance drop is observed compared with PO and PF with our proposed EM pretraining strategy, demonstrating the significance of our design. Without the iterative EM procedure, the noisy addressee labels obtained from the discourse parser can cause error propaga1 2 3 4 5 6 7 8 9 Number of iterations1.001.051.101.151.201.251.30BLEU4 BLEU4 5560657075 Accuracy AccuracyFigure 4: Line chart of the BLEU4 score and addressee prediction accuracy with the increase of EM iterations. tion, which makes the model learn noisy features to predict a response, and hurts the performance. Finally, aiming at investigating whether the performance gains come from seeing more indomain data in the pretraining process, we use the same pretraining data to train another model with the denoising objectives proposed in BART (Lewis et al., 2020), then also finetune it on the Ubuntu IRC benchmark. The last line of the lower part presents the results, where we observe nearly the same performance compared with FO. This observation indicates that simply performing domain adaptation using the general pretraining objectives is insufficient to benefit the MPDRG task. 5.2 Response Generation vs. Addressee Prediction In Section 3.3, we prove that p(z|c, r)∝p(r|c, z). To verify the correctness of this equation and also to investigate the training process of our EM strategy, we draw the line chart of the BLEU4 score and addressee prediction accuracy of the top30% confidence samples on the validation set with the increasing of pretraining iterations. The addressees are predicted using Eq. (4), where we take the ziwith the highest conditional probability as the predicted addressee. Figure 4 illustrates the trending of the BLEU4 score and addressee prediction accuracy. On the one hand, we see that the trending of both metrics is consistent, which means with a more powerful response generation model comes a higher addressee prediction accuracy. This observation verifies the correctness of Eq. (3). On the other hand, with the increasing of iterations, both metrics grow mutually, then reach their tops at around the 6thiteration, demonstrating the effectiveness of the EM process.98Addressee relations and dialogue history are in Figure 1. U7:[Speaker #4:Erm, how doIrunrdesktop?]Human Response: Generated Responses: Our Method :[Speaker #4:Well, how doIinstall rdesktop from theterminal? ] Baseline Model :[Speaker #4:Itried butitdidn’t work .]Figure 5: The first example of Case Studies, which shows the generated responses of our model and the baseline model. 5.3 Case Studies To understand the effect of our method intuitively, we sample two cases from the testing set and present them in this section. Figure 5 illustrates an example whose addressee relations and dialogue history are shown in Figure 1. This conversation is about how to run the compiz orberyl in a comp with 256MB RAM. Speaker #2points that it’s the graphic card that is important, but Speaker #4 seems unsatisfied by saying that didn’t tell me much . After that, Speaker #5 suggests using the rdesktop andSpeaker #4 replies him/her. Our model is able to capture the key information rdesktop andterminal in the addressee utterance U6, and generate a proper response Well, how do I install rdesktop from the terminal , which is very close to the human answer and even better with more information from the terminal . On the contrary, the baseline model (BART) fails to capture the addressee information and just replies with a safe response I tried but it didn’t work . This case shows the great significance of modeling the addressee information, and also demonstrates the effectiveness of our model design. Figure 6 presents another example sampled from the testing set, where we investigate how different addressee labels affect the generated responses. In the figure, different colors represent different utterances in the Dialogue History part, and different responses generated by giving the corresponding utterances as addressees in the Generated Responses part. This conversation is about discussing the file system in Ubuntu that can share on a network with windows machines. When the addressee is given asU1, our model suggests using samba , which is a solution to the question of U1. Responses to U2andU3are like safe responses, but they make sense in their contexts: the former expresses its confusion about a confusing utterance ( U2), and the latter expresses its gratitude to the suggestion in U4 U1U5 U2 U3U6 U1:[Speaker #1:What isagood filesystem torunwith ubuntu and also tobeable toshare onanetwork with windows machines? Does itmatter very much? ] U2:[Speaker #2:Windows canFILEP ATH toFILEP ATH .] U3:[Speaker #3:Usesamba forthenetwork connection . Thelocal filesystem doesn't matter over anetwork. ] U4:[Speaker #4:Yeshe's doing itonanetwork, fsis irrelevant .] U5:[Speaker #5:Gonna have some funwith gentoo on myother pcIhave here.] U6:[Speaker #6:_______________________________ ]Addressee Relations: Dialogue History: Generated Responses: U6:[Speaker #6:Samba isaway tocooperate with the windows environments and canbeadministrated bythe webwith wat.] U6:[Speaker #6:I'mnotsure what you'retalking about bysaying FILEP ATH toFILEP ATH .] U6:[Speaker #6:Thanks ,I'llgive itashot.] U6:[Speaker #6:Icanuseanyfilesystem isthatright? ] U6:[Speaker #6:I'musing gentoo onmycomputer too.]Figure 6: The second example of Case Studies, which illustrates the generated response of our model given different addressee labels. Better view in color. U3. Response to U4states his/her understanding towards U4, and questions if his/her understanding is right. Response to U5acknowledges the solution gentoo inU5by saying using gentoo on my computer too . In general, this case demonstrates the ability of our model to generate diverse responses according to the specified addressees and contexts of the dialogue history. 5.4 Response Parser: A Byproduct for Free Another contribution of our EM pretraining is that a response parser can be freely obtained. This byproduct comes from Eq. (4), where given a response generation model with addressee modeling, we can predict the addressee for each utterance in the dialogue. Previous literature has studied and proved that explicitly modeling the structural information is beneficial to understanding specific structured data. (Li et al., 2020, 2022a,b). In this context, the response parser can be used to infer the discourse structures, which contributes to boosting the performance of some multiparty dialogue comprehension tasks like response selection and question answering. (Jia et al., 2020; Li and Zhao, 2021; Ma et al., 2022)996 Conclusion Most multiparty dialogue corpora are not annotated with addressee labels, making them unable to support the pretraining of response generation models. To solve this problem, we design a simple yet effective way to model the addressee of a response as a latent variable and propose an EM pretraining approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Mathematical derivation, experimental results on the Ubuntu IRC benchmark, and extensive analyses have justified the theoretical feasibility and actual effectiveness of our method. Limitations First, Due to the lack of datasets to evaluate the MPDRG task, we perform our experiments only on the Ubuntu IRC benchmark and pretrain our model only on the domain of Ubuntu chats. However, the potential of our approach goes far beyond that since it is applicable to any opendomain multiparty dialogue dataset. In the future work, we will consider applying our method in more opendomain conversational datasets, such as the transcripts of TV series or movies. Additionally, the pretraining process solely relies on the addressee information of individual turns, disregarding the replyto relations within the dialogue history. This oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multiparty dialogue. In our future work, we will explore the integration of discourselevel replyto relations into the pretraining process to further enrich the capabilities of the model. References Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2020. PLATO: Pretrained dialogue generation model with discrete latent variable. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 85–96, Online. Association for Computational Linguistics. Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. 2022. DialogVED: A pretrained latent variable encoderdecoder model for dialog response generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers) , pages 4852–4864, Dublin, Ireland. Association for Computational Linguistics. Kevin Clark, MinhThang Luong, Quoc V . Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020 . OpenReview.net. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. JiaChen Gu, ChaoHong Tan, Chongyang Tao, ZhenHua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang. 2022. HeterMPC: A heterogeneous graph neural network for response generation in multiparty conversations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5086–5097, Dublin, Ireland. Association for Computational Linguistics. Wenpeng Hu, Zhangming Chan, Bing Liu, Dongyan Zhao, Jinwen Ma, and Rui Yan. 2019. GSN: A graphstructured network for multiparty dialogues. InProceedings of the TwentyEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 1016, 2019 , pages 5010–5016. ijcai.org. Qi Jia, Yizhu Liu, Siyu Ren, Kenny Zhu, and Haifeng Tang. 2020. Multiturn response selection using dialogue dependency relations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1911–1920, Online. Association for Computational Linguistics. Ran Le, Wenpeng Hu, Mingyue Shang, Zhenjun You, Lidong Bing, Dongyan Zhao, and Rui Yan. 2019. Who is speaking to whom? learning to identify utterance addressee in multiparty conversations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 1909–1919, Hong Kong, China. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequenceto-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online. Association for Computational Linguistics.100Jiaqi Li, Ming Liu, MinYen Kan, Zihao Zheng, Zekun Wang, Wenqiang Lei, Ting Liu, and Bing Qin. 2020. Molweni: A challenge multiparty dialoguesbased machine reading comprehension dataset with discourse structure. In Proceedings of the 28th International Conference on Computational Linguistics , pages 2642–2652, Barcelona, Spain (Online). International Committee on Computational Linguistics. Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022a. Semanticpreserving adversarial code comprehension. In Proceedings of the 29th International Conference on Computational Linguistics , pages 3017– 3028, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Yiyang Li and Hai Zhao. 2021. Selfand pseudoself- supervised prediction of speaker and keyutterance for multiparty dialogue reading comprehension. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2053–2063, Punta Cana, Dominican Republic. Association for Computational Linguistics. Yiyang Li, Hai Zhao, and Zhuosheng Zhang. 2022b. Back to the future: Bidirectional information decoupling network for multiturn dialogue modeling. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 2761–2774, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue , pages 285–294, Prague, Czech Republic. Association for Computational Linguistics. Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2022. Structural characterization for dialogue disentanglement. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 285–297, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2851– 2864, Hong Kong, China. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pretraining. OpenAI Technical Report . Zhouxing Shi and Minlie Huang. 2019. A deep sequential model for discourse parsing on multiparty dialogues. In The ThirtyThird AAAI Conference onArtificial Intelligence, AAAI 2019, The ThirtyFirst Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7007–7014. AAAI Press. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA , pages 5998–6008. Weishi Wang, Steven C.H. Hoi, and Shafiq Joty. 2020. Response selection for multiparty conversations with dynamic topic tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6581–6591, Online. Association for Computational Linguistics. Rui Zhang, Honglak Lee, Lazaros Polymenakos, and Dragomir R. Radev. 2018. Addressee and response selection in multiparty conversations with speaker interaction rnns. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI18), New Orleans, Louisiana, USA, February 27, 2018 , pages 5690–5697. AAAI Press. Yizhe Zhang, Siqi Sun, Michel Galley, YenChun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Largescale generative pretraining for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pages 270–278, Online. Association for Computational Linguistics.101ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? The last Section. /squareA2. Did you discuss any potential risks of your work? Not applicable. Left blank. /square\u0013A3. Do the experimental setup, including hyperparameter search and bestfound hyperparameter values? Section 4. /square\u0013C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of Evaluation Metrics Table 1 shows the results of our method and previous models, where GPT2, GSN, and HeterMPC (Radford et al., 2018; Hu et al., 2019; Gu et al., 2022) are introduced in section 2.1 and 2.2, respectively. BART is a sequenceto-sequence model with encoderdecoder Transformer architecture and is trained using denoising objectives. Following Hu et al. (2019), we also adopt BLEU1 to BLEU4, METEOR, and ROUGEL as the automatic evaluation metrics, which can be calculated using the pycocoevalcap package. Besides automatic evaluation, human evaluation is also conducted and will be introduced in Section 4.4. 4.3 Automatic Evaluation Results Let’s firstly focus on the upper and middle part of Table 1, where we present the results of previous models and our methods. Three settings of our method based on BART are experimented with: pretraining only (PO), finetuning only (FO), and pretraining-finetuning (PF). Results of PO are obtained by directly using the pretrained model to generate the response for each dialogue. FO means the checkpoint of BART is directly finetuned on the Ubuntu IRC benchmark without pretraining. PF follows a pretraining-finetuning paradigm, where the best checkpoint of the pretraining process is further finetuned on the downstream dataset. Three observations can be seen from the table. First of all, solely pretraining with our proposed EM method with unlabeled corpus is alreadyModel Score Kappa Best (%) Human References 2.20 0 .56 28 .00 BART 1.68 0 .45 8 .00 HeterMPC BART 1.88 0 .48 8 .00 Ours (PF) 1.92 0.47 28 .00 Table 2: Human evaluation results, where Score is the average score and Best means the ratio of each system being the best response. able to achieve comparable results with the previous stateof-theart (SOTA) models. It is surprising since the pretraining requires no annotated addressee labels, while previous models not merely utilize the addressee information of the response utterance, but also make use of the addressee labels of the dialogue history to form a response graph. Second, finetuning our model on the downstream dataset with the ground truth addressee labels yields better results compared with pretraining only. Since it uses the ground truth addressee labels of responses, the results of it can be regarded as an upper bound of what the EM training can achieve. Besides, FO outperforms the previous SOTA model by large margins with even simpler architecture and fewer annotations (without addressee labels in the dialogue history), demonstrating the effectiveness of our proposed addressee embeddings. Finally, by further finetuning the pretrained checkpoint with the ground truth addressee labels, we achieve the best performance on all metrics, which shows the transferability of our pretrained model. 4.4 Human Evaluation Results For human evaluation, we recruit a team with 8 members who have at least a Bachelor’s degree in97Computer Science and are familiar with Ubuntu and Linux. We randomly sample 100examples from the testing set, then ask the team members to score each prediction and select the best one. The quality scores are considered in terms of three independent aspects: 1) relevance, 2) fluency and 3) informativeness. They are scored from 03 and the average values were reported. The evaluation results are shown in Table 2, where our model (Pretraining + Finetuning) constantly outperforms vanilla BART and the previous SOTA model HeterMPC BART . We also report the Fleiss’s Kappa to indicate the agreement between annotators. Besides, the ratio of our predictions being the best response is the same as that of human responses, demonstrating the high quality of the generated responses of our model. 5 Analysis In order to get more insights into the proposed EM pretraining method, we dive deeper into it by conducting extensive analyses. 5.1 Ablation Study We conduct ablation studies to investigate the contribution of our different designs, whose results are tabulated in the lower part of Table 1. Firstly, let’s focus on the first line of the lower part. To study whether other utterances that are not in the reply chain of the current addressee can help to generate a better response, we extract the reply train by traversing from the current leave utterance (the response) up to the root node (the first utterance), then train a model by inputting this chain only. We see a large performance drop on all metrics in this setting, demonstrating the significance of the side information provided by the whole context. Second, let’s pay attention to the second and third lines of the lower part. In order to study the effect of the EM pretraining process, which is the key contribution of our work, we remove this process and pretrain a model using only the addressee labels obtained from the discourse parser (i.e. the initial training data used in the first iteration of our EM approach). A sharp performance drop is observed compared with PO and PF with our proposed EM pretraining strategy, demonstrating the significance of our design. Without the iterative EM procedure, the noisy addressee labels obtained from the discourse parser can cause error propaga1 2 3 4 5 6 7 8 9 Number of iterations1.001.051.101.151.201.251.30BLEU4 BLEU4 5560657075 Accuracy AccuracyFigure 4: Line chart of the BLEU4 score and addressee prediction accuracy with the increase of EM iterations. tion, which makes the model learn noisy features to predict a response, and hurts the performance. Finally, aiming at investigating whether the performance gains come from seeing more indomain data in the pretraining process, we use the same pretraining data to train another model with the denoising objectives proposed in BART (Lewis et al., 2020), then also finetune it on the Ubuntu IRC benchmark. The last line of the lower part presents the results, where we observe nearly the same performance compared with FO. This observation indicates that simply performing domain adaptation using the general pretraining objectives is insufficient to benefit the MPDRG task. 5.2 Response Generation vs. Addressee Prediction In Section 3.3, we prove that p(z|c, r)∝p(r|c, z). To verify the correctness of this equation and also to investigate the training process of our EM strategy, we draw the line chart of the BLEU4 score and addressee prediction accuracy of the top30% confidence samples on the validation set with the increasing of pretraining iterations. The addressees are predicted using Eq. (4), where we take the ziwith the highest conditional probability as the predicted addressee. Figure 4 illustrates the trending of the BLEU4 score and addressee prediction accuracy. On the one hand, we see that the trending of both metrics is consistent, which means with a more powerful response generation model comes a higher addressee prediction accuracy. This observation verifies the correctness of Eq. (3). On the other hand, with the increasing of iterations, both metrics grow mutually, then reach their tops at around the 6thiteration, demonstrating the effectiveness of the EM process.98Addressee relations and dialogue history are in Figure 1. U7:[Speaker #4:Erm, how doIrunrdesktop?]Human Response: Generated Responses: Our Method :[Speaker #4:Well, how doIinstall rdesktop from theterminal? ] Baseline Model :[Speaker #4:Itried butitdidn’t work .]Figure 5: The first example of Case Studies, which shows the generated responses of our model and the baseline model. 5.3 Case Studies To understand the effect of our method intuitively, we sample two cases from the testing set and present them in this section. Figure 5 illustrates an example whose addressee relations and dialogue history are shown in Figure 1. This conversation is about how to run the compiz orberyl in a comp with 256MB RAM. Speaker #2points that it’s the graphic card that is important, but Speaker #4 seems unsatisfied by saying that didn’t tell me much . After that, Speaker #5 suggests using the rdesktop andSpeaker #4 replies him/her. Our model is able to capture the key information rdesktop andterminal in the addressee utterance U6, and generate a proper response Well, how do I install rdesktop from the terminal , which is very close to the human answer and even better with more information from the terminal . On the contrary, the baseline model (BART) fails to capture the addressee information and just replies with a safe response I tried but it didn’t work . This case shows the great significance of modeling the addressee information, and also demonstrates the effectiveness of our model design. Figure 6 presents another example sampled from the testing set, where we investigate how different addressee labels affect the generated responses. In the figure, different colors represent different utterances in the Dialogue History part, and different responses generated by giving the corresponding utterances as addressees in the Generated Responses part. This conversation is about discussing the file system in Ubuntu that can share on a network with windows machines. When the addressee is given asU1, our model suggests using samba , which is a solution to the question of U1. Responses to U2andU3are like safe responses, but they make sense in their contexts: the former expresses its confusion about a confusing utterance ( U2), and the latter expresses its gratitude to the suggestion in U4 U1U5 U2 U3U6 U1:[Speaker #1:What isagood filesystem torunwith ubuntu and also tobeable toshare onanetwork with windows machines? Does itmatter very much? ] U2:[Speaker #2:Windows canFILEP ATH toFILEP ATH .] U3:[Speaker #3:Usesamba forthenetwork connection . Thelocal filesystem doesn't matter over anetwork. ] U4:[Speaker #4:Yeshe's doing itonanetwork, fsis irrelevant .] U5:[Speaker #5:Gonna have some funwith gentoo on myother pcIhave here.] U6:[Speaker #6:_______________________________ ]Addressee Relations: Dialogue History: Generated Responses: U6:[Speaker #6:Samba isaway tocooperate with the windows environments and canbeadministrated bythe webwith wat.] U6:[Speaker #6:I'mnotsure what you'retalking about bysaying FILEP ATH toFILEP ATH .] U6:[Speaker #6:Thanks ,I'llgive itashot.] U6:[Speaker #6:Icanuseanyfilesystem isthatright? ] U6:[Speaker #6:I'musing gentoo onmycomputer too.]Figure 6: The second example of Case Studies, which illustrates the generated response of our model given different addressee labels. Better view in color. U3. Response to U4states his/her understanding towards U4, and questions if his/her understanding is right. Response to U5acknowledges the solution gentoo inU5by saying using gentoo on my computer too . In general, this case demonstrates the ability of our model to generate diverse responses according to the specified addressees and contexts of the dialogue history. 5.4 Response Parser: A Byproduct for Free Another contribution of our EM pretraining is that a response parser can be freely obtained. This byproduct comes from Eq. (4), where given a response generation model with addressee modeling, we can predict the addressee for each utterance in the dialogue. Previous literature has studied and proved that explicitly modeling the structural information is beneficial to understanding specific structured data. (Li et al., 2020, 2022a,b). In this context, the response parser can be used to infer the discourse structures, which contributes to boosting the performance of some multiparty dialogue comprehension tasks like response selection and question answering. (Jia et al., 2020; Li and Zhao, 2021; Ma et al., 2022)996 Conclusion Most multiparty dialogue corpora are not annotated with addressee labels, making them unable to support the pretraining of response generation models. To solve this problem, we design a simple yet effective way to model the addressee of a response as a latent variable and propose an EM pretraining approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Mathematical derivation, experimental results on the Ubuntu IRC benchmark, and extensive analyses have justified the theoretical feasibility and actual effectiveness of our method. Limitations First, Due to the lack of datasets to evaluate the MPDRG task, we perform our experiments only on the Ubuntu IRC benchmark and pretrain our model only on the domain of Ubuntu chats. However, the potential of our approach goes far beyond that since it is applicable to any opendomain multiparty dialogue dataset. In the future work, we will consider applying our method in more opendomain conversational datasets, such as the transcripts of TV series or movies. Additionally, the pretraining process solely relies on the addressee information of individual turns, disregarding the replyto relations within the dialogue history. This oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multiparty dialogue. In our future work, we will explore the integration of discourselevel replyto relations into the pretraining process to further enrich the capabilities of the model. References Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2020. PLATO: Pretrained dialogue generation model with discrete latent variable. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 85–96, Online. Association for Computational Linguistics. Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. 2022. DialogVED: A pretrained latent variable encoderdecoder model for dialog response generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers) , pages 4852–4864, Dublin, Ireland. Association for Computational Linguistics. Kevin Clark, MinhThang Luong, Quoc V . Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020 . OpenReview.net. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. JiaChen Gu, ChaoHong Tan, Chongyang Tao, ZhenHua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang. 2022. HeterMPC: A heterogeneous graph neural network for response generation in multiparty conversations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5086–5097, Dublin, Ireland. Association for Computational Linguistics. Wenpeng Hu, Zhangming Chan, Bing Liu, Dongyan Zhao, Jinwen Ma, and Rui Yan. 2019. GSN: A graphstructured network for multiparty dialogues. InProceedings of the TwentyEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 1016, 2019 , pages 5010–5016. ijcai.org. Qi Jia, Yizhu Liu, Siyu Ren, Kenny Zhu, and Haifeng Tang. 2020. Multiturn response selection using dialogue dependency relations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1911–1920, Online. Association for Computational Linguistics. Ran Le, Wenpeng Hu, Mingyue Shang, Zhenjun You, Lidong Bing, Dongyan Zhao, and Rui Yan. 2019. Who is speaking to whom? learning to identify utterance addressee in multiparty conversations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 1909–1919, Hong Kong, China. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequenceto-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online. Association for Computational Linguistics.100Jiaqi Li, Ming Liu, MinYen Kan, Zihao Zheng, Zekun Wang, Wenqiang Lei, Ting Liu, and Bing Qin. 2020. Molweni: A challenge multiparty dialoguesbased machine reading comprehension dataset with discourse structure. In Proceedings of the 28th International Conference on Computational Linguistics , pages 2642–2652, Barcelona, Spain (Online). International Committee on Computational Linguistics. Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022a. Semanticpreserving adversarial code comprehension. In Proceedings of the 29th International Conference on Computational Linguistics , pages 3017– 3028, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Yiyang Li and Hai Zhao. 2021. Selfand pseudoself- supervised prediction of speaker and keyutterance for multiparty dialogue reading comprehension. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2053–2063, Punta Cana, Dominican Republic. Association for Computational Linguistics. Yiyang Li, Hai Zhao, and Zhuosheng Zhang. 2022b. Back to the future: Bidirectional information decoupling network for multiturn dialogue modeling. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 2761–2774, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue , pages 285–294, Prague, Czech Republic. Association for Computational Linguistics. Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2022. Structural characterization for dialogue disentanglement. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 285–297, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2851– 2864, Hong Kong, China. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pretraining. OpenAI Technical Report . Zhouxing Shi and Minlie Huang. 2019. A deep sequential model for discourse parsing on multiparty dialogues. In The ThirtyThird AAAI Conference onArtificial Intelligence, AAAI 2019, The ThirtyFirst Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7007–7014. AAAI Press. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA , pages 5998–6008. Weishi Wang, Steven C.H. Hoi, and Shafiq Joty. 2020. Response selection for multiparty conversations with dynamic topic tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6581–6591, Online. Association for Computational Linguistics. Rui Zhang, Honglak Lee, Lazaros Polymenakos, and Dragomir R. Radev. 2018. Addressee and response selection in multiparty conversations with speaker interaction rnns. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI18), New Orleans, Louisiana, USA, February 27, 2018 , pages 5690–5697. AAAI Press. Yizhe Zhang, Siqi Sun, Michel Galley, YenChun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Largescale generative pretraining for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pages 270–278, Online. Association for Computational Linguistics.101ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? The last Section. /squareA2. Did you discuss any potential risks of your work? Not applicable. Left blank. /square\u0013A3. Do the Analysis In order to get more insights into the proposed EM pretraining method, we dive deeper into it by conducting extensive analyses. 5.1 Ablation Study We conduct ablation studies to investigate the contribution of our different designs, whose results are tabulated in the lower part of Table 1. Firstly, let’s focus on the first line of the lower part. To study whether other utterances that are not in the reply chain of the current addressee can help to generate a better response, we extract the reply train by traversing from the current leave utterance (the response) up to the root node (the first utterance), then train a model by inputting this chain only. We see a large performance drop on all metrics in this setting, demonstrating the significance of the side information provided by the whole context. Second, let’s pay attention to the second and third lines of the lower part. In order to study the effect of the EM pretraining process, which is the key contribution of our work, we remove this process and pretrain a model using only the addressee labels obtained from the discourse parser (i.e. the initial training data used in the first iteration of our EM approach). A sharp performance drop is observed compared with PO and PF with our proposed EM pretraining strategy, demonstrating the significance of our design. Without the iterative EM procedure, the noisy addressee labels obtained from the discourse parser can cause error propaga1 2 3 4 5 6 7 8 9 Number of iterations1.001.051.101.151.201.251.30BLEU4 BLEU4 5560657075 Accuracy AccuracyFigure 4: Line chart of the BLEU4 score and addressee prediction accuracy with the increase of EM iterations. tion, which makes the model learn noisy features to predict a response, and hurts the performance. Finally, aiming at investigating whether the performance gains come from seeing more indomain data in the pretraining process, we use the same pretraining data to train another model with the denoising objectives proposed in BART (Lewis et al., 2020), then also finetune it on the Ubuntu IRC benchmark. The last line of the lower part presents the results, where we observe nearly the same performance compared with FO. This observation indicates that simply performing domain adaptation using the general pretraining objectives is insufficient to benefit the MPDRG task. 5.2 Response Generation vs. Addressee Prediction In Section 3.3, we prove that p(z|c, r)∝p(r|c, z). To verify the correctness of this equation and also to investigate the training process of our EM strategy, we draw the line chart of the BLEU4 score and addressee prediction accuracy of the top30% confidence samples on the validation set with the increasing of pretraining iterations. The addressees are predicted using Eq. (4), where we take the ziwith the highest conditional probability as the predicted addressee. Figure 4 illustrates the trending of the BLEU4 score and addressee prediction accuracy. On the one hand, we see that the trending of both metrics is consistent, which means with a more powerful response generation model comes a higher addressee prediction accuracy. This observation verifies the correctness of Eq. (3). On the other hand, with the increasing of iterations, both metrics grow mutually, then reach their tops at around the 6thiteration, demonstrating the effectiveness of the EM process.98Addressee relations and dialogue history are in Figure 1. U7:[Speaker #4:Erm, how doIrunrdesktop?]Human Response: Generated Responses: Our Method :[Speaker #4:Well, how doIinstall rdesktop from theterminal? ] Baseline Model :[Speaker #4:Itried butitdidn’t work .]Figure 5: The first example of Case Studies, which shows the generated responses of our model and the baseline model. 5.3 Case Studies To understand the effect of our method intuitively, we sample two cases from the testing set and present them in this section. Figure 5 illustrates an example whose addressee relations and dialogue history are shown in Figure 1. This conversation is about how to run the compiz orberyl in a comp with 256MB RAM. Speaker #2points that it’s the graphic card that is important, but Speaker #4 seems unsatisfied by saying that didn’t tell me much . After that, Speaker #5 suggests using the rdesktop andSpeaker #4 replies him/her. Our model is able to capture the key information rdesktop andterminal in the addressee utterance U6, and generate a proper response Well, how do I install rdesktop from the terminal , which is very close to the human answer and even better with more information from the terminal . On the contrary, the baseline model (BART) fails to capture the addressee information and just replies with a safe response I tried but it didn’t work . This case shows the great significance of modeling the addressee information, and also demonstrates the effectiveness of our model design. Figure 6 presents another example sampled from the testing set, where we investigate how different addressee labels affect the generated responses. In the figure, different colors represent different utterances in the Dialogue History part, and different responses generated by giving the corresponding utterances as addressees in the Generated Responses part. This conversation is about discussing the file system in Ubuntu that can share on a network with windows machines. When the addressee is given asU1, our model suggests using samba , which is a solution to the question of U1. Responses to U2andU3are like safe responses, but they make sense in their contexts: the former expresses its confusion about a confusing utterance ( U2), and the latter expresses its gratitude to the suggestion in U4 U1U5 U2 U3U6 U1:[Speaker #1:What isagood filesystem torunwith ubuntu and also tobeable toshare onanetwork with windows machines? Does itmatter very much? ] U2:[Speaker #2:Windows canFILEP ATH toFILEP ATH .] U3:[Speaker #3:Usesamba forthenetwork connection . Thelocal filesystem doesn't matter over anetwork. ] U4:[Speaker #4:Yeshe's doing itonanetwork, fsis irrelevant .] U5:[Speaker #5:Gonna have some funwith gentoo on myother pcIhave here.] U6:[Speaker #6:_______________________________ ]Addressee Relations: Dialogue History: Generated Responses: U6:[Speaker #6:Samba isaway tocooperate with the windows environments and canbeadministrated bythe webwith wat.] U6:[Speaker #6:I'mnotsure what you'retalking about bysaying FILEP ATH toFILEP ATH .] U6:[Speaker #6:Thanks ,I'llgive itashot.] U6:[Speaker #6:Icanuseanyfilesystem isthatright? ] U6:[Speaker #6:I'musing gentoo onmycomputer too.]Figure 6: The second example of Case Studies, which illustrates the generated response of our model given different addressee labels. Better view in color. U3. Response to U4states his/her understanding towards U4, and questions if his/her understanding is right. Response to U5acknowledges the solution gentoo inU5by saying using gentoo on my computer too . In general, this case demonstrates the ability of our model to generate diverse responses according to the specified addressees and contexts of the dialogue history. 5.4 Response Parser: A Byproduct for Free Another contribution of our EM pretraining is that a response parser can be freely obtained. This byproduct comes from Eq. (4), where given a response generation model with addressee modeling, we can predict the addressee for each utterance in the dialogue. Previous literature has studied and proved that explicitly modeling the structural information is beneficial to understanding specific structured data. (Li et al., 2020, 2022a,b). In this context, the response parser can be used to infer the discourse structures, which contributes to boosting the performance of some multiparty dialogue comprehension tasks like response selection and question answering. (Jia et al., 2020; Li and Zhao, 2021; Ma et al., 2022)996 Conclusion Most multiparty dialogue corpora are not annotated with addressee labels, making them unable to support the pretraining of response generation models. To solve this problem, we design a simple yet effective way to model the addressee of a response as a latent variable and propose an EM pretraining approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Mathematical derivation, experimental results on the Ubuntu IRC benchmark, and extensive analyses have justified the theoretical feasibility and actual effectiveness of our method. Limitations First, Due to the lack of datasets to evaluate the MPDRG task, we perform our experiments only on the Ubuntu IRC benchmark and pretrain our model only on the domain of Ubuntu chats. However, the potential of our approach goes far beyond that since it is applicable to any opendomain multiparty dialogue dataset. In the future work, we will consider applying our method in more opendomain conversational datasets, such as the transcripts of TV series or movies. Additionally, the pretraining process solely relies on the addressee information of individual turns, disregarding the replyto relations within the dialogue history. This oversight prevents the model from benefiting from valuable contextual cues necessary for a comprehensive understanding of the multiparty dialogue. In our future work, we will explore the integration of discourselevel replyto relations into the pretraining process to further enrich the capabilities of the model. References Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. 2020. PLATO: Pretrained dialogue generation model with discrete latent variable. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 85–96, Online. Association for Computational Linguistics. Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. 2022. DialogVED: A pretrained latent variable encoderdecoder model for dialog response generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers) , pages 4852–4864, Dublin, Ireland. Association for Computational Linguistics. Kevin Clark, MinhThang Luong, Quoc V . Le, and Christopher D. Manning. 2020. ELECTRA: pretraining text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 2630, 2020 . OpenReview.net. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. JiaChen Gu, ChaoHong Tan, Chongyang Tao, ZhenHua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang. 2022. HeterMPC: A heterogeneous graph neural network for response generation in multiparty conversations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 5086–5097, Dublin, Ireland. Association for Computational Linguistics. Wenpeng Hu, Zhangming Chan, Bing Liu, Dongyan Zhao, Jinwen Ma, and Rui Yan. 2019. GSN: A graphstructured network for multiparty dialogues. InProceedings of the TwentyEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 1016, 2019 , pages 5010–5016. ijcai.org. Qi Jia, Yizhu Liu, Siyu Ren, Kenny Zhu, and Haifeng Tang. 2020. Multiturn response selection using dialogue dependency relations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1911–1920, Online. Association for Computational Linguistics. Ran Le, Wenpeng Hu, Mingyue Shang, Zhenjun You, Lidong Bing, Dongyan Zhao, and Rui Yan. 2019. Who is speaking to whom? learning to identify utterance addressee in multiparty conversations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 1909–1919, Hong Kong, China. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequenceto-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7871–7880, Online. Association for Computational Linguistics.100Jiaqi Li, Ming Liu, MinYen Kan, Zihao Zheng, Zekun Wang, Wenqiang Lei, Ting Liu, and Bing Qin. 2020. Molweni: A challenge multiparty dialoguesbased machine reading comprehension dataset with discourse structure. In Proceedings of the 28th International Conference on Computational Linguistics , pages 2642–2652, Barcelona, Spain (Online). International Committee on Computational Linguistics. Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022a. Semanticpreserving adversarial code comprehension. In Proceedings of the 29th International Conference on Computational Linguistics , pages 3017– 3028, Gyeongju, Republic of Korea. International Committee on Computational Linguistics. Yiyang Li and Hai Zhao. 2021. Selfand pseudoself- supervised prediction of speaker and keyutterance for multiparty dialogue reading comprehension. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 2053–2063, Punta Cana, Dominican Republic. Association for Computational Linguistics. Yiyang Li, Hai Zhao, and Zhuosheng Zhang. 2022b. Back to the future: Bidirectional information decoupling network for multiturn dialogue modeling. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 2761–2774, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multiturn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue , pages 285–294, Prague, Czech Republic. Association for Computational Linguistics. Xinbei Ma, Zhuosheng Zhang, and Hai Zhao. 2022. Structural characterization for dialogue disentanglement. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 285–297, Dublin, Ireland. Association for Computational Linguistics. Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard EM approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2851– 2864, Hong Kong, China. Association for Computational Linguistics. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pretraining. OpenAI Technical Report . Zhouxing Shi and Minlie Huang. 2019. A deep sequential model for discourse parsing on multiparty dialogues. In The ThirtyThird AAAI Conference onArtificial Intelligence, AAAI 2019, The ThirtyFirst Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pages 7007–7014. AAAI Press. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA , pages 5998–6008. Weishi Wang, Steven C.H. Hoi, and Shafiq Joty. 2020. Response selection for multiparty conversations with dynamic topic tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6581–6591, Online. Association for Computational Linguistics. Rui Zhang, Honglak Lee, Lazaros Polymenakos, and Dragomir R. Radev. 2018. Addressee and response selection in multiparty conversations with speaker interaction rnns. In Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI18), New Orleans, Louisiana, USA, February 27, 2018 , pages 5690–5697. AAAI Press. Yizhe Zhang, Siqi Sun, Michel Galley, YenChun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Largescale generative pretraining for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pages 270–278, Online. Association for Computational Linguistics.101ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? The last Section. /squareA2. Did you discuss any potential risks of your work? Not applicable. Left blank. /square\u0013A3. Do the experiments reveal that the performance of the current SOTA NER method (Zhou and Chen, 2021) (previously evaluated only on the CoNLL 2003 dataset) drops by 23% when evaluated on MultiCoNER and 31.8% when evaluated on a lowresource setting with just 500 training samples (more details in Table 8). Thus, we emphasize that research on building systems that can effectively detect complex NEs in the text is currently understudied in the field of NLP. In the past, researchers have made several attempts at building supervised approaches to detect complex and compositional noun phrase entities in sentences (Doddington et al., 2004; Biggio et al., 2010; Magnolini et al., 2019). However, the scarcity of annotated training data for building effective systems has always been a challenge. Data augmentation has been shown to be an effective solution for lowresource NER (Ding et al., 2020; Liu et al., 2021; Zhou et al., 2022). In practice, though these systems perform well and generate104coherent augmentations on common NER benchmark datasets with easy proper noun NEs, they fail to be effective for complex NER, often generating incoherent augmentations. We first argue that certain types of complex NEs follow specific linguistic patterns and appear only in specific contexts (examples in Appendix 4), and augmentations that do not follow these patterns impede a NER model from learning such patterns effectively. This sometimes also leads to augmentations with contextentity mismatch, further hurting the learning process. For e.g., unlike proper names, substituting complex NEs from other sentences in the corpus or replacing them with synonyms (Dai and Adel, 2020a) often leads to augmentations where the NE does not fit into the new context (e.g., swapping proper names across sentences might still keep the sentence coherent butswapping the name of a book with a movie (both creative work entity) orthe name of a football team with a political party (both group entity) makes it incoherent). Finetuning pretrained language models (PLMs), similar to priorwork (Ding et al., 2020; Liu et al., 2021; Zhou et al., 2022), fail to generate new context around complex NEs or completely new NEs with the desired linguistic patterns due to lowcontext sentences and the lack of existing knowledge of such linguistically complex NEs (examples in Fig. 3). This leads to incoherent augmentations and poses a severe problem in knowledgeintensive tasks like biomedical NER, where nonfactual augmentations severely hurt learning. Our experiments also reveal that introducing new context patterns around NEs proves to be a more effective data augmentation technique for complex NER than diversifying NEs (ACLM vs. MELM in Table 1). Main Results: To overcome the aforesaid problems, we formulate data augmentation as a conditional generation task and propose ACLM, a conditional text generation model that generates augmentation samples by introducing new and diverse context patterns around a NE. ACLM builds on BART (Lewis et al., 2020) and is finetuned on a modification of the text reconstruction from corrupted text task, a common denoisingbased PLM pretraining objective. In contrast to other PLM pretraining strategies, which randomly mask a portion of the text for corruption, our modified objective is based on selective masking , wherein we mask all other words in the sentence except the NEs and a small percentage of keywords related to the NEs.We refer to this corrupted sentence as a template , and it serves as input to the model for both the training and generation phases. These keywords are other nonNE tokens in the sentence that provide contextually relevant additional knowledge or hints to BART about the complex NEs without the need of retrieving knowledge from any external sources. We select these keywords using attention maps obtained from a transformer model finetuned on the NER task, and they help the PLM overcome the problem where it might not possess enough knowledge about a semantically ambiguous complex NE (example in Fig. 3). Training ACLM on this modified objective allows us to generate diverse, coherent, factual, and highquality augmentations given templates. We also propose mixner , a novel algorithm that mixes two templates during the augmentation generation phase and boosts the diversity of augmentations. Our primary contributions are as follows: •We propose ACLM, a novel data augmentation framework specially designed for lowresource complex NER. Compared with previous methods in the literature, ACLM effectively alleviates the contextentity mismatch problem by preserving the true sense of semantically ambiguous NEs in augmentations. Additionally, to accompany ACLM, we propose mixner , which boosts the diversity of ACLM generations. •We qualitatively and quantitively show the benefits of ACLM for monolingual, crosslingual, and multilingual complex NER across various lowresource settings on the MultiCoNER dataset. Our proposed ACLM outperforms all other baselines in literature by a significant margin (1%-36%) and generates more diverse, coherent, and highquality augmentations compared to them. •We perform extensive experiments to study the application of ACLM in three other domains, including science and medicine. ACLM outperforms all our baselines in these domains (absolute gains in the range of 1%- 11%) and generates more factual augmentations. 2 Experimental Setup ACLM. We use mBart50-large (Tang et al., 2020) with a condition generation head to finetune ACLM. We finetune ACLM for 10 epochs using Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e−5and a batch size of 32. NER. We use XLMRoBERTa-large with a linear head as our NER model. Though the field of NER has grown enormously, in this paper, we adhere to the simplest formulation and treat the task as a tokenlevel classification task with a BIO tagging scheme. We use the Adam optimizer to optimize our model, set the learning rate to 1e−2, and train with a batch size of 16. The NER model is trained for 100 epochs, and the model with the best performance on the dev set is used for testing. Hyperparameter Tuning. For template creation during finetuning and generation, we set the selection rate pand the Gaussian µto be 0.3 and 0.5, respectively. The number of augmentation rounds Ris set as 5. For mixner we set Gaussian µandβto be 0.5 and 0.7, respectively. All hyperparameters are tuned on the development set with grid search. More details can be found in Appendix A. 4.3 Baselines To prove the effectiveness of our proposed ACLM, we compare it with several strong NER augmentation baselines in the literature. In this subsection, we briefly describe each of these baselines. All baselines were run for Rrounds. GoldOnly. The NER model is trained using only gold data from the MultiCoNER dataset without any augmentation.109MONOLINGUAL CROSS -LINGUAL #Gold evaluation of generation quality from various systems on the measures of perplexity and diversity. DiversityE, N, and L stand for Entity, NonEntity, and Length, respectively. calculate the average absolute difference between the number of tokens in generated samples and the original samples. ACLM achieves the lowest perplexity and highest nonNE and length diversity compared with other baselines. NE diversity in ACLM is achieved with mixner where ACLM fairs well compared to MELM which just replaces NEs. LwTR achieves the highest perplexity, thereby reaffirming that it generates incoherent augmentations. Qualtitative Analysis. Fig. 3 illustrates the superiority of augmentations generated by ACLM when compared with our other baselines. As clearly evident, while MELM generates just minor changes in NEs, augmentations produced by LwTR often tend to be nonsensical and incoherent. On the other hand, ACLM generates meaningful and diverse sentences around NEs, which is further boosted with mixner . We provide examples in Appendix D.1. 5.2 Application to other domains To evaluate the transferability of ACLM to other domains, we evaluate ACLM on 4 more datasets beyond MultiCoNER. These datasets include CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) (news), BC2GM (Smith et al., 2008) (biomedical), NCBI Disease (Do ˘gan et al., 2014) (biomedical) and TDMSci (Hou et al., 2021) (science). Table 4 compares our baselines with ACLM across 2 lowresource settings on all 4 datasets. ACLM outperforms all our baselines on all settings except LwTR on CoNLL 2003. This occurs because LwTR generates a large variety of effective augmentations with NE replacement on easy entities in CoNLL 2003. The results demonstrate the effectiveness of ACLM over diverse domains, including domains with an acute scarcity of data (biomedical). Additionally, we also emphasize that ACLM produces more factual augmentations and, unlike our other baselines, avoids contextentity mismatch, which makes the NER model store wrong knowledge in111Originalit was developed by a team led by former [blizzard entertainment] CORP employees, some of whom had overseen the creation of the [diablo] CW series. The original sentence describes the employees of an organization and provides details about them. LwTRit was developed by a makers led by, [blizzard entertainment] CORP., some of whom had elevation the serving of the [diablo] CW 12th.❌ LwTR replaces random words in the sentence, which makes it incoherent. MELMit was developed by a team led by former [blizzago games] CORP employees, some of whom had overseen the creation of the [hablo] CW series.❌ MELM keeps the sentence coherent but generates new NEs that do not correspond to realworld entities. ACLM w/o mixner[blizzard entertainment] CORP employees have overseen the production of the animated films, including the production of the [diablo] CW series. ACLM generates new context patterns around the NE, keeping the sentence coherent and avoiding contextentity mismatch. ACLM w/ mixnerthe team of the [blizzard entertainment] CORP had overseen the creation of the game [diablo] CW and many of its workers founded [pyro studios] CORP in the early 1960s. mixner boosts ACLM diversity and still keeps the sentence coherent. It adds a NE in the sentence and augments the sentence with extra details about the employees of the organization. OriginalThe control group consisted of 40 consecutive [FMF] DISEASE patients , who arrived at the [FMF] DISEASE clinic for their regular followup visit and were 40 years of age or older at the time of the examination . The original sentence describes an occasion where a group of 40 patients diagnosed with a certain kind of disease visited a clinic, and the sentence provides us with information on the age statistics of the patients. LwTRThe control, consisted of 40 consecutive [fragile] DISEASE patients, who arrived at the [FMF] DISEASE status for their regular follow - up and were 40 years of age or older at the time of the examination analyzed ❌ LwTR replaces \"FMF\" in the sentence with \"fragile\" and the phrase \"fragile patients\" does not make sense. It also adds an extra word, \"analyzed\", at the end of the sentence. MELMThe control group consisted of 40 consecutive [FMR] DISEASE patients, who arrived at the [PDA] DISEASE clinic for their regular followup visit and were 40 years of age or older at the time of the examination.❌ MELM replaces the 1st occurrence of \"FMF\" in the sentence with \"FMR\" and the second occurrence with \"PDA\". \"FMR\" is not the name of a disease and is closest to \"FMR1\", which is the name of a gene. \"PDA\" stands for \"Patent ductus arteriosus.\" Thus, the entire sentence does not make much sense. ACLM w/o mixnerThe sample consisted of four consecutive [FMF] DISEASE patients who arrived at the [FMF] DISEASE clinic for a visit of examination. Only one of the 4 remaining patients had [FMF] DISEASE.  ACLM introduces a new context pattern around the sentence. The entire sentence is coherent. ACLM w/ mixnerOf 4000 (40%) patients with onset [FMF] DISEASE, patients with [FRDA] DISEASE had no tendon reflexes at all.  mixner boosts ACLM diversity and still keeps the sentence coherent. \"FRDA\" (Friedreich's ataxia) is a genetic disease that causes difficulty in walking and a loss of sensation in the arms and legs.Figure 3: Examples of augmentations generated with different methods (Left) and explanation (Right). Words in red are Named Entities, and words underlined in the Original sentence are identified ACLM keywords . ACLM generates much more diverse, detailed, and coherent augmentations, which maintain factuality and also prove to be more effective. Generation diversity is further amplified with mixner . #Gold Analysis 5.1 Generation Quality Quantitative Analysis. Table 3 compares augmentations from various systems on the quantitative measures of perplexity and diversity. Perplexity (Jelinek et al., 1977) is a common measure of text fluency, and we measure it using GPT2 (Radford et al., 2019). We calculate 3 types of diversity metrics: for DiversityE and DiversityN, we calculate the average percentage of new NE and nonNE words in the generated samples compared with the original samples, respectively. For DiversityL, we#Gold Experiments Models We evaluate PACHINC Oand stateof- theart public code LLMs, namely CODE GEN(Nijkamp et al., 2022) and INCODER (Fried et al., 2022). We test both the mono lingual (Pythononly) and the multi lingual version of CODEGEN.INCODER may be a more appealing comparison since it is trained on 5GB of Jupyter notebooks. Inference and Metrics We convert each problem into a prompt (§5.1) and draw samples using nucleus sampling. Following Chen et al. (2021a), we report pass@kmetric, deﬁned as the fraction130[1][2]# Schema of Dataframes:# Columns in df with example values:# Stu_Name (Mike),Engineering (90),English (89),Math (92)[3] Get the students with an average score above 90 for science subjects[3a]df['Science_Avg'] = (df['Engineering']+df['Math'])/2df[df['Science_Avg'] > 90][['Stu_Name','Science_Avg']]▸ Vanilla Prediction (no exemplars):[3b]▸ Stepby-Step Prompting (with exemplars):df['Science_Avg'] = (df['Engineering'] + df['Math']) / 2df_score_above_90 = df[df['Science_Avg'] > 90]result = df_score_above_90[['Stu_Name', 'Science_Avg']]# Solution: Let's solve this problem stepby-step.preamble# Step 1: Create a new column with the average score of# engineering and mathexplanation# Step 2: Get the rows whose average score is above 90# Step 3: Return the student name and average scoresimport pandas as pddf = pd.read_csv('stores.csv')[1]Which countries host at least two Olympic games? count_df = df['Country'].value_counts()filtered_df = count_df[count_df >= 2]filtered_df.index.tolist()# Solution: Let's solve this problem stepby-step.# Step 1: Get the counts each country hosted Olympics# Step 2: Get the rows whose average score is above 90# Step 3: Get the country names as a listPrompt Prefix (Exemplars)preambleexplanationFigure 3: An example problem. Cells 12 ( c1,c2) are the notebook context, and Cell 3 ( c3) contains the intent. Cells 3a and 3b show two example completions of c3. of problems with at least one correct sample given a sample size k. To reduce variance, we estimate pass@k(k≤30) by drawing 50samples for each problem (Chen et al., 2021a). Decoding temperaturetis0.2fork= 1and0.8fork >1. Refer to Appendix E for inference details. 5.1 LM Prompting Strategies We explore two prompting strategies: prompting using the notebook context of a problem (§5.2), and fewshot prompting with extra exemplars as a prompt preﬁx before the notebook context (§5.3) to impose more control on the predicted code’s style. Prompting with Notebook Contexts Fig. 3 depicts an example problem at c3for prompting, where the prompt is the notebook context (preceding cells c1andc2) and the current intent. The context also includes NL descriptions of the imported DataFrame schema ( c2), such as its columns and example cell values, crucial for grounded understanding of structured knowledge (Xie et al., 2022). Completion 3a shows an example prediction. For the following problems after c3(not shown), we use annotated reference solutions to previous turns in their contexts, reminiscent of multiturn taskoriented dialogue evaluation (Andreas et al., 2020). Using Extra Fewshot Exemplars Besides the basic setting, we also explore prompting using fourpass@kExisting Tasks New Tasks 1 5 30 1 5 30 Existing Models INCODER 1B 20 .8 30.9 47.0 2.3 4.0 9.9 INCODER 6B 28.2 40.6 56.2 3.5 7.1 15.8 CODEGENmulti 350M 9 .0 13 .6 21.3 0.8 0.9 2.6 CODEGENmulti 2B 18 .7 25.9 39.3 1.5 2.6 6.8 CODEGENmulti 6B 20 .0 28.5 42.8 1.7 3.4 8.9 CODEGENmulti 16B 20 .9 31.4 47.1 2.5 4.8 12 .4 CODEGENmono350M 11 .3 18.5 32.8 1.5 1.9 5.1 CODEGENmono2B 24 .7 35.5 52.9 3.1 6.3 16 .0 CODEGENmono6B 28 .7 42.2 60.9 4.0 8.6 20 .4 CODEGENmono16B 32 .6 46.2 63.9 6.1 12 .1 25.2 CODEcushman- 001 38 .1 50.4 68.8 8.9 14 .5 31.0 CODEdavinci- 002 53 .0 66.3 81.5 23 .4 36.0 54.7 Our Models Base P ALM62B 35 .7 49.4 67.8 7.2 12 .7 26.4 +Python f.t. 43.6 58.8 75.3 11 .9 21.7 40.7 +PACHINCO 48.9 64.3 78.3 18 .0 30.5 47.7 −Schema Desc. 44.2 60.0 75.0 13 .0 22.2 36.1 Table 2: pass@kusing notebook context as prompts. additional NLto-code exemplars as prompt preﬁx before the notebook context. As shown in Fig. 3 (Completion 3b), we focus on prompting LMs to generate code that follows a multiline, stepby- step ( SbS) decomposition structure, in contrast with the common practice of chaining multiple API calls in a single line (Completion 3a). Each step is also optionally inlined with NL explanations . Such stepwise explanations could help novice developers understand model predictions, and they have been found effective for reasoning (Wei et al., 2022; Gao et al., 2022) and program induction (Nye et al., 2021) tasks. Following Kojima et al. (2022), we also use a preamble to further elicit stepwise decomposition in predictions. See Appendix L for a complete list of example prompts. 5.2 Main Results Tab. 2 reports pass@konARCADE using notebook contexts as prompts. PACHINC Oachieves strong performance on both the Existing Tasks split and the New Tasks split due to its larger size and domainspeciﬁc ﬁnetuning. Impact of Finetuning The base PALMmodel outperforms most public code LMs and is on par with CODEGENmono 16B. Finetuning on Python ( +Python f.t., Tab. 2) and notebooks data (+PACHINCO) further closes the domain gap with improved pass@k. The absolute gain after ﬁnetuning on Python code is higher than continued training on notebooks, likely because the semantic gap between NL data and Python code is larger than131Dataset HUMAN EVAL MBPP TRANSCODER Metric pass@100 pass@80 pass@25 PALMC ODER 540B†88.4 80 .8 82 .5 CODEdavinci-002 92.1α84.5α87.9 PaLM 62B (Python f.t.§4) 91.5 86 .0 86 .4 Table 3: Evaluation of existing code LMs and PaLM 62B after the ﬁrststage ﬁnetuning on Python code.†Results from Chowdhery et al. (2022).αResults from Chen et al. (2022) . that between general Python code and notebooks. We note that the base PALM62Bmodel after ﬁnetuning on Python code corpora is already a strong code LM, performing competitively compared to other strong code LMs on established code generation ( HUMAN EVAL andMBPP ) and translation ( TRANSCODER ) tasks (Tab. 3). With 7×more Python code tokens, our Python ﬁnetuned PALM62Bmodel outperforms the 8×larger PALMC ODER 540B model on all the three tasks. Comparing Existing Code LMs Among models with similar size and amount of Python training data ( INCODER 6Bvs.CODEGENmulti 6B), INCODER 6Bperforms better, likely because INCODER was trained on Jupyter notebooks.7With 4×more Python data, CODEGENmono6Btakes over. Appendix F further reports the scaling curve ofCODEGENonARCADE , where pass@kscales as a power law with model size. For reference, we also report the results using the C ODEX API. P ACHINCOsigniﬁcantly outperforms the smaller cushman API, while davinci002 is stronger. While we cannot gain much insight from the results due to limited knowledge about davinci002, through error  evaluation accuracies due to potential leakage of evaluation notebooks in the training data of LLMs, which is a common issue in LLM evaluation (Brown et al., 2020).3To prevent contamination, we additionally build the New Tasks split with 660 problems in notebooks created from scratch. Speciﬁcally, we create notebooks with wrangling and EDA tasks for 70tabular ML datasets that appeared on Kaggle since February 2022 and are manually veriﬁed to differ from existing datasets on the Web. For each Kaggle dataset, we instructed the annotators to create a notebook with tasks that would provide insights for building an ML model for the dataset. To make the problems more challenging, we also encouraged them to make tasks that require at least 5pandas API calls to solve. Annotating NL Intents When creating NL intents for a problem,4annotators are instructed to phrase their intents in the way they prefer when interacting with an AI system to help them implement the existing code solution, while keeping the intents natural and concise, without redundant elaboration such as lineby-line explanation. In addition, to 3JuICe and BigQuery primarily contain source ﬁles from 2019 or earlier, which exacerbates this issue. 4ForNew Tasks , intents are created before the solutions.make the intents more challenging, we encourage annotators to refer to entities and variables in the intents using semantic rewrites without introducing ambiguity ( e.g., use “ convert all binary columns to bool” instead of listing columns verbatim), reminiscent of synonym substitution for labeling utterances in textto-SQL (Gan et al., 2021). Mitigating Ambiguity in NL Intents Creating succinct NL intents without ambiguity could be nontrivial in this opendomain code generation setting, especially when there could be multiple plausible interpretations of an intent. For example, without the underlined part ofu5(Fig. 1), a programmer or a system may propose alternative solutions using different table schema. Therefore, for such openended problems where there could be multiple alternative ways to present the answer, we ask annotators to provide extra speciﬁcation in their intents about the desired output ( e.g., schema of the output DataFrame , such as the underlined part inu5). Even with these additional semantic constraints, empirically we observe that about 50% of intents are still underspeciﬁed, making ARCADE a challenging benchmark for handling realistic NL intents with uncertainty. We present more analysis in §3.2 and introduce a robust evaluation metric that mitigates this issue in §3.3. Annotation Guideline Besides mitigating ambiguity in intents, there are many other aspects to consider during annotation, such as notebook style (e.g., removing analysis framework in data science notebooks. A RCADE features multiple rounds of NLto-code problems from the same notebook. It requires a model to understand rich multimodal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop P ACHINCO, a 62B code language model (LM) for Python computational notebooks, which signiﬁcantly outperforms public code LMs. Finally, we explore fewshot prompting strategies to elicit better code with stepby-step decomposition and NL explanations, showing the potential to improve the diversity and explainability of model predictions. A RCADE is publicly available at https://github.com/ googleresearch/arcadenl2code/ . 1 Introduction Data science is the process of extracting insights from data (Wang et al., 2021a), and has become an integral part of decision making and knowledge discovery (Donoho, 2017). Data scientists and machine learning (ML) practitioners often use computational notebooks , which are interactive environments such as Jupyter notebooks (Kluyver et al., 2016) and Google Colab, in their work. Data scientists spend a signiﬁcant amount of time on data wrangling tasks to process raw data into usable forms (illustrated in Fig. 1), as well as exploratory data analysis (EDA) to gain insights ∗Correspondence to pcyin@google.com [1]import pandas as pddf = pd.read_csv('dataset/Gamepass_Games_v1.csv') [2]def get_avg(x): try: return float(x[0]) , float(x[1]) except: return 0, 0df['min'], df['max'] = zip(*df['TIME'].str.replace( ' hours','').str.split(\"-\").apply(get_avg))Extract min and max hours as two columns [3]df['ADDED'] = pd.to_datetime( df['ADDED'],format=\"%d %b %y\",errors='coerce') [4]df['GAMERS']=df['GAMERS'].str.replace( ',',' ').astype(int)added_year=df[df['GAMERS'].idxmax()]['ADDED'].year In which year was the most played game added? [5]df[(df['ADDED'].dt.year== added_date.year) & (df['RATING']>4)].groupby( df[\"ADDED\"].dt.month)['GAME'].count() For each month in that year, how many games that has a rating of more than four? [6]fallout=df[df['GAME'].str.contains('Fallout')]fallout.groupby(fallout['ADDED'].dt.year).get_group( 2021)['max'].mean()What is the average maximum completion time for all fallout games added in 2021? [7]pd.pivot_table(df, index=df['ADDED'].dt.year, ..., aggfunc=np.count_nonzero, fill_value='0').rename_axis( index='Year', columns='Month')What is the amount of games added in each year for each month? (show a table with index as years, columns as months and fill null values with 0) Figure 1: An example of a computational notebook adapted from our dataset, with examples of reading data (cell c1), data wrangling ( c2, c3), and exploratory data analysis ( c4∼c7). Annotated NL intents ( ui) are shown in green. for decision making (Agashe et al., 2019; Wang et al., 2022a). This has motivated research on automating and accelerating the data science workﬂow in general (Aggarwal et al., 2019; Wang et al., 2021a,b), with particular interest in data wrangling and EDA tasks (Bavishi et al., 2019; Jain et al., 2021; Nazabal et al., 2020; Kandel et al., 2011). Meanwhile, large language models (LLMs) trained on code can assist developers by translating natural language (NL) intents into executable programs (Chen et al., 2021a; Austin et al., 2021; Chowdhery et al., 2022; Nijkamp et al., 2022;126Fried et al., 2022), with promising applications in synthesizing code for data wrangling and EDA tasks (Jain et al., 2021; Rajkumar et al., 2022; Cheng et al., 2022b). Computational notebooks also present unique challenges to LLMs, as notebooks freely mix NL, code, graphics, and execution results (Perkel, 2021), and because of their interactivity, notebooks feature multiple interdependent NLto-code problems (Heyman et al., 2021). Several benchmarks have been proposed to evaluate program synthesis of data science programs from NL intents, but these datasets have several limitations. First, some datasets derive from data science tutorial notebooks (Agashe et al., 2019; Chandel et al., 2022), which tend to contain NL text ( e.g., exercise questions) that is verbose and elaborate, instead of the concise, ephemeral style that developers write when interacting with code LMs (Barke et al., 2022, more in §3). Other datasets assume that the developer provides extra information, such as unit tests or input/output examples (Chandel et al., 2022; Jain et al., 2022), but such systems pose an extra burden to users who might not normally write such tests or examples during their workﬂow (Pimentel et al., 2019). Finally, existing datasets usually contain independent tasks with isolated contexts (Lai et al., 2022), or a limited number of contextually dependent problems (Huang et al., 2022), rather than having multiple, related tasks such as in Fig. 1. Therefore, there is a need for a benchmark with realistic NL intents, rich notebook context, and a series of interrelated problems , so as to better reﬂect realworld usage by data scientists. To ﬁll this gap, we present ARCADE ,1a new benchmark for code generation for data wrangling and EDA tasks in computational notebooks (§3). ARCADE consists of 1,078 problems spanning across 136 notebooks based on 106ML datasets. It features a series of NL utterances written by professional data scientists with the intention of interacting with an AI assistant ( e.g., green texts in Fig. 1), with highquality code solutions using the pandas library. To mitigate the risk of data leakage, 60% of the problems are created from scratch, based on recent ML datasets on Kaggle ( e.g., the csv ﬁle in c1, Fig. 1).2ARCADE also challenges LLMs with grounded language understanding, where a model needs to leverage variable states ( e.g.,df['TIME '] inc2) to interpret NL semantics ( e.g., “min and 1Answer Repository for Computational Analysis and Data Engineering. 2https://www.kaggle.com/max” inu1). Finally, problems in ARCADE are challenging, involving richer data science API usage than existing benchmarks. To demonstrate how ARCADE can motivate new research on LLMs for data science, we develop PACHINC O, a62B code LM tailored for Python computational notebooks, trained on a mixture of NL, source code, and Jupyter notebooks data (§4). PACHINCOsigniﬁcantly outperforms public code LMs on ARCADE (§5.2). Even so, all models have difﬁculty on our benchmark, showing that it is a challenging task. Further, we explore fewshot prompting strategies to alter the style of model predictions, such as decomposing code into stepby- step structures and adding inline NL explanations. Not only is code in this style potentially more understandable to novice data scientists, prompting the model to explain its solutions also improves the diversity of the model’s predictions (§5.3). 2 Problem Statement A computational notebook is an interactive computing environment that allows mixing code, text, and graphics. A notebook consists of a sequence of Markdown or source code cells. Given a partial notebook context with ncells{ci}n i=1and a userspeciﬁed intent ufor the next cell cn+1(e.g.,u1 in Fig. 1 for n= 1), we aim to generate code for cn+1that fulﬁlls the user’s intent (Agashe et al., 2019). We refer to the pair ({ci},u)as aproblem . This process could proceed sequentially with multiple rounds between the user and a system (Heyman et al., 2021), so a single notebook can contain multiple problems. To satisfy subsequent intents (e.g.,u4), a system will leverage the updated notebook context ( e.g.,{ci}5 i=1) which includes previous problems ( e.g., those involving u1tou3). As in Fig. 1, problems within a notebook often have interesting dependency structures. They may share execution context ( e.g.,DataFrame df ), form semantically coherent turns ( e.g.,c4andc5), or exhibit nontrivial long range data dependencies (e.g., from c6toc2, orc7toc3). These dependency structures are more diverse than existing multiturn code generation tasks with sequentially dependent problems (Nijkamp et al., 2022). 3 A RCADE : A Benchmark of pandas Data Science Code Generation 3.1 Constructing A RCADE ARCADE consists of 1,078NLto-code problems from 131 notebooks based on 106 unique ML127datasets, sourced from existing data science notebooks on GitHub ( Existing Tasks split) and new ones created from scratch ( New Tasks split). The problems are annotated by professional data science freelancers. This section outlines the dataset creation process. See Appendix A for more details. Repurposing Existing Notebooks To build the Existing Tasks split, we identify candidate code cells performing data wrangling and EDA tasks from existing highquality notebooks, and then manually annotate these cells with NL intents. Speciﬁcally, we perform static analysis to identify notebooks with rich code cells related to data wrangling and EDA tasks ( e.g., by identifying cells using pandas functions) from public notebook corpora such as JuICe (Agashe et al., 2019) and BIGQUERY . We then select 63 notebooks with the greatest number of candidate code cells for annotation, covering 36 ML datasets from a variety of domains. Annotation consists of judging the quality of candidate cells, ﬁxing errors, and creating intents summarizing the code (described below). Creating Notebooks for Novel ML Datasets TheExisting Tasks split captures realistic problems and notebook contexts, but may result in artiﬁcially high evaluation accuracies due to potential leakage of evaluation notebooks in the training data of LLMs, which is a common issue in LLM evaluation (Brown et al., 2020).3To prevent contamination, we additionally build the New Tasks split with 660 problems in notebooks created from scratch. Speciﬁcally, we create notebooks with wrangling and EDA tasks for 70tabular ML datasets that appeared on Kaggle since February 2022 and are manually veriﬁed to differ from existing datasets on the Web. For each Kaggle dataset, we instructed the annotators to create a notebook with tasks that would provide insights for building an ML model for the dataset. To make the problems more challenging, we also encouraged them to make tasks that require at least 5pandas API calls to solve. Annotating NL Intents When creating NL intents for a problem,4annotators are instructed to phrase their intents in the way they prefer when interacting with an AI system to help them implement the existing code solution, while keeping the intents natural and concise, without redundant elaboration such as lineby-line explanation. In addition, to 3JuICe and BigQuery primarily contain source ﬁles from 2019 or earlier, which exacerbates this issue. 4ForNew Tasks , intents are created before the solutions.make the intents more challenging, we encourage annotators to refer to entities and variables in the intents using semantic rewrites without introducing ambiguity ( e.g., use “ convert all binary columns to bool” instead of listing columns verbatim), reminiscent of synonym substitution for labeling utterances in textto-SQL (Gan et al., 2021). Mitigating Ambiguity in NL Intents Creating succinct NL intents without ambiguity could be nontrivial in this opendomain code generation setting, especially when there could be multiple plausible interpretations of an intent. For example, without the underlined part ofu5(Fig. 1), a programmer or a system may propose alternative solutions using different table schema. Therefore, for such openended problems where there could be multiple alternative ways to present the answer, we ask annotators to provide extra speciﬁcation in their intents about the desired output ( e.g., schema of the output DataFrame , such as the underlined part inu5). Even with these additional semantic constraints, empirically we observe that about 50% of intents are still underspeciﬁed, making ARCADE a challenging benchmark for handling realistic NL intents with uncertainty. We present more analysis in §3.2 and introduce a robust evaluation metric that mitigates this issue in §3.3. Annotation Guideline Besides mitigating ambiguity in intents, there are many other aspects to consider during annotation, such as notebook style (e.g., removing experiments on the WMT’19 Germanto-English, the datastore has 862M tokens, the vocabulary size is 42k, and the batch size was set to 12,000 tokens. While a normal Transformer translates 2,000 sentences in 7.5 seconds, kNNMT takes 2446.0 seconds. Note the kNN search is executed for each timestep in generating a target sentence.175Figure 2: Distance computation using asymmetric distance computation (ADC). and a target sentence. Concretely, a sentence datastoreSis deﬁned as follows: S={(h(x),y)|(x,y)∈D} , (4) where h:V|x| X→RD′represents a sentence encoder, which is a function that returns a D′- dimensional vector representation of a source sentence. Decoding At the beginning of decoding, the model retrieves the nnearest-neighbor sentences of the given input sentence from the sentence datastoreS. Let ˆS⊂S be the subset comprising nnearest-neighbor sentences. The nearest neighbor search space for target tokens in kNNMT is then drastically reduced by constructing the datastore corresponding to ˆSas follows: ˆM={(f(x,y<t), yt)| (h(x),y)∈ˆS,1≤t≤|y|},(5) where ˆM⊂M is the reduced datastore for the translation examples coming from the nnearest- neighbor sentences. During decoding, the model uses the same algorithm as kNNMT except that ˆMis used as the datastore instead of M. The proposed   analysis (PCA), and these vectors were then177quantized by PQ. At search time, a query vector is pretransformed to 256dimensions by multiplying the PCA matrix, and then the kNN target tokens are searched by ADC. The subset of a datastore can be loaded into GPU memory since it is signiﬁcantly smaller than the original kNNMT datastore, so we retrieved knearest-neighbor tokens from a subset on a GPU. Sentence Encoder We compared 4 different sentence encoders: LaBSE, AvgEnc, TFIDF, and BM25. LaBSE ( Feng et al. ,2022 ) is a pretrained sentence encoder, ﬁnetuned from multilingual BERT. AvgEnc is an average pooled encoder hidden vector of the Transformer NMT model, which is also used for translation. TFIDF ( Jones ,1972 ) and BM25 ( Jones et al. ,2000 ) compute vectors weighted the important words in a sentence. We used the raw count of tokens as the term frequency and applied addone smoothing to calculate the inverse document frequency, where a sentence was regarded as a document. We set k1= 2.0, b= 0.75in BM25 ( Jones et al. ,2000 ). Both TFIDF and BM25 vectors were normalized by their L2norm and their dimensionality was reduced to 256dimensions by singular value decomposition. 4.2 InDomain Translation We evaluated the translation quality and speed of subset kNNMT in the WMT’19 DeEn translation task (newstest2019; 2,000 sentences) and compared them with the kNNMT baselines ( Khandelwal et al. ,2021 ;Meng et al. ,2022 ). We used a trained Transformer big implemented in FAIRSEQ (Ott et al. ,2019 ) as the base MT model. We constructed the datastore from the parallel data of the WMT’19 DeEn news translation task with subword lengths of 250 or less and a sentence length ratio of 1.5 or less between the source and target sentences. The datastore contained 862.6M target tokens obtained from 29.5M sentence pairs. The subset size was set to n= 512 . Table 1shows our experimental results. In the table, “tok/s” denotes the number of tokens generated per second. The table shows that, although kNNMT improves 0.9 BLEU point from the base MT without additional training, the decoding speed is 326.1 times and 51.7 times slower with the B ∞and B 1settings, respectively. In contrast, our subset kNNMT ( h: LaBSE) is 111.8 times (with B ∞) and 47.4 times (with B 1) faster thankNNMT with no degradation in the BLEU↑tok/s Model ↑BLEU ↑COMET B ∞ B1 Base MT 39.2 84.56 6375.2 129.14 kNNMT 40.1 84.73 19.6 2.5 FastkNNMT 40.3 84.70 286.9 27.1 Ours: Subset kNNMT h: LaBSE 40.1 84.66 2191.4 118.4 h: AvgEnc 39.9 84.68 1816.8 97.3 h: TFIDF 40.0 84.63 2199.1 113.0 h: BM25 40.0 84.60 1903.9 108.4 Table 1: Results of translation quality and decoding speed in the WMT’19 DeEn translation task. “ h:” shows the type of sentence encoder used. score. Subset kNNMT ( h: AvgEnc) achieved speedups of 92.7 times (with B ∞) and 38.9 times (with B 1) with a slight quality degradation ( −0.2 BLEU and−0.05 COMET), despite using no external models. We also evaluated our subset kNNMT when using nonneural sentence encoders ( h: TFIDF, BM25). The results show that both TFIDF and BM25 can generate translations with almost the same BLEU score and speed as neural sentence encoders. In summary, this experiment showed that our subset kNNMT is two orders of magnitude faster than kNNMT and has the same translation performance. 4.3 Domain Adaptation Germanto-English We evaluated subset kNNMT on outof-domain translation in the IT, Koran, Law, Medical, and Subtitles domains ( Koehn and Knowles ,2017 ;Aharoni and Goldberg ,2020 ) with opendomain settings. The datastore was constructed from parallel data by merging all target domains and the general domain (WMT’19 DeEn) assuming that the domain of the input sentences is unknown. The datastore contained 895.9M tokens obtained from 30.8M sentence pairs. The NMT model is the same as that used in Section 4.2trained from WMT’19 DeEn. The subset size was set to n= 256 , and the batch size was set to 12,000 tokens. Table 2shows the results. Compared with base MT, kNNMT improves the translation performance in all domains but the decoding speed is much slower. In contrast, our subset kNNMT generates translations faster than kNNMT. However, in the domain adaptation task, there are differences in translation quality between those using neural sentence encoders and those using nonneural sentence encoders. The table shows178IT Koran Law Medical Subtitles Model BLEU tok/s BLEU tok/s BLEU tok/s BLEU tok/s BLEU tok/s Base MT 38.7 4433.2 17.1 5295.0 46.1 4294.0 42.1 4392.1 29.4 6310.5 kNNMT 41.0 22.3 19.5 19.3 52.6 18.6 48.2 19.8 29.6 30.3 Subset kNNMT h: LaBSE 41.9 2362.2 20.1 2551.3 53.6 2258.0 49.8 2328.3 29.9 3058.4 h: AvgEnc 41.9 2197.8 19.9 2318.4 53.2 1878.8 49.2 2059.9 30.0 3113.0 h: TFIDF 40.0 2289.0 19.3 2489.5 51.4 2264.3 47.5 2326.6 29.3 2574.4 h: BM25 40.0 1582.4 19.1 2089.5 50.8 1946.3 47.4 1835.6 29.4 1567.7 Table 2: Results of outof-domain translation with opendomain settings. The speed is evaluated with B ∞.Bold scores show the best translation performance in each domain. The COMET scores are listed in the appendix due to space limitations. that the use of nonneural sentence encoders (TFIDF and BM25) causes drop in translation quality, whereas the use of neural sentence encoders (LaBSE and AvgEnc) do not. In addition, compared with kNNMT, our subset kNNMT with neural encoders achieves an improvement of up to 1.6 BLEU points on some datasets. In summary, these results show that neural sentence encoders are effective in retrieving domainspeciﬁc nearest neighbor sentences from a large datastore. Englishto-Japanese We also evaluated our model on Englishto-Japanese translation. We used a pretrained Transformer big model trained from JParaCrawl v3 ( Morishita et al. ,2022 ) and evaluated its performance on Asian Scientiﬁc Paper Excerpt Corpus (ASPEC) ( Nakazawa et al. , 2016 ) and Kyoto Free Translation Task (KFTT; created from Wikipedia’s Kyoto articles) ( Neubig , 2011 ). The datastore was constructed from parallel data by merging ASPEC, KFTT, and the general domain (JParaCrawl v3). Note that ASPEC contains 3M sentence pairs, but we used only the ﬁrst 2M pairs for the datastore to remove noisy data, following Neubig (2014 ). The datastore contained 735.9M tokens obtained from 24.4M sentence pairs. The subset size was set to n= 512 , and the batch size was set to 12,000 tokens. Table 3shows the results. These show that kNNMT improves outof-domain translation performance compared with base MT on other language pairs other than Germanto-English. On Englishto-Japanese, subset kNNMT improves the decoding speed, but subset kNNMT with TFIDF and BM25 degrades the translation quality compared with kNNMT. However, subset kNNMT still achieves higher BLEU scores than base MT without any additional training steps, and it is two orders of magnitude faster than kNNMT.In summary, subset kNNMT can achieve better translation performance than base MT in exchange for a small slowdown in opendomain settings. 5 Discussion 5.1 Case Study: Effects of Subset Search Translation examples in the medical domain are shown in Table 4and the search results of the top3 nearest neighbor sentences are shown in Table 5. In the table, the subset kNNMT results are obtained using a LaBSE encoder. Table 4shows that subset kNNMT correctly generates the medical term “Coadministration”. The results of the nearest neighbor sentence search (Table 5) show that “Coadministration” is included in the subset. In detail, there are 30 cases of “Coadministration” and no case of “A joint use” in the whole subset consisting of k= 256 neighbor sentences. Base MT and kNNMT have the subwords of “Coadministration” in the candidates; however, the subwords of “A joint use” have higher scores. Table6shows the negative loglikelihood (NLL) of the ﬁrst three tokens and their average for each model. The second token of subset kNNMT, “- ” (hyphen), has a signiﬁcantly lower NLL than the other tokens. The number of “joint” and “- ” in the subset were 0 and 101, respectively, and theknearest-neighbor tokens were all “-” in subsetkNN-MT. Therefore, the NLL was low because pkNN(“-”) = 1 .0, so the joint probability of a beam that generates the sequence “Coadministration” is higher than “A joint use”. In summary, the proposed experiments conditioned on two widelyused datasets: RealToxicityPrompts (Gehman et al., 2020) and a QAdataset provided by Solaiman and Dennison (2021). Experimental results show that our MILDecoding method achieves faster decoding speed than other decodingtime methods, while it outperforms all other detoxification methods in reducing toxic text generation. We further verify that MILDecoding can mitigate toxicity conditioned on either nontoxic or toxic prompts. In summary, the contributions of our work are as follows: •We propose MILDecoding that introduces a trained MIL network to help avoid toxic generation. •Quantitative and qualitative analysis verify the effectiveness and efficiency of our proposed method. •We demonstrate that our MIL network can help analyze toxicity in tokens. 2  evaluation, where MILDecoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit. 1 Introduction Trained on huge amount of text corpora, Transformerbased (Vaswani et al., 2017) pretrained language models (LMs) have led to a wave of advances in natural language generation tasks (Radford et al. (2019); Lewis et al. (2019); Roberts et al. (2019)). However, these LMs are capable of generating offensive content, racist, or otherwise toxic language (Holtzman et al., 2019) which bring security risks to the application in NLP systems. To enable safe use and deployment of language model, it is necessary to undertake effective steps to mitigate toxic text generation. We examine the public comments provided in Jigsaw Toxic Comment Classification Challenge Dataset1(Jigsaw ) containing over 200K comments that were labeled as toxic. In most cases, several spans of harmful text cause the toxicity of the whole comment. In the example given in Table 1, 1https://www.kaggle.com/c/jigsawtoxic-commentclassification-challenge/[Comment] The only people who seem to give a crap about that stupid book are people like you who cite it as a pretense to claims of victimhood at the hands of those people. That’s the only reason it’s ever discussed. [...] Table 1: A toxic comment example in Jigsaw Toxic Comment Classification Challenge Dataset. The red tokens indicate the spans in the comment that induce toxicity. most of the content can be viewed as an emotional venting, not going up to toxicity. However, \"crap\" and\"stupid\" in this comment make it offensive. Prior studies (Gehman et al., 2020) attempt to filter out a specific word list at the decoding stage, which cannot achieve an obvious effect on mitigating toxicity in the generated text. Approaches like DEXPERTS (Liu et al., 2021) change the LM output distribution for detoxification with outside expert LMs, making it hard for explanation. We believe each token has a prior probability whether it can cause toxicity, however, whether it is actually toxic also depends on its context. Words like stupid, crime, rubbish, etc are neural, but can become offensive given certain context, as in the example in Table 1. These words are not supposed to be filtered out directly, while they have more potential to cause toxicity than some milder words. Therefore, we present MILDecoding, a tokenlevel detoxification in consideration of the contextual information with a multiple instance learning (MIL) neural network. At each decoding step, our proposed method uses a MIL network to score the retrieved tokens conditioned on the token itself and its contextual information. The MIL network predicts the toxicity of the token’s occurrence in the generated context to compute an extra toxicity distribution over candidate tokens to avoid toxic190generation. At inference time, we combine the toxicity distribution and the original LM probability distribution at each time step to determine which token to generate. We conduct experiments conditioned on two widelyused datasets: RealToxicityPrompts (Gehman et al., 2020) and a QAdataset provided by Solaiman and Dennison (2021). Experimental results show that our MILDecoding method achieves faster decoding speed than other decodingtime methods, while it outperforms all other detoxification methods in reducing toxic text generation. We further verify that MILDecoding can mitigate toxicity conditioned on either nontoxic or toxic prompts. In summary, the contributions of our work are as follows: •We propose MILDecoding that introduces a trained MIL network to help avoid toxic generation. •Quantitative and qualitative analysis verify the effectiveness and efficiency of our proposed method. •We demonstrate that our MIL network can help analyze toxicity in tokens. 2 analysis verify the effectiveness and efficiency of our proposed method. •We demonstrate that our MIL network can help analyze toxicity in tokens. 2 experiments we compare the ability of humans and several pretrained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexicallyguided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in nonadjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution. 1 Introduction Treating pretrained language models (LMs) as psycholinguistic subjects via the behavioral evaluation of their probability distributions has proven to be a very useful strategy to study to which extent they are able to generalize grammatical information from raw text (Linzen et al., 2016; Futrell et al., 2019). A common method consists of comparing model probabilities for grammatical and ungrammatical sentences (e.g., “The key to the cabinets is|*are on the table”). These experiments often concentrate on syntactic phenomena that are instantiated with surface strings that provide unequivocal information about the elements that enter the dependency, e.g. agreement morphology (Gulordava et al., 2018; Kuncoro et al., 2018a). Yet, we know less about the ability of LMs to coordinate syntactic and semantic information during the resolution of dependencies whose elements are not overtly signaled by morphosyntactic cues in theinput. Such is the case of control structures like those in (1). These superficially simple constructions underlie complex lexicallyguided antecedent retrieval processes, and they represent an interesting candidate to study dependency resolution at the syntaxsemantics interface. (1) a. María ifle prometió a José jmser ordenada if. María promised José to be tidy. b. José imle ordenó a María jfser ordenada jf. José ordered María to be tidy. At the infinitive verb serin (1), it is crucial to interpret its implicit subject. In other words, who is tidy? The term control reflects the idea that the interpretation of the implicit subject is controlled by, or is determined by, another referent (Rosenbaum, 1967; Chomsky, 1981). This type of control dependencies entail interpreting an anaphoric relation between the implicit subject of the embedded clause and one of the NPs in the main clause (known as controller or antecedent). Crucially, this interpretive relation is guided by specific lexicosemantic properties of the main clause predicates (Jackendoff and Culicover, 2003). In (1a), the correct antecedent is Juan (the main clause subject) because promise has subject control properties. In (1b), the correct antedecent is María (the main clause object), because order has object control properties. Retrieving the correct antecedent is essential in order to build an accurate representation of the message and to compute the agreement dependency that is established between the controller and the adjective tidy. Consequently, the resolution of these dependencies entails coordinating information about the lexicosemantic properties of control predicates, coreference, and agreement morphology, and provides a great context for probing LMs’ grammatical abilities beyond morphosyntax. In this work, we take advantage of the rich agreement properties of two Romance languages (Spanish and Galician) in order to examine humans’ and203language models’ ability to correctly identify control dependencies. To do so, we have carefully created an experimental design via the manipulation of the gender of the NPs (feminine/masculine), the type of control verb (subject/object control), and the gender of the embedded adjective. This design will allow us to test whether humans and LMs identify or produce agreement violations at the adjective, which is used as a proxy for the accuracy of antecedent retrieval processes. Furthermore, this design will allow us to test for the presence of interference effects of noncontrolling NPs (referred to as distractors) when they match or mismatch in gender with the embedded adjective. We created several datasets that have been used for a human acceptability judgement task (Experiment 1), a LM acceptability task (Experiment 2), and a LM prediction task (Experiment 3). For Experiments 2 and 3, we tested the most prominent monolingual and multilingual masked LMs based on transformers for Spanish, and provide additional translated datasets and results from the same computational experiments carried out with Galician LMs in order to confirm the crosslinguistic robustness of our findings. Our results show that while humans correctly identify the acceptability of the strings regardless of the configuration of the NPs, language models often fail to correctly identify the relevant antecedent in subject control dependencies, showing their reliance on linear relations rather than linguistic information, something which is observed in their belowchance accuracy for discontinuous dependencies. The main contributions of our paper are: (i) the release of widecovering and highly controlled datasets to evaluate control structures in Spanish and Galician, (ii) a psycholinguistic evaluation of humans’ performance, a computational evaluation of monolingual and multilingual LMs’ performance, and a careful comparison between humans and LMs; (iii) a demonstration of the limitations of LMs to capture grammatical information thanks to the adversarial example of control constructions. 2 Related work Targeted evaluation of LMs: Targeted evaluations of LMs focusing on different syntactic phenomena have found evidence suggesting that these models may generalize syntactic information from raw text (Linzen et al., 2016; Goldberg, 2019; Futrell et al., 2019; Mueller et al., 2020). In thisregard, the subjectverb (number) agreement task is one of the most used adversarial examples for these evaluations, although Marvin and Linzen (2018) introduced further experiments dealing with other syntactic phenomena in English (such as negative polarity items or reflexive anaphora). These types of datasets have been extended and adapted to different languages (Warstadt et al., 2020; Mueller et al., 2020; PérezMayos et al., 2021) and incorporated into online evaluation platforms (Gauthier et al., 2020). In these experiments, the overall performance of large pretrained LMs is found to be comparable to that of human subjects (Bernardy and Lappin, 2017; Gulordava et al., 2018; Kuncoro et al., 2018b), except for longdistance dependencies with distracting nouns between the elements of the target dependencies (Marvin and Linzen, 2018), where LMs often fail to identify the target dependency relation. Besides, recent work found that LMs’ perplexity is not always correlated to their syntactic generalization abilities (Hu et al., 2020), nor with human reading times (Eisape et al., 2020). Other complex structures that seem difficult to interpret by LMs are nested constructions, which may require recursive abilities to be solved. Recent studies on Italian and English have found that, although both recurrent and transformer neural networks achieve nearperfect performance on short embedded dependencies, their performance drops to belowchance levels on slightly longer dependencies, unlike humans (Lakretz et al., 2021, 2022). Lampinen (2022), however, questions these comparisons between humans and LMs, as the former receive guidance before the experiments, while LMs are evaluated on zeroshot scenarios, and their performance improves with fewshot prompts. Despite the fact that most of the work evaluating the linguistic capabilities of LMs has been carried out in English, there exist some experiments that have focused on Spanish and Galician LMs showing that the LMs tested in this work perform very well in the context of different linguistic dependencies, including simple and complex agreement dependencies with distractors. Recent studies in both Spanish and Galician show that models’ performance for these dependencies (which rely on morphosyntactic information) are similar to those in English (with expected variations across models). For instance, PérezMayos et al. (2021) found that monolingual and multilingual models achieve even better performance in agreement resolution in Span204ish than BERT in English. For Galician, several experiments showed that the monolingual BERT models can generalize morphosyntactic agreement (number and gender) on complex subjectverb and subjectpredicative adjective dependencies (Garcia and CrespoOtero, 2022), and that this information is learned relatively early in the training process (de DiosFlores and Garcia, 2022). The syntactic strengths observed in these models establish a baseline performance against which we can examine the results obtained for control dependencies. Concerning control constructions, studies exploring LMs’ abilities to solve these complex relations are very scarce. In a recent paper, Kogkalidis and Wijnholds (2022) trained supervised models that take advantage of contextualized representations extracted from BERT, and evaluate them at capturing control verb nesting and verb raising in Dutch. The results suggest that transformer LMs do not adequately capture the target relations, although finetuning the pretrained models in oneshot learning scenarios improves the performance of the probes. More similar to our study, an initial approximation by Lee and Schuster (2022) evaluated GPT2 on object and subject control verbs, using number agreement with an embedded reflexive pronoun to track dependency resolution. Their findings suggest generative LMs are unable to differentiate between these two types of constructions. However, their manipulations were very limited in scope, as they only used 5 noun phrases, and 3 control verbs. Pycholinguistics and control dependencies: Even though control constructions have been at the center of linguistic theorizing over the past decades, their theoretical interest has not translated into an equivalent amount of experimental research in the psycholinguistics literature. The key question, though, is whether (and how) control information is used in parsing. Some early works have argued that control information was not used during initial parsing stages due to its lexicosemantic nature (e.g., Frazier et al., 1983; Nicol and Swinney, 1989). Nonetheless, these works barely looked at the contrast between lexically induced subject and object control relations. In this regard, more recent eyetracking investigations have produced results that could be interpreted as evidence that lexical control information is used from early parsing stages (e.g. de DiosFlores, 2021; Betancort et al., 2006; Kwon and Sturt, 2016) while they also suggest that object control dependencies seem to besolved faster due to their linear proximity. Yet, to our knowledge, no previous work provided acceptability judgements contrasting subject and object control dependencies with distractors, which is a highly informative measurement to establish the grammatical and psycholinguistic status of such constructions. 3 The present work The present work takes control dependencies as an adversarial case to test LMs’ ability to generalize grammatical information at the syntaxsemantics interface (Experiments 2 and 3). Given the complexity of these constructions, and the lack of psycholinguistic evidence, we go one step further and start by evaluating humans’ grammaticality perception (Experiment 1), not only to obtain a grammatical verification of the acceptability status of such innovative experimental materials and to be able to directly compare humans’ and LMs’ performance, but also to contribute to the scarce psycholinguistic evidence on the processing of control. The datasets, code, and results from all the experiments are freely available.1 3.1 Experimental materials For the main dataset, used in Experiments 1 and 2, the experimental materials consisted of 96 items that had 8 different versions (768 experimental sentences). An example set is shown in Table 1. The experimental conditions were created by manipulating the type of control verb and the gender of the main clause nouns, while keeping the gender of the adjective constant. It is a factorial design that fully crosses the factors control (subject/object), grammaticality (grammatical/ungrammatical) and distractor (match/mismatch). To create the control conditions, we selected 12 subject and 12 object control verbs whose control preferences (i.e. subject and object) had been shown to be robust in a largesample cloze task conducted by de DiosFlores (2021). A sentence is ungrammatical when the adjective and the target controller differ in gender. The term distractor is used to refer to the noncontroller NP in the sentence. A distractor was considered a match when it matches in gender with the adjective, and a mismatch when it mismatches in gender. One of the key elements of our manipulation is the difference in dependency length between sub1https://github.com/iriadf/ACL2023_Control205Subject control Gramm.Dist. match Maríafle prometió a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Maríafle prometió a Manuelmser más ordenadafcon los apuntes. Ungramm.Dist. match Josémle prometió a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Josémle prometió a Manuelmser más ordenadafcon los apuntes. Object control Gramm.Dist. match Maríafle ordenó a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Josémle ordenó a Carmenfser más ordenadafcon los apuntes. Ungramm.Dist. match Maríafle ordenó a Manuelmser más ordenadafcon los apuntes. Dist. mismatch Josémle ordenó a Manuelmser más ordenadafcon los apuntes. Table 1: Sample set of the experimental materials for Experiments 1 and 2 meaning María promised/ordered Carmen to be tidier with the notes (obviating the names, whose gender is indicated with superscripts fandm). The correct antecedents are boldtyped. The nonboltyped nouns are the distractors. ject and object control. While subject control constructions engage in a discontinuous dependency where the object NP (the distractor) is intervening, object control dependencies engage in an adjacent dependency, where the subject NP (the distractor) precedes the dependency. Those conditions in which the two NPs (controller and distractor) have the same gender are respectively taken as grammatical and ungrammatical baselines for both subject and object control sentences. Hence, the critical conditions are those in which only one of the NPs agrees in gender with the adjective (i.e. grammatical sentences with a matching distractor and ungrammatical sentences with a mismatching distractor). Humans’ and LMs’ behavior in these conditions will be essential to ascertain whether they can accurately implement controldetermined antecedent retrieval processes and whether they are fallible to interference effects from gender matching but structurally irrelevant antecedents, in a similar vein as the attraction effects observed in agreement dependencies (e.g. Bock and Miller, 1991). While there are very few genderambiguous names in Spanish, in order to maximize gender transparency, the nouns used to create the materials were carefully selected according to the most frequent femaleonly and maleonly names on the official Spanish census. In addition, we created an adaptation of the main dataset substituting proper nouns with personal pronouns (e.g. ‘She promised him to be tidier’), to avoid potential bias, ambiguities or misrepresentations of proper nouns (Shwartz et al., 2020). Both versions of the dataset (with nouns and with pronouns) were translated into Galician by a native speaker linguist, to put Galician LMs to the test and to check if our findings heldcrosslinguistically. These materials were adapted for the LM prediction task (see section 6.1). 3.2 Pretrained models We evaluate the following pretrained models using HuggingFace’s transformers library (Wolf et al., 2020): Multilingual : mBERT (12 layers) (Devlin et al., 2019), and XLMRoBERTa base and large (12 and 24 layers) (Conneau et al., 2020). Spanish : BETO (12 layers) (Cañete et al., 2020), and RoBERTa base and large (12 and 24 layers) (Gutiérrez Fandiño et al., 2022). Galician : Bertinho small and base (6 and 12 layers) (Vilares et al., 2021), and BERT small and base (6 and 12 layers) (Garcia, 2021). 4 Experiment 1: human acceptability The primary goal of this acceptability task is to determine whether native speakers of Spanish are able to detect agreement violations that do not conform with the control properties of main predicates. This is, to our knowledge, the first experimental investigation on control of its kind, and we believe it is essential to corroborate native speakers’ offline sensitivity to the different control manipulations that will be then put to the test with artificial LMs. It will be of particular importance to elucidate whether comprehenders are able to correctly distinguish the acceptability of the strings regardless of the type of control (subject or object) and the presence of a gender matching or mismatching distractor.2064.1 Participants and procedure 40 native speakers of Spanish recruited at the Universidade de Santiago de Compostela participated in this experiment. Their participation was voluntary and all of them provided informed consent. Participants were presented with the entire sentence in the middle of the screen along with a rating scale, and they could only move to the next one once they had emitted a rating. They were instructed to rate the sentences in terms of whether they came across as wellformed Spanish: 7 meaning totally acceptable and 1 totally unacceptable. Experimental sentences were intermixed with 96 filler sentences of similar structure and complexity. The task was completed by all participants in less than 30 minutes. Figure 1: Mean ratings in Experiment 1 . Error bars indicate standard error of the mean. 4.2 Results The average rating for each condition is shown in Figure 1. For this and the following experiments, we carried out a statistical analysis of variance in order to observe differences among the experimental conditions. For the sake of clarity and space, the most relevant significant differences will be marked with an asterisk in the figures. The statistical analyses revealed a significant main effect of grammaticality, such that grammatical sentences (green bars) received much higher ratings than ungrammatical ones (red bars). Importantly, there was a significant interaction between the factors grammaticality and distractor. Planned comparisons showed thatthis interaction was driven by a significant effect of distractor only in ungrammatical sentences. This is shown in significant higher ratings for the distractor match condition in ungrammatical sentences compared to distractor mismatch ones. Such an effect is not present in grammatical sentences. Critically, no differences were observed between subject and object control conditions. In addition, we took the 17 ratings produced by humans and converted them into a binary accuracy measure by classifying their answers as correct or incorrect depending on the grammaticality of the sentence and whether the rating issued was above or below the sample mean (3.79). As expected, accuracy was above 85% for all conditions. This value will allow us to have a more direct comparison with the results from Experiment 3. 4.3 Discussion The results from this experiment clearly show that native speakers are able to detect agreement violations that arise when the adjective did not match in gender the appropriate antecedent, and hence, that they are able to correctly use control information to retrieve the antecedent. This finding also provides a confirmation that the items display unequivocal control readings. Crucially, subject and object control sentences were rated similarly across all four conditions. In addition to the clear contrast between grammatical and ungrammatical conditions, an important result from this experiment is that there is evidence for interference effects in ungrammatical sentences. That is, ungrammatical sentences with a matching distractor received slightly higher ratings than ungrammatical sentences with a mismatching distractor. This effect shows that the presence of a matching distractor leads them to accept ungrammatical sentences more often than when the distractor does not match in gender with the adjective. Crucially, this effect appeared equally in subject and object control conditions, that is, independently of the position of the distractor. This represents evidence for a facilitatory interference effect, or an illusion of grammaticality (Phillips et al., 2011), a pattern akin to the widely attested agreement attraction effect (Wagers et al., 2009). 5 Experiment 2: LM acceptability This experiment aims at observing whether the probabilities of the language models are similar207to those of humans. That is, whether LMs assign lower surprisal to grammatical than to ungrammatical sentences regardless of the presence of a matching or mismatching distractor. For this purpose, we use the exact same dataset as in Experiment 1.2 5.1 Procedure The minicons library (Misra, 2022) was used to compute the surprisal assigned by the LM to the embedded adjectives, which function as a proxy for antecedent retrieval. 5.2 Results The Spanish models’ results for the different experimental conditions are shown in Figure 2. It should be noted that, for ease of interpretation and comparison with Experiment 1 (Figure 1), the surprisal values were inverted such that higher values mean less surprisal (hence more acceptability) while lower values mean more surprisal (hence less acceptability).3While we observe significant effects of grammaticality for all models (meaning that, overall, grammatical sentences were more acceptable than ungrammatical ones), the results show a very different pattern of contrasts for subject and object control sentences. On the one hand, in subject control sentences, all the models showed higher acceptance for grammatical sentences with a matching distractor (dark green bars) than for grammatical sentences with a mismatching distractor (light green bars). Furthermore, also in subject control sentences, ungrammatical sentences with a matching distractor (light red bars) received unexpectedly high acceptance levels, which, for most models, are higher than those observed for grammatical sentences with a mismatching distractor (light green bars). On the other hand, in object control sentences, the pattern of contrasts is very different. First, none of the models exhibited differences among the grammatical conditions regardless of the gender of the distractor. Second, while for all the models, the values observed for ungrammatical sentences with a matching distractor (light red bars) were higher than those for ungrammatical sentences with a mismatching distractor (dark red 2It should be noted that comparing these two dependent measurements (human likertscale acceptability judgements and LM’s surprisal values) is not an optimal contrast, but in our view, conceptually reasonable, as similar LM model measurements are often taken as a proxy for acceptability (e.g. Futrell et al., 2019). 3This was done by subtracting each mean value from the highest mean value observed.bars), this difference was only statistically significant for some models. The same pattern of results is observed using pronouns instead of names (see Figure 4) and for the Galician models using names and pronouns (Figures 5 and 6). This is also corroborated by the very strong correlations ( ρ > 0.9) observed for the adjective surprisal values using names and pronouns, in both languages. Furthermore, we calculated the Spearman ρcorrelations between the acceptability values provided by the humans and the models’ surprisal values. Overall, they revealed weak to moderate correlations, while higher correlations are found for object control sentences than for subject control ones. The correlations for each model at each experimental condition can be found in Table 4.4 5.3 Discussion The results for Experiment 2 show that, unlike humans, all the LMs evaluated behave very differently for subject than for object control dependencies, being better at detecting the acceptability of the strings in object control conditions. The key question here is whether they are able to do so by leveraging the lexicosemantic information of control in order to find the correct antecedent. The pattern of results obtained suggests that, rather than control information, the relevant cue being used is linear proximity. It must be reminded that, in subject control dependencies, the correct antecedent is the NP that is further away from the adjective, while the distractor NP is closer to it. The presence of significant differences between the two grammatical conditions, and the two ungrammatical conditions, found for subject control dependencies, points to the fact that LMs are taking the closer (and wrong) NP, the object, as the antecedent. This explains why the acceptability is reduced for grammatical sentences with a mismatching distractor, despite being perfectly grammatical, and that it is dramatically increased for ungrammatical sentences with a matching distractor, despite being ungrammatical. Reliance on linear proximity also explains why LMs are better, and more akin to humans, on object control dependencies. In these structures, the correct antecedent (i.e. the object) coincides with the linearly closest NP. Interestingly, nonetheless, LMs also exhibit evidence for interference effects from 4This table also includes correlations for wholesentence surprisal measurements.208Figure 2: Spanish LMs acceptability predictions for the adjective in Experiment 2 using names . Error bars indicate standard error of the mean. controlirrelevant but gendermatching distractors. This is perhaps clearer in the case of object control sentences, since linear proximity and interference converge in the case of subject control ungrammatical sentences with a matching distractor. In object control sentences, some models also exhibit higher acceptability for ungrammatical sentences with a matching distractor even when in this case the gendermatching NP is the farthest NP. These issues are further explored in Experiment 3. 6 Experiment 3: LM masked prediction This experiment aims at further exploring the behavior of LMs using the masked prediction task. In contrast with Experiment 2, where we compute the surprisal for the same adjective in a given (grammatical or ungrammatical) sentence, our objective here is to test whether LMs predict grammatically compatible adjectives in subject and object control sentences regardless of the presence of a matching or mismatching distractor. In Experiment 2 the adjective’s gender was kept constant across experimental conditions, and hence, we could not assess LMs’ preferences for the masculine or feminine version. By contrast, here we test if LMs predict grammatically compatible adjectives in subject and object control sentences by directly comparing the probabilities of a given adjective in its masculine or feminine form, something which provides uswith more comprehensive information in this respect. Furthermore, evaluating model accuracy rather than surprisal values will also allow us to assess and compare the performance across models. 6.1 Experimental materials The experimental materials used for Experiment 3 are an adaptation of the dataset described in section 3.1 (including its variants with personal pronouns and Galician translations) so that they could be used in the masked prediction task. This allows us to evaluate our dataset in the two possible gender configurations, expanding it such that each sentence has two possible outcomes: a grammatical and an ungrammatical one. Therefore, the manipulation is a 2x2 factorial design ( control x distractor ), as shown in Table 2 6.2 Procedure We rely on the standard approach for targeted syntactic evaluation to obtain the accuracy of the models on the minimal pairs (Linzen et al., 2016; Warstadt et al., 2020). For each sentence, we extract the probabilities of the grammatical and ungrammatical target adjectives, and consider a trial as correct if the model gives a higher probability to the grammatical target adjective. It is worth noting that this method requires compatible tokenization between both variants (grammatical and ungrammatical). To make a fair evaluation,209Subject control Dist. match Maríafle prometió a Carmen fser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Maríafle prometió a Manuelmser más [ordenadaf|*ordenadom]con los apuntes. Object control Dist. match Maríafle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Josémle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Table 2: Sample set of the experimental materials for Experiment 3. The correct antecedents and correct and incorrect the target adjectives are bold typed. See Table 1 for comparison with the original dataset. we check if both variants appear as single tokens in the models’ vocabulary, or whether their last subtokens (the ones that carry the morphosyntactic information) are comparable so that we can use their probabilities. For instance, the Spanish pair afectuoso|afectuosa (‘affectionate’) is tokenized by RoBERTa as afect+uoso|uosa , and hence, we can use the last subtokens for comparison. However, desconfiado|desconfiada (‘skeptical’) is divided as desconf+iado anddescon+fi+ada . We discard these incompatible cases (19% of the items for Spanish, and 16% for Galician, on average).5 6.3 Results Table 3 displays the global accuracy for all the models under evaluation in Experiment 3 (global accuracy values for all the datasets tested in Spanish and Galician are in Table 5). RoBERTa large emerges as the best performing model, closely followed by XLM RoBERTa large, while mBERT base emerges as the worst performing model. Nonetheless, in order to analyze the impact of linear proximity on model performance, it is essential to examine the factors control anddistractor separately. Figure 3 shows the accuracy per condition for the target adjectives. Statistical analyses show a main effect of distractor, such that the accuracy was higher for distractor match sentences (dark green bars, when the two NPs had the same gender) than for distractor mismatch ones (light green bars, when the NPs differed in gender). However, this difference was much more acute for subject control sentences, where significant differences arise for all the models, than for object control sentences, where significant differences are only found for 5We also assessed the models’ performance by computing the probability mass that models put on the feminine and masculine inflections rather than on a particular adjective pair, inspired by Newman et al. (2021). We used morphological lexicons to obtain the masculine and feminine probabilities from the top N adjectives predicted by the models in the masked position (N=100). The results for top N (to be found in Appendix C) followed the same pattern as for target adjectives.RoBERTalarge and XLMRoBERTa-base. The same pattern of results is observed using pronouns instead of names (see Figure 7) and for the Galician models using names and pronouns (Figures 8 and 9). This is also shown in the very strong correlations ( ρ > 0.8) observed for the results using names and pronouns in both languages. Model Accuracy BETO base 0.78 RoBERTa base 0.77 RoBERTa large 0.83 mBERT base 0.61 XLM RoBERTa base 0.78 XLM RoBERTa large 0.82 Table 3: Global accuracy in Experiment 3. 6.4 Discussion The results from Experiment 3 reinforce and complement the findings from Experiment 2 in several respects. First, reliance on linear proximity is, if anything, even clearer, as subject control sentences with a mismatching distractor display clear interference effects, which are materialized in a dramatically belowchance accuracy. These are the cases in which the distractor is the sentence object, which is also the closer NP. In these cases, LMs’ predict a target adjective that agrees in gender with the object, rather than the subject (i.e. the correct antecedent) and hence, demonstrating that antecedent retrieval processes unfold disregarding the lexicosemantic information on control. Importantly, these effects are almost absent in object control sentences, where only two models show evidence for interference effects, these being much less pronounced (only a few accuracy points). Even though the results from this experiment cannot be directly compared with those of humans (Experiment 1), it should be noted that human accuracy was above 80% for all conditions.210Figure 3: Spanish LMs accuracies for the target adjective in Experiment 3 using names . 7 General discussion and conclusions The empirical evidence gathered in this work provides a very straightforward picture: whereas humans’ can coordinate lexicosemantic and syntactic information in order to determine the (un)acceptability of control structures, LMs resort to a heuristic based on linear proximity, disregarding control information. These findings are robust, as they replicate across tasks (acceptability and masked prediction), models (monolingual and multilingual), languages (Spanish and Galician LMs), and type of antecedent (names and pronouns). Furthermore, they go in line with evidence advanced in Lee and Schuster (2022) for English with respect to autoregressive language models. These findings contrast with those obtained for superficially similar dependencies like subjectverb agreement, in which these models have been attested to display accurate levels of performance for Spanish and Galician (PérezMayos et al., 2021; de DiosFlores and Garcia, 2022; Garcia and CrespoOtero, 2022). Crucially, however, agreement and control dependencies engage different types of linguistic information. While the former rely on coocurring patterns containing overt morphological cues which are pervasive in the training data, control dependencies rely on experimental setup, including hyperparameter search and bestfound hyperparameter values? Not applicable. Left blank. /square\u0013C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Results section: 4.2, 5.2, and 6.3 /squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank. D/square\u0013Did you use human annotators (e.g., crowdworkers) or research with human participants? Section 4.1 /square\u0013D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? We reported a summary in section 4.1. The acceptability task is a widespread evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution. 1 Introduction Treating pretrained language models (LMs) as psycholinguistic subjects via the behavioral evaluation of their probability distributions has proven to be a very useful strategy to study to which extent they are able to generalize grammatical information from raw text (Linzen et al., 2016; Futrell et al., 2019). A common method consists of comparing model probabilities for grammatical and ungrammatical sentences (e.g., “The key to the cabinets is|*are on the table”). These experiments often concentrate on syntactic phenomena that are instantiated with surface strings that provide unequivocal information about the elements that enter the dependency, e.g. agreement morphology (Gulordava et al., 2018; Kuncoro et al., 2018a). Yet, we know less about the ability of LMs to coordinate syntactic and semantic information during the resolution of dependencies whose elements are not overtly signaled by morphosyntactic cues in theinput. Such is the case of control structures like those in (1). These superficially simple constructions underlie complex lexicallyguided antecedent retrieval processes, and they represent an interesting candidate to study dependency resolution at the syntaxsemantics interface. (1) a. María ifle prometió a José jmser ordenada if. María promised José to be tidy. b. José imle ordenó a María jfser ordenada jf. José ordered María to be tidy. At the infinitive verb serin (1), it is crucial to interpret its implicit subject. In other words, who is tidy? The term control reflects the idea that the interpretation of the implicit subject is controlled by, or is determined by, another referent (Rosenbaum, 1967; Chomsky, 1981). This type of control dependencies entail interpreting an anaphoric relation between the implicit subject of the embedded clause and one of the NPs in the main clause (known as controller or antecedent). Crucially, this interpretive relation is guided by specific lexicosemantic properties of the main clause predicates (Jackendoff and Culicover, 2003). In (1a), the correct antecedent is Juan (the main clause subject) because promise has subject control properties. In (1b), the correct antedecent is María (the main clause object), because order has object control properties. Retrieving the correct antecedent is essential in order to build an accurate representation of the message and to compute the agreement dependency that is established between the controller and the adjective tidy. Consequently, the resolution of these dependencies entails coordinating information about the lexicosemantic properties of control predicates, coreference, and agreement morphology, and provides a great context for probing LMs’ grammatical abilities beyond morphosyntax. In this work, we take advantage of the rich agreement properties of two Romance languages (Spanish and Galician) in order to examine humans’ and203language models’ ability to correctly identify control dependencies. To do so, we have carefully created an experimental design via the manipulation of the gender of the NPs (feminine/masculine), the type of control verb (subject/object control), and the gender of the embedded adjective. This design will allow us to test whether humans and LMs identify or produce agreement violations at the adjective, which is used as a proxy for the accuracy of antecedent retrieval processes. Furthermore, this design will allow us to test for the presence of interference effects of noncontrolling NPs (referred to as distractors) when they match or mismatch in gender with the embedded adjective. We created several datasets that have been used for a human acceptability judgement task (Experiment 1), a LM acceptability task (Experiment 2), and a LM prediction task (Experiment 3). For Experiments 2 and 3, we tested the most prominent monolingual and multilingual masked LMs based on transformers for Spanish, and provide additional translated datasets and results from the same computational experiments carried out with Galician LMs in order to confirm the crosslinguistic robustness of our findings. Our results show that while humans correctly identify the acceptability of the strings regardless of the configuration of the NPs, language models often fail to correctly identify the relevant antecedent in subject control dependencies, showing their reliance on linear relations rather than linguistic information, something which is observed in their belowchance accuracy for discontinuous dependencies. The main contributions of our paper are: (i) the release of widecovering and highly controlled datasets to evaluate control structures in Spanish and Galician, (ii) a psycholinguistic evaluation of humans’ performance, a computational evaluation of monolingual and multilingual LMs’ performance, and a careful comparison between humans and LMs; (iii) a demonstration of the limitations of LMs to capture grammatical information thanks to the adversarial example of control constructions. 2 Related work Targeted evaluation of LMs: Targeted evaluations of LMs focusing on different syntactic phenomena have found evidence suggesting that these models may generalize syntactic information from raw text (Linzen et al., 2016; Goldberg, 2019; Futrell et al., 2019; Mueller et al., 2020). In thisregard, the subjectverb (number) agreement task is one of the most used adversarial examples for these evaluations, although Marvin and Linzen (2018) introduced further experiments dealing with other syntactic phenomena in English (such as negative polarity items or reflexive anaphora). These types of datasets have been extended and adapted to different languages (Warstadt et al., 2020; Mueller et al., 2020; PérezMayos et al., 2021) and incorporated into online evaluation platforms (Gauthier et al., 2020). In these experiments, the overall performance of large pretrained LMs is found to be comparable to that of human subjects (Bernardy and Lappin, 2017; Gulordava et al., 2018; Kuncoro et al., 2018b), except for longdistance dependencies with distracting nouns between the elements of the target dependencies (Marvin and Linzen, 2018), where LMs often fail to identify the target dependency relation. Besides, recent work found that LMs’ perplexity is not always correlated to their syntactic generalization abilities (Hu et al., 2020), nor with human reading times (Eisape et al., 2020). Other complex structures that seem difficult to interpret by LMs are nested constructions, which may require recursive abilities to be solved. Recent studies on Italian and English have found that, although both recurrent and transformer neural networks achieve nearperfect performance on short embedded dependencies, their performance drops to belowchance levels on slightly longer dependencies, unlike humans (Lakretz et al., 2021, 2022). Lampinen (2022), however, questions these comparisons between humans and LMs, as the former receive guidance before the experiments, while LMs are evaluated on zeroshot scenarios, and their performance improves with fewshot prompts. Despite the fact that most of the work evaluating the linguistic capabilities of LMs has been carried out in English, there exist some experiments that have focused on Spanish and Galician LMs showing that the LMs tested in this work perform very well in the context of different linguistic dependencies, including simple and complex agreement dependencies with distractors. Recent studies in both Spanish and Galician show that models’ performance for these dependencies (which rely on morphosyntactic information) are similar to those in English (with expected variations across models). For instance, PérezMayos et al. (2021) found that monolingual and multilingual models achieve even better performance in agreement resolution in Span204ish than BERT in English. For Galician, several experiments showed that the monolingual BERT models can generalize morphosyntactic agreement (number and gender) on complex subjectverb and subjectpredicative adjective dependencies (Garcia and CrespoOtero, 2022), and that this information is learned relatively early in the training process (de DiosFlores and Garcia, 2022). The syntactic strengths observed in these models establish a baseline performance against which we can examine the results obtained for control dependencies. Concerning control constructions, studies exploring LMs’ abilities to solve these complex relations are very scarce. In a recent paper, Kogkalidis and Wijnholds (2022) trained supervised models that take advantage of contextualized representations extracted from BERT, and evaluate them at capturing control verb nesting and verb raising in Dutch. The results suggest that transformer LMs do not adequately capture the target relations, although finetuning the pretrained models in oneshot learning scenarios improves the performance of the probes. More similar to our study, an initial approximation by Lee and Schuster (2022) evaluated GPT2 on object and subject control verbs, using number agreement with an embedded reflexive pronoun to track dependency resolution. Their findings suggest generative LMs are unable to differentiate between these two types of constructions. However, their manipulations were very limited in scope, as they only used 5 noun phrases, and 3 control verbs. Pycholinguistics and control dependencies: Even though control constructions have been at the center of linguistic theorizing over the past decades, their theoretical interest has not translated into an equivalent amount of experimental research in the psycholinguistics literature. The key question, though, is whether (and how) control information is used in parsing. Some early works have argued that control information was not used during initial parsing stages due to its lexicosemantic nature (e.g., Frazier et al., 1983; Nicol and Swinney, 1989). Nonetheless, these works barely looked at the contrast between lexically induced subject and object control relations. In this regard, more recent eyetracking investigations have produced results that could be interpreted as evidence that lexical control information is used from early parsing stages (e.g. de DiosFlores, 2021; Betancort et al., 2006; Kwon and Sturt, 2016) while they also suggest that object control dependencies seem to besolved faster due to their linear proximity. Yet, to our knowledge, no previous work provided acceptability judgements contrasting subject and object control dependencies with distractors, which is a highly informative measurement to establish the grammatical and psycholinguistic status of such constructions. 3 The present work The present work takes control dependencies as an adversarial case to test LMs’ ability to generalize grammatical information at the syntaxsemantics interface (Experiments 2 and 3). Given the complexity of these constructions, and the lack of psycholinguistic evidence, we go one step further and start by evaluating humans’ grammaticality perception (Experiment 1), not only to obtain a grammatical verification of the acceptability status of such innovative experimental materials and to be able to directly compare humans’ and LMs’ performance, but also to contribute to the scarce psycholinguistic evidence on the processing of control. The datasets, code, and results from all the experiments are freely available.1 3.1 Experimental materials For the main dataset, used in Experiments 1 and 2, the experimental materials consisted of 96 items that had 8 different versions (768 experimental sentences). An example set is shown in Table 1. The experimental conditions were created by manipulating the type of control verb and the gender of the main clause nouns, while keeping the gender of the adjective constant. It is a factorial design that fully crosses the factors control (subject/object), grammaticality (grammatical/ungrammatical) and distractor (match/mismatch). To create the control conditions, we selected 12 subject and 12 object control verbs whose control preferences (i.e. subject and object) had been shown to be robust in a largesample cloze task conducted by de DiosFlores (2021). A sentence is ungrammatical when the adjective and the target controller differ in gender. The term distractor is used to refer to the noncontroller NP in the sentence. A distractor was considered a match when it matches in gender with the adjective, and a mismatch when it mismatches in gender. One of the key elements of our manipulation is the difference in dependency length between sub1https://github.com/iriadf/ACL2023_Control205Subject control Gramm.Dist. match Maríafle prometió a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Maríafle prometió a Manuelmser más ordenadafcon los apuntes. Ungramm.Dist. match Josémle prometió a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Josémle prometió a Manuelmser más ordenadafcon los apuntes. Object control Gramm.Dist. match Maríafle ordenó a Carmenfser más ordenadafcon los apuntes. Dist. mismatch Josémle ordenó a Carmenfser más ordenadafcon los apuntes. Ungramm.Dist. match Maríafle ordenó a Manuelmser más ordenadafcon los apuntes. Dist. mismatch Josémle ordenó a Manuelmser más ordenadafcon los apuntes. Table 1: Sample set of the experimental materials for Experiments 1 and 2 meaning María promised/ordered Carmen to be tidier with the notes (obviating the names, whose gender is indicated with superscripts fandm). The correct antecedents are boldtyped. The nonboltyped nouns are the distractors. ject and object control. While subject control constructions engage in a discontinuous dependency where the object NP (the distractor) is intervening, object control dependencies engage in an adjacent dependency, where the subject NP (the distractor) precedes the dependency. Those conditions in which the two NPs (controller and distractor) have the same gender are respectively taken as grammatical and ungrammatical baselines for both subject and object control sentences. Hence, the critical conditions are those in which only one of the NPs agrees in gender with the adjective (i.e. grammatical sentences with a matching distractor and ungrammatical sentences with a mismatching distractor). Humans’ and LMs’ behavior in these conditions will be essential to ascertain whether they can accurately implement controldetermined antecedent retrieval processes and whether they are fallible to interference effects from gender matching but structurally irrelevant antecedents, in a similar vein as the attraction effects observed in agreement dependencies (e.g. Bock and Miller, 1991). While there are very few genderambiguous names in Spanish, in order to maximize gender transparency, the nouns used to create the materials were carefully selected according to the most frequent femaleonly and maleonly names on the official Spanish census. In addition, we created an adaptation of the main dataset substituting proper nouns with personal pronouns (e.g. ‘She promised him to be tidier’), to avoid potential bias, ambiguities or misrepresentations of proper nouns (Shwartz et al., 2020). Both versions of the dataset (with nouns and with pronouns) were translated into Galician by a native speaker linguist, to put Galician LMs to the test and to check if our findings heldcrosslinguistically. These materials were adapted for the LM prediction task (see section 6.1). 3.2 Pretrained models We evaluate the following pretrained models using HuggingFace’s transformers library (Wolf et al., 2020): Multilingual : mBERT (12 layers) (Devlin et al., 2019), and XLMRoBERTa base and large (12 and 24 layers) (Conneau et al., 2020). Spanish : BETO (12 layers) (Cañete et al., 2020), and RoBERTa base and large (12 and 24 layers) (Gutiérrez Fandiño et al., 2022). Galician : Bertinho small and base (6 and 12 layers) (Vilares et al., 2021), and BERT small and base (6 and 12 layers) (Garcia, 2021). 4 Experiment 1: human acceptability The primary goal of this acceptability task is to determine whether native speakers of Spanish are able to detect agreement violations that do not conform with the control properties of main predicates. This is, to our knowledge, the first experimental investigation on control of its kind, and we believe it is essential to corroborate native speakers’ offline sensitivity to the different control manipulations that will be then put to the test with artificial LMs. It will be of particular importance to elucidate whether comprehenders are able to correctly distinguish the acceptability of the strings regardless of the type of control (subject or object) and the presence of a gender matching or mismatching distractor.2064.1 Participants and procedure 40 native speakers of Spanish recruited at the Universidade de Santiago de Compostela participated in this experiment. Their participation was voluntary and all of them provided informed consent. Participants were presented with the entire sentence in the middle of the screen along with a rating scale, and they could only move to the next one once they had emitted a rating. They were instructed to rate the sentences in terms of whether they came across as wellformed Spanish: 7 meaning totally acceptable and 1 totally unacceptable. Experimental sentences were intermixed with 96 filler sentences of similar structure and complexity. The task was completed by all participants in less than 30 minutes. Figure 1: Mean ratings in Experiment 1 . Error bars indicate standard error of the mean. 4.2 Results The average rating for each condition is shown in Figure 1. For this and the following experiments, we carried out a statistical analysis of variance in order to observe differences among the experimental conditions. For the sake of clarity and space, the most relevant significant differences will be marked with an asterisk in the figures. The statistical analyses revealed a significant main effect of grammaticality, such that grammatical sentences (green bars) received much higher ratings than ungrammatical ones (red bars). Importantly, there was a significant interaction between the factors grammaticality and distractor. Planned comparisons showed thatthis interaction was driven by a significant effect of distractor only in ungrammatical sentences. This is shown in significant higher ratings for the distractor match condition in ungrammatical sentences compared to distractor mismatch ones. Such an effect is not present in grammatical sentences. Critically, no differences were observed between subject and object control conditions. In addition, we took the 17 ratings produced by humans and converted them into a binary accuracy measure by classifying their answers as correct or incorrect depending on the grammaticality of the sentence and whether the rating issued was above or below the sample mean (3.79). As expected, accuracy was above 85% for all conditions. This value will allow us to have a more direct comparison with the results from Experiment 3. 4.3 Discussion The results from this experiment clearly show that native speakers are able to detect agreement violations that arise when the adjective did not match in gender the appropriate antecedent, and hence, that they are able to correctly use control information to retrieve the antecedent. This finding also provides a confirmation that the items display unequivocal control readings. Crucially, subject and object control sentences were rated similarly across all four conditions. In addition to the clear contrast between grammatical and ungrammatical conditions, an important result from this experiment is that there is evidence for interference effects in ungrammatical sentences. That is, ungrammatical sentences with a matching distractor received slightly higher ratings than ungrammatical sentences with a mismatching distractor. This effect shows that the presence of a matching distractor leads them to accept ungrammatical sentences more often than when the distractor does not match in gender with the adjective. Crucially, this effect appeared equally in subject and object control conditions, that is, independently of the position of the distractor. This represents evidence for a facilitatory interference effect, or an illusion of grammaticality (Phillips et al., 2011), a pattern akin to the widely attested agreement attraction effect (Wagers et al., 2009). 5 Experiment 2: LM acceptability This experiment aims at observing whether the probabilities of the language models are similar207to those of humans. That is, whether LMs assign lower surprisal to grammatical than to ungrammatical sentences regardless of the presence of a matching or mismatching distractor. For this purpose, we use the exact same dataset as in Experiment 1.2 5.1 Procedure The minicons library (Misra, 2022) was used to compute the surprisal assigned by the LM to the embedded adjectives, which function as a proxy for antecedent retrieval. 5.2 Results The Spanish models’ results for the different experimental conditions are shown in Figure 2. It should be noted that, for ease of interpretation and comparison with Experiment 1 (Figure 1), the surprisal values were inverted such that higher values mean less surprisal (hence more acceptability) while lower values mean more surprisal (hence less acceptability).3While we observe significant effects of grammaticality for all models (meaning that, overall, grammatical sentences were more acceptable than ungrammatical ones), the results show a very different pattern of contrasts for subject and object control sentences. On the one hand, in subject control sentences, all the models showed higher acceptance for grammatical sentences with a matching distractor (dark green bars) than for grammatical sentences with a mismatching distractor (light green bars). Furthermore, also in subject control sentences, ungrammatical sentences with a matching distractor (light red bars) received unexpectedly high acceptance levels, which, for most models, are higher than those observed for grammatical sentences with a mismatching distractor (light green bars). On the other hand, in object control sentences, the pattern of contrasts is very different. First, none of the models exhibited differences among the grammatical conditions regardless of the gender of the distractor. Second, while for all the models, the values observed for ungrammatical sentences with a matching distractor (light red bars) were higher than those for ungrammatical sentences with a mismatching distractor (dark red 2It should be noted that comparing these two dependent measurements (human likertscale acceptability judgements and LM’s surprisal values) is not an optimal contrast, but in our view, conceptually reasonable, as similar LM model measurements are often taken as a proxy for acceptability (e.g. Futrell et al., 2019). 3This was done by subtracting each mean value from the highest mean value observed.bars), this difference was only statistically significant for some models. The same pattern of results is observed using pronouns instead of names (see Figure 4) and for the Galician models using names and pronouns (Figures 5 and 6). This is also corroborated by the very strong correlations ( ρ > 0.9) observed for the adjective surprisal values using names and pronouns, in both languages. Furthermore, we calculated the Spearman ρcorrelations between the acceptability values provided by the humans and the models’ surprisal values. Overall, they revealed weak to moderate correlations, while higher correlations are found for object control sentences than for subject control ones. The correlations for each model at each experimental condition can be found in Table 4.4 5.3 Discussion The results for Experiment 2 show that, unlike humans, all the LMs evaluated behave very differently for subject than for object control dependencies, being better at detecting the acceptability of the strings in object control conditions. The key question here is whether they are able to do so by leveraging the lexicosemantic information of control in order to find the correct antecedent. The pattern of results obtained suggests that, rather than control information, the relevant cue being used is linear proximity. It must be reminded that, in subject control dependencies, the correct antecedent is the NP that is further away from the adjective, while the distractor NP is closer to it. The presence of significant differences between the two grammatical conditions, and the two ungrammatical conditions, found for subject control dependencies, points to the fact that LMs are taking the closer (and wrong) NP, the object, as the antecedent. This explains why the acceptability is reduced for grammatical sentences with a mismatching distractor, despite being perfectly grammatical, and that it is dramatically increased for ungrammatical sentences with a matching distractor, despite being ungrammatical. Reliance on linear proximity also explains why LMs are better, and more akin to humans, on object control dependencies. In these structures, the correct antecedent (i.e. the object) coincides with the linearly closest NP. Interestingly, nonetheless, LMs also exhibit evidence for interference effects from 4This table also includes correlations for wholesentence surprisal measurements.208Figure 2: Spanish LMs acceptability predictions for the adjective in Experiment 2 using names . Error bars indicate standard error of the mean. controlirrelevant but gendermatching distractors. This is perhaps clearer in the case of object control sentences, since linear proximity and interference converge in the case of subject control ungrammatical sentences with a matching distractor. In object control sentences, some models also exhibit higher acceptability for ungrammatical sentences with a matching distractor even when in this case the gendermatching NP is the farthest NP. These issues are further explored in Experiment 3. 6 Experiment 3: LM masked prediction This experiment aims at further exploring the behavior of LMs using the masked prediction task. In contrast with Experiment 2, where we compute the surprisal for the same adjective in a given (grammatical or ungrammatical) sentence, our objective here is to test whether LMs predict grammatically compatible adjectives in subject and object control sentences regardless of the presence of a matching or mismatching distractor. In Experiment 2 the adjective’s gender was kept constant across experimental conditions, and hence, we could not assess LMs’ preferences for the masculine or feminine version. By contrast, here we test if LMs predict grammatically compatible adjectives in subject and object control sentences by directly comparing the probabilities of a given adjective in its masculine or feminine form, something which provides uswith more comprehensive information in this respect. Furthermore, evaluating model accuracy rather than surprisal values will also allow us to assess and compare the performance across models. 6.1 Experimental materials The experimental materials used for Experiment 3 are an adaptation of the dataset described in section 3.1 (including its variants with personal pronouns and Galician translations) so that they could be used in the masked prediction task. This allows us to evaluate our dataset in the two possible gender configurations, expanding it such that each sentence has two possible outcomes: a grammatical and an ungrammatical one. Therefore, the manipulation is a 2x2 factorial design ( control x distractor ), as shown in Table 2 6.2 Procedure We rely on the standard approach for targeted syntactic evaluation to obtain the accuracy of the models on the minimal pairs (Linzen et al., 2016; Warstadt et al., 2020). For each sentence, we extract the probabilities of the grammatical and ungrammatical target adjectives, and consider a trial as correct if the model gives a higher probability to the grammatical target adjective. It is worth noting that this method requires compatible tokenization between both variants (grammatical and ungrammatical). To make a fair evaluation,209Subject control Dist. match Maríafle prometió a Carmen fser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Maríafle prometió a Manuelmser más [ordenadaf|*ordenadom]con los apuntes. Object control Dist. match Maríafle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Josémle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Table 2: Sample set of the experimental materials for Experiment 3. The correct antecedents and correct and incorrect the target adjectives are bold typed. See Table 1 for comparison with the original dataset. we check if both variants appear as single tokens in the models’ vocabulary, or whether their last subtokens (the ones that carry the morphosyntactic information) are comparable so that we can use their probabilities. For instance, the Spanish pair afectuoso|afectuosa (‘affectionate’) is tokenized by RoBERTa as afect+uoso|uosa , and hence, we can use the last subtokens for comparison. However, desconfiado|desconfiada (‘skeptical’) is divided as desconf+iado anddescon+fi+ada . We discard these incompatible cases (19% of the items for Spanish, and 16% for Galician, on average).5 6.3 Results Table 3 displays the global accuracy for all the models under evaluation in Experiment 3 (global accuracy values for all the datasets tested in Spanish and Galician are in Table 5). RoBERTa large emerges as the best performing model, closely followed by XLM RoBERTa large, while mBERT base emerges as the worst performing model. Nonetheless, in order to analyze the impact of linear proximity on model performance, it is essential to examine the factors control anddistractor separately. Figure 3 shows the accuracy per condition for the target adjectives. Statistical analyses show a main effect of distractor, such that the accuracy was higher for distractor match sentences (dark green bars, when the two NPs had the same gender) than for distractor mismatch ones (light green bars, when the NPs differed in gender). However, this difference was much more acute for subject control sentences, where significant differences arise for all the models, than for object control sentences, where significant differences are only found for 5We also assessed the models’ performance by computing the probability mass that models put on the feminine and masculine inflections rather than on a particular adjective pair, inspired by Newman et al. (2021). We used morphological lexicons to obtain the masculine and feminine probabilities from the top N adjectives predicted by the models in the masked position (N=100). The results for top N (to be found in Appendix C) followed the same pattern as for target adjectives.RoBERTalarge and XLMRoBERTa-base. The same pattern of results is observed using pronouns instead of names (see Figure 7) and for the Galician models using names and pronouns (Figures 8 and 9). This is also shown in the very strong correlations ( ρ > 0.8) observed for the results using names and pronouns in both languages. Model Accuracy BETO base 0.78 RoBERTa base 0.77 RoBERTa large 0.83 mBERT base 0.61 XLM RoBERTa base 0.78 XLM RoBERTa large 0.82 Table 3: Global accuracy in Experiment 3. 6.4 Discussion The results from Experiment 3 reinforce and complement the findings from Experiment 2 in several respects. First, reliance on linear proximity is, if anything, even clearer, as subject control sentences with a mismatching distractor display clear interference effects, which are materialized in a dramatically belowchance accuracy. These are the cases in which the distractor is the sentence object, which is also the closer NP. In these cases, LMs’ predict a target adjective that agrees in gender with the object, rather than the subject (i.e. the correct antecedent) and hence, demonstrating that antecedent retrieval processes unfold disregarding the lexicosemantic information on control. Importantly, these effects are almost absent in object control sentences, where only two models show evidence for interference effects, these being much less pronounced (only a few accuracy points). Even though the results from this experiment cannot be directly compared with those of humans (Experiment 1), it should be noted that human accuracy was above 80% for all conditions.210Figure 3: Spanish LMs accuracies for the target adjective in Experiment 3 using names . 7 General discussion and conclusions The empirical evidence gathered in this work provides a very straightforward picture: whereas humans’ can coordinate lexicosemantic and syntactic information in order to determine the (un)acceptability of control structures, LMs resort to a heuristic based on linear proximity, disregarding control information. These findings are robust, as they replicate across tasks (acceptability and masked prediction), models (monolingual and multilingual), languages (Spanish and Galician LMs), and type of antecedent (names and pronouns). Furthermore, they go in line with evidence advanced in Lee and Schuster (2022) for English with respect to autoregressive language models. These findings contrast with those obtained for superficially similar dependencies like subjectverb agreement, in which these models have been attested to display accurate levels of performance for Spanish and Galician (PérezMayos et al., 2021; de DiosFlores and Garcia, 2022; Garcia and CrespoOtero, 2022). Crucially, however, agreement and control dependencies engage different types of linguistic information. While the former rely on coocurring patterns containing overt morphological cues which are pervasive in the training data, control dependencies rely on analysis of variance in order to observe differences among the experimental conditions. For the sake of clarity and space, the most relevant significant differences will be marked with an asterisk in the figures. The statistical analyses revealed a significant main effect of grammaticality, such that grammatical sentences (green bars) received much higher ratings than ungrammatical ones (red bars). Importantly, there was a significant interaction between the factors grammaticality and distractor. Planned comparisons showed thatthis interaction was driven by a significant effect of distractor only in ungrammatical sentences. This is shown in significant higher ratings for the distractor match condition in ungrammatical sentences compared to distractor mismatch ones. Such an effect is not present in grammatical sentences. Critically, no differences were observed between subject and object control conditions. In addition, we took the 17 ratings produced by humans and converted them into a binary accuracy measure by classifying their answers as correct or incorrect depending on the grammaticality of the sentence and whether the rating issued was above or below the sample mean (3.79). As expected, accuracy was above 85% for all conditions. This value will allow us to have a more direct comparison with the results from Experiment 3. 4.3 Discussion The results from this experiment clearly show that native speakers are able to detect agreement violations that arise when the adjective did not match in gender the appropriate antecedent, and hence, that they are able to correctly use control information to retrieve the antecedent. This finding also provides a confirmation that the items display unequivocal control readings. Crucially, subject and object control sentences were rated similarly across all four conditions. In addition to the clear contrast between grammatical and ungrammatical conditions, an important result from this experiment is that there is evidence for interference effects in ungrammatical sentences. That is, ungrammatical sentences with a matching distractor received slightly higher ratings than ungrammatical sentences with a mismatching distractor. This effect shows that the presence of a matching distractor leads them to accept ungrammatical sentences more often than when the distractor does not match in gender with the adjective. Crucially, this effect appeared equally in subject and object control conditions, that is, independently of the position of the distractor. This represents evidence for a facilitatory interference effect, or an illusion of grammaticality (Phillips et al., 2011), a pattern akin to the widely attested agreement attraction effect (Wagers et al., 2009). 5 Experiment 2: LM acceptability This experiment aims at observing whether the probabilities of the language models are similar207to those of humans. That is, whether LMs assign lower surprisal to grammatical than to ungrammatical sentences regardless of the presence of a matching or mismatching distractor. For this purpose, we use the exact same dataset as in Experiment 1.2 5.1 Procedure The minicons library (Misra, 2022) was used to compute the surprisal assigned by the LM to the embedded adjectives, which function as a proxy for antecedent retrieval. 5.2 Results The Spanish models’ results for the different experimental conditions are shown in Figure 2. It should be noted that, for ease of interpretation and comparison with Experiment 1 (Figure 1), the surprisal values were inverted such that higher values mean less surprisal (hence more acceptability) while lower values mean more surprisal (hence less acceptability).3While we observe significant effects of grammaticality for all models (meaning that, overall, grammatical sentences were more acceptable than ungrammatical ones), the results show a very different pattern of contrasts for subject and object control sentences. On the one hand, in subject control sentences, all the models showed higher acceptance for grammatical sentences with a matching distractor (dark green bars) than for grammatical sentences with a mismatching distractor (light green bars). Furthermore, also in subject control sentences, ungrammatical sentences with a matching distractor (light red bars) received unexpectedly high acceptance levels, which, for most models, are higher than those observed for grammatical sentences with a mismatching distractor (light green bars). On the other hand, in object control sentences, the pattern of contrasts is very different. First, none of the models exhibited differences among the grammatical conditions regardless of the gender of the distractor. Second, while for all the models, the values observed for ungrammatical sentences with a matching distractor (light red bars) were higher than those for ungrammatical sentences with a mismatching distractor (dark red 2It should be noted that comparing these two dependent measurements (human likertscale acceptability judgements and LM’s surprisal values) is not an optimal contrast, but in our view, conceptually reasonable, as similar LM model measurements are often taken as a proxy for acceptability (e.g. Futrell et al., 2019). 3This was done by subtracting each mean value from the highest mean value observed.bars), this difference was only statistically significant for some models. The same pattern of results is observed using pronouns instead of names (see Figure 4) and for the Galician models using names and pronouns (Figures 5 and 6). This is also corroborated by the very strong correlations ( ρ > 0.9) observed for the adjective surprisal values using names and pronouns, in both languages. Furthermore, we calculated the Spearman ρcorrelations between the acceptability values provided by the humans and the models’ surprisal values. Overall, they revealed weak to moderate correlations, while higher correlations are found for object control sentences than for subject control ones. The correlations for each model at each experimental condition can be found in Table 4.4 5.3 Discussion The results for Experiment 2 show that, unlike humans, all the LMs evaluated behave very differently for subject than for object control dependencies, being better at detecting the acceptability of the strings in object control conditions. The key question here is whether they are able to do so by leveraging the lexicosemantic information of control in order to find the correct antecedent. The pattern of results obtained suggests that, rather than control information, the relevant cue being used is linear proximity. It must be reminded that, in subject control dependencies, the correct antecedent is the NP that is further away from the adjective, while the distractor NP is closer to it. The presence of significant differences between the two grammatical conditions, and the two ungrammatical conditions, found for subject control dependencies, points to the fact that LMs are taking the closer (and wrong) NP, the object, as the antecedent. This explains why the acceptability is reduced for grammatical sentences with a mismatching distractor, despite being perfectly grammatical, and that it is dramatically increased for ungrammatical sentences with a matching distractor, despite being ungrammatical. Reliance on linear proximity also explains why LMs are better, and more akin to humans, on object control dependencies. In these structures, the correct antecedent (i.e. the object) coincides with the linearly closest NP. Interestingly, nonetheless, LMs also exhibit evidence for interference effects from 4This table also includes correlations for wholesentence surprisal measurements.208Figure 2: Spanish LMs acceptability predictions for the adjective in Experiment 2 using names . Error bars indicate standard error of the mean. controlirrelevant but gendermatching distractors. This is perhaps clearer in the case of object control sentences, since linear proximity and interference converge in the case of subject control ungrammatical sentences with a matching distractor. In object control sentences, some models also exhibit higher acceptability for ungrammatical sentences with a matching distractor even when in this case the gendermatching NP is the farthest NP. These issues are further explored in Experiment 3. 6 Experiment 3: LM masked prediction This experiment aims at further exploring the behavior of LMs using the masked prediction task. In contrast with Experiment 2, where we compute the surprisal for the same adjective in a given (grammatical or ungrammatical) sentence, our objective here is to test whether LMs predict grammatically compatible adjectives in subject and object control sentences regardless of the presence of a matching or mismatching distractor. In Experiment 2 the adjective’s gender was kept constant across experimental conditions, and hence, we could not assess LMs’ preferences for the masculine or feminine version. By contrast, here we test if LMs predict grammatically compatible adjectives in subject and object control sentences by directly comparing the probabilities of a given adjective in its masculine or feminine form, something which provides uswith more comprehensive information in this respect. Furthermore, evaluating model accuracy rather than surprisal values will also allow us to assess and compare the performance across models. 6.1 Experimental materials The experimental materials used for Experiment 3 are an adaptation of the dataset described in section 3.1 (including its variants with personal pronouns and Galician translations) so that they could be used in the masked prediction task. This allows us to evaluate our dataset in the two possible gender configurations, expanding it such that each sentence has two possible outcomes: a grammatical and an ungrammatical one. Therefore, the manipulation is a 2x2 factorial design ( control x distractor ), as shown in Table 2 6.2 Procedure We rely on the standard approach for targeted syntactic evaluation to obtain the accuracy of the models on the minimal pairs (Linzen et al., 2016; Warstadt et al., 2020). For each sentence, we extract the probabilities of the grammatical and ungrammatical target adjectives, and consider a trial as correct if the model gives a higher probability to the grammatical target adjective. It is worth noting that this method requires compatible tokenization between both variants (grammatical and ungrammatical). To make a fair evaluation,209Subject control Dist. match Maríafle prometió a Carmen fser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Maríafle prometió a Manuelmser más [ordenadaf|*ordenadom]con los apuntes. Object control Dist. match Maríafle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Dist. mismatch Josémle ordenó a Carmenfser más [ordenadaf|*ordenadom]con los apuntes. Table 2: Sample set of the experimental materials for Experiment 3. The correct antecedents and correct and incorrect the target adjectives are bold typed. See Table 1 for comparison with the original dataset. we check if both variants appear as single tokens in the models’ vocabulary, or whether their last subtokens (the ones that carry the morphosyntactic information) are comparable so that we can use their probabilities. For instance, the Spanish pair afectuoso|afectuosa (‘affectionate’) is tokenized by RoBERTa as afect+uoso|uosa , and hence, we can use the last subtokens for comparison. However, desconfiado|desconfiada (‘skeptical’) is divided as desconf+iado anddescon+fi+ada . We discard these incompatible cases (19% of the items for Spanish, and 16% for Galician, on average).5 6.3 Results Table 3 displays the global accuracy for all the models under evaluation in Experiment 3 (global accuracy values for all the datasets tested in Spanish and Galician are in Table 5). RoBERTa large emerges as the best performing model, closely followed by XLM RoBERTa large, while mBERT base emerges as the worst performing model. Nonetheless, in order to analyze the impact of linear proximity on model performance, it is essential to examine the factors control anddistractor separately. Figure 3 shows the accuracy per condition for the target adjectives. Statistical analyses show a main effect of distractor, such that the accuracy was higher for distractor match sentences (dark green bars, when the two NPs had the same gender) than for distractor mismatch ones (light green bars, when the NPs differed in gender). However, this difference was much more acute for subject control sentences, where significant differences arise for all the models, than for object control sentences, where significant differences are only found for 5We also assessed the models’ performance by computing the probability mass that models put on the feminine and masculine inflections rather than on a particular adjective pair, inspired by Newman et al. (2021). We used morphological lexicons to obtain the masculine and feminine probabilities from the top N adjectives predicted by the models in the masked position (N=100). The results for top N (to be found in Appendix C) followed the same pattern as for target adjectives.RoBERTalarge and XLMRoBERTa-base. The same pattern of results is observed using pronouns instead of names (see Figure 7) and for the Galician models using names and pronouns (Figures 8 and 9). This is also shown in the very strong correlations ( ρ > 0.8) observed for the results using names and pronouns in both languages. Model Accuracy BETO base 0.78 RoBERTa base 0.77 RoBERTa large 0.83 mBERT base 0.61 XLM RoBERTa base 0.78 XLM RoBERTa large 0.82 Table 3: Global accuracy in Experiment 3. 6.4 Discussion The results from Experiment 3 reinforce and complement the findings from Experiment 2 in several respects. First, reliance on linear proximity is, if anything, even clearer, as subject control sentences with a mismatching distractor display clear interference effects, which are materialized in a dramatically belowchance accuracy. These are the cases in which the distractor is the sentence object, which is also the closer NP. In these cases, LMs’ predict a target adjective that agrees in gender with the object, rather than the subject (i.e. the correct antecedent) and hence, demonstrating that antecedent retrieval processes unfold disregarding the lexicosemantic information on control. Importantly, these effects are almost absent in object control sentences, where only two models show evidence for interference effects, these being much less pronounced (only a few accuracy points). Even though the results from this experiment cannot be directly compared with those of humans (Experiment 1), it should be noted that human accuracy was above 80% for all conditions.210Figure 3: Spanish LMs accuracies for the target adjective in Experiment 3 using names . 7 General discussion and conclusions The empirical evidence gathered in this work provides a very straightforward picture: whereas humans’ can coordinate lexicosemantic and syntactic information in order to determine the (un)acceptability of control structures, LMs resort to a heuristic based on linear proximity, disregarding control information. These findings are robust, as they replicate across tasks (acceptability and masked prediction), models (monolingual and multilingual), languages (Spanish and Galician LMs), and type of antecedent (names and pronouns). Furthermore, they go in line with evidence advanced in Lee and Schuster (2022) for English with respect to autoregressive language models. These findings contrast with those obtained for superficially similar dependencies like subjectverb agreement, in which these models have been attested to display accurate levels of performance for Spanish and Galician (PérezMayos et al., 2021; de DiosFlores and Garcia, 2022; Garcia and CrespoOtero, 2022). Crucially, however, agreement and control dependencies engage different types of linguistic information. While the former rely on coocurring patterns containing overt morphological cues which are pervasive in the training data, control dependencies rely on Experiments on the storytelling and multiparagraph opinionated article writing tasks show that pretrained MLMs can achieve more than 3× → 13×speedup with better performance than strong AR models. Our code is available at GitHub *. 1 Introduction Pretrained language models (PLMs) like BART (Lewis et al., 2020) and GPTs (Radford et al.; Radford et al.; Brown et al., 2020) have achieved remarkable progress in OpenLTG. Through modeling languages from left to right, they can autoregressively “create” fluent and grammatical content. With the further enhancement of planning strategies (Hua and Wang, 2020; Hu et al., 2022) or highlevel representation learning (Guan ∗Equal Contribution †Corresponding Author *https://github .com/dropreg/OpenLTGMLMModel Type Iter Tokens/s BART base AR - 151.3 BART base + Planning † AR - 5.8 BERTCRF † NAR 0 2,597.4 RoBERTa base NAR 0 1,561.2 1 1,068.9 4 505.2 Table 1: Inference speed of each model with a single GPU ( NVIDIA A100 40GB). For a fair comparison, we force all models to generate 200 tokens. The models labeled with †are implemented with the Hugging Face platform, while the rest are implemented with Fairseq. et al., 2021a), pretrained AR language models can achieve promising OpenLTG. However, the low inference efficiency of AR impedes their usability in realworld applications. Table 1 presents the inference speed of a few typical AR language models. We can see that BART (Lewis et al., 2020) requires at least 1.3 seconds to generate a story with 200 tokens on the powerful NVIDIA A100 GPU, and extra planning (Hua and Wang, 2020) can make the inference process even slower (more than 30 seconds to create a 200tokens story). In great contrast with AR models, NAR models (e.g., BERT -CRF (Su et al., 2021)) can generate more than 12 stories with the same length within one second, but their effectiveness in openended long text generation has not been proven yet. The high inference efficiency of NAR models is at the sacrifice of output dependency modeling, in which each generation is executed in parallel (Xiao et al., 2022). Thus, NAR models are mainly explored and utilized for text generation tasks with adequate input information to predict each output token of different positions and extra correlations to constrain the generation process, e.g., neural machine translation (Gu et al., 2018; Huang et al., 2022), summarization (Qi et al., 2021; Agrawal and Carpuat, 2022), sentence compression (Su et al., 2021), dialogue generation (Zou et al., 2021), and constrained storyending generation (Yang et al.,2232021). To the best of our knowledge, none of the existing research explores OpenLTG with NAR models, particularly based on pretrained MLMs. We fill this gap by first conducting a preliminary study to calibrate the potential and limitations of a pretrained MLM, i.e., RoBERTa (Liu et al., 2019)†, on two story generation corpora, i.e., ROCStories (ROC) (Mostafazadeh et al., 2016) and WritingPrompts (WP) (Fan et al., 2018). To achieve conditional generation, we simply use RoBERTa as both the encoder and the decoder with mixed attention (He et al., 2018) to achieve encoderdecoder crossattention. Through experiments, we found that: (1) pretrained MLMs can achieve competitive performance in the iterative NAR fashion for openended short text generation (e.g., a paragraph with around 40 tokens), (2) pretrained MLMs fail to model OpenLTG (with about 140 tokens on average), which will generate uninformative content with highfrequency and repeated tokens (e.g., “.” and “,”). Furthermore, we offer three possible reasons for the attention mechanism of MLMs and inference strategy to explain the collapse of the iterative NAR model based on pretrained MLMs for the OpenLTG scenario. Inspired by the above observations, we introduce two improvement strategies: Dynamic Sliding Window Attention (DSWA) and linear temperature decay strategy (LTD) to maintain more informative context content in the iterative NAR generation. As a result, iterative NAR models based on pretrained MLMs can achieve much longer text generation than the vanilla setting. Experiments on two OpenLTG tasks (i.e., storytelling and multiparagraph opinionated article writing) with four widelyused datasets demonstrate that the pretrained MLM can achieve better performance (BLEU score, ROUGE score, BERT score, and Perplexity) than multiple strong AR models without extra posttraining, structure modification, or using more model parameters. Importantly, our approach can speed up the inference process due to nonautoregressive properties, making the pretrained MLM as a promising candidate for the OpenLTG community. The RoBERTa base achieves more than 3× → 13× with better performance to the competitive BART. †MLMs can achieve iterative NAR generation with the maskpredict inference strategy (Ghazvininejad et al., 2019).2 Related Work Long Text Generation Text generation tasks can be classified into two categories: directed generation and openend generation. The directed generation (Sutskever et al., 2014; Li et al., 2015; Vaswani et al., 2017) for long text scenarios has long source than the target, which is also constrained by source sequence, e.g., neural machine translation and summarization. These tasks aim to solve the quadratic growth requirement of the memory and computational of the selfattention mechanism. The openended generation task (Guo et al., 2018; Tan et al., 2020; GoldfarbTarrant et al., 2020; Hua and Wang, 2020; Orbach and Goldberg, 2020; Hu et al., 2022) desire to generate more freedom content and has recently become a promising research direction. Previous works have explored multiple generation strategies to generate highquality and fluent text, e.g., planning then generating (Guo et al., 2018; Tan et al., 2020; GoldfarbTarrant et al., 2020; Hua and Wang, 2020; Orbach and Goldberg, 2020; Hu et al., 2022) and introducing external knowledge (Guan et al., 2020; Xu et al., 2020). Although the above strategies enable the model to achieve significant advances, timeconsuming is still a critical issue that hinders their usage in realworld applications (Guan et al., 2021a; Tan et al., 2020). Iterative Nonautoregressive Generation Nonautoregressive (NAR) model breaks the sequential dependencies from front to back for parallel text generation (Gu et al., 2018; Guo et al., 2020; Saharia et al., 2020). Furthermore, the iterativebased NAR model (Lee et al., 2018; Gu et al., 2019; Chi et al., 2021) can achieve comparable performance with the AR model. The typical CMLM model (Ghazvininejad et al., 2019) can generate fluent results conditioned on the predictions from the previous iteration instead of previous tokens: P(Yt|X) =P(Yt|Yt−1, X) (1) Benefiting from this, the iterative NAR model is more flexibly compared with the AR model, which can easily generate consistent and controllable text for each iteration step. To the best of our knowledge, the iterative NAR model has never been used to solve openended generation. Especially, we investigate its usability for the long text scenario, i.e., target lengths between 100 and 400, which is still underexplored in the directed generation tasks.224Figure 1: The overview of MLM for text generation. (We concatenate the hidden states of XandYas the key and value of the mixedattention mechanism.) 3 Preliminary Study We first present the training and inference paradigm of utilizing the pretrained MLMs for OpenLTG (§ 3.1), e.g., BERT or RoBERTa. Then, we study the significant collapse problem in a long text generation scenario by conducting preliminary experiments on two datasets with different target lengths (§ 3.2). Finally, we investigate the reason for the above issues with an exhaustive case study and exploration tests to motivate our method design (§ 3.3), where the model can generate text in nonautoregressive manner to speed up the inference. 3.1 Text Generation via Pretrained MLMs Pretrained MLMs are typically used as the encoder to extract the representations of sentences instead of generating texts. Previous works (Dong et al., 2019; Wang et al., 2019) have indicated that the MLM encoder can support text generation tasks via attention masks or Gibbs sampling. In contrast, we introduce mixed attention and parameter sharing to the encoderbased model to solve the sequence to sequence tasks, as shown in Figure 1. Model Training Given the parallel text generation dataset D={(X,Y)}|D|, we can feed the source Xinto the MLM encoder to obtain the representation Hl srcoflth layer. Concretely, each layer comprises two sublayers, including one selfattention layer and one feedforward layer: ¯Hl src=SelfATTN (Hl−1 src) +Hl−1 src Hl src=FFN(¯Hl src) +¯Hl src.(2) Then, we random mask Y={y1, y2,···, y|Y|}to obtain corrupted target YM={y1, m2,···, m|Y|} (mis the symbol of mask token “<mask>”). As before, we can obtain the representation Hl tgtby usingthe shared parameter MLM encoder and then try to recover the masked sequence, where the mixedattention mechanism (He et al., 2018) is applied to aggregate the source HL srcand the target Hl tgt: ¯Hl tgt=MixedATTN (Hl−1 tgt,HL src) +Hl−1 tgt Hl tgt=FFN(¯Hl tgt) +¯Hl tgt.(3) Mixedattention does not break the original attention mechanism, which only utilizes the target hidden states as query vector and the concatenated vector of source and target hidden states as key and value. It is worth noting that this approach is available for transformer encoder models without additional parameters. Specifically, we uniformly mask 1ton(target length) tokens from Yfor model training. The training objective is thus to minimize the conditional MLM loss like the pretraining stage: LMLM =−M/summationdisplay i=1logP(yi|X,YM) P(yj|X,YM) =exp(utgt/T)/summationtext |u′ tgt|exp(u′ tgt/T),(4) where Mis the number of masked tokens, utgt is the output logit, and Tis the temperature to reestimate the final probability. Model Inference We use an iterative refinement strategy to generate text like CMLM (Ghazvininejad et al., 2019). In particular, We use the fully masked sequence {m1, m2,···, mn}to initialize the target sequence and predict all masked tokens at the first step. Then, we iteratively regenerate the lowconfidence tokens at the subsequent iteration steps to obtain better performance. For OpenLTG, we utilize the nucleus sampling (Holtzman et al., 2019) decoding strategy instead of beam search. Length Prediction It is necessary to obtain the target length to initialize the full mask sequence as model input before inference. Specifically, we provide two strategies: 1) Fixed Length, which initializes the target length according to the average length of the validation set or human experience. 2) Prediction Module, which uses the meanpooling layer followed by one classification layer to predict the target length by feeding HL srcinto them: P(Ltgt|X) =Softmax (WL(MeanPooling (HL src))) , (5) where Ltgtis the target length, and WLis the learnable parameter. Specifically, we will adjust Ltgt according to the specific offset, which is the parameter based on the validation dataset.225Figure 2: The iterative inference process of typical good and bad cases, randomly sampled from ROC and WP. The histogram refers to the output distributions (Iter=1) across candidate tokens for a randomly picked position. Data Model B1 B2 R1 R2 Dist Rep ROCBART 30.06 14.37 22.37 2.42 3.93 79.07 RoBERTa 30.89 14.36 25.01 3.48 5.24 73.42 WPBART 29.69 10.26 24.34 2.20 0.47 90.15 RoBERTa 15.80 5.21 10.08 0.84 8.48 17.08 Table 2: The performance on WP and ROC. 3.2 Extensive Trials Study Settings We use Writing Prompt (WP) and ROC Stories (ROC) datasets to conduct experiments for validating whether pretrained MLMs can work better on OpenLTG tasks. In particular, these two datasets have different lengths for target sentences, i.e., the average length of WP is 140 and ROC is 40, and more details are given in Section 5 and Appendix A. We choose RoBERTa base (Liu et al., 2019) as our backbone model and use BLEU, ROUGE, Distinct, and Lexical Repetition metrics for evaluation. During inference, we set nucleus sampling hyperparameter topp=0.9, temperature T=1.0, and limit the maximum iteration steps to 6 for ROC and 8for WP. Results As shown in Table 2, For the ROC dataset, the RoBERTa base model obtains comparable performance with BART. However, the generation quality significantly decreases for the WP dataset, which involves much longer targets. Specifically, most of the generated results are made up of duplicated function words or punctuations, e.g., “it”,“to”,“the” , and “.”, etc, which makes themodel outputs unreadable and meaningless. One intuitive question is What causes the collapse problem in OpenLTG when using pretrained MLMs? 3.3 Analysis and Possible Improvements We show typical good case andbad case in Figure 2, which are randomly selected from the ROC and WP datasets respectively to demonstrate the generation process. For each iterative refinement step of bad case , the informative tokens will be replaced by the placeholder token “ <mask> ” and are replaced by the function words at the subsequent steps. Thus it is unable to generate fluent results likegood case . According to this observation, we try to provide some possible explanations for the aforementioned collapse issues: 1)The most intuitive reason is that the function words are often located at the front of the output distribution, which dominates the high probability region, causing the informative tokens hard to be sampled. . The output distribution trained with the ROC dataset contains more promptrelated tokens than WP, e.g., the “swim” and“water” in the top 50candidates of ROC output, as shown in Figure 2 (distribution histogram). Worse still, the function words dominate the high probability regions (from 35% to 45%) for the bad case and lead to terrible initialization at the first iteration step. 2)The iterative refinement mechanism depends on the token confidence of generated sequences, and it is easier for the lowconfidence but infor226Data Recurrent B1 B2 R1 R2 Dist Rep WP1 15.80 5.21 10.08 0.84 17.08 94.25 2 22.42 8.70 16.81 2.14 34.82 83.87 4 26.91 10.67 21.32 2.81 50.32 35.93 Table 3: The performance of different recurrent steps. mative tokens to be masked . In fact, the iterative refinement mechanism is designed for directed generation tasks, e.g., neural machine translation or summarization, which usually apply the argmax operation to sample results, and the evaluation of confidence is reasonable in different iterations. Nevertheless, we use the nucleus sampling strategy for inference in OpenLTG, which leads to the lowconfidence tokens with high priority being masked. 3)The massive absent context tokens suffer a more serious multimodality problem on long text generation in early iteration steps . As a result, the model is inclined to generate duplicated tokens due to the multimodal output distribution. Although iterative refinement can provide additional context to alleviate this issue, the model still cannot generate the expected results. The possible explanation is that the selfattention layer needs the context token as keyvalue pairs to calculate the token representation. Unfortunately, the massive uninformative mask tokens (“ <mask> ”) in context lead to model collapse steadily worsening in the following iteration steps. Thus, we utilize the recurrent generation mechanism for model training and inference to reduce the context dependency, which can also flexibly control the maximum length of the generated sequence (please refer to the Appendix B for more details about the model architectures and experiments). The results are shown in Table 3. We can observe that the model can gradually improve its performance as the recurrent steps increase, demonstrating that informative context dependency is the implicit reason for the model collapse. Improvements Based on the above analysis and findings, we categorize these critical factors into two types: the defects of attention mechanism andinappropriate inference strategies . In particular, we believe that each token should not pays attention to all context information, and most tokens only need the neighbor tokens’ information to represent the hidden states and predict the results. Therefore, we will change the selfattention mechanism of the pretrained MLMs so that each tokens can attend to the restricted neighbors. Besides, Figure 3: The overview of sliding window attention. we will adjust the confidence score of the output distributions to keep the informative tokens in subsequent iteration steps instead of being masked. 4 Method In this section, we propose two simple yet effective strategies for attention mechanism and inference to mitigate the model collapse problems: Dynamic Sliding Window Attention (DSWA) and Linear Temperature Decay (LTD). These designs do not break the paradigm of MLM so that it can flexibly adapt to the pretrained models. 4.1 Dynamic Sliding Window Attention We first introduced the sliding window mechanism (Beltagy et al., 2020) for the selfattention layer to adjust each token’s attention pattern, which also ensures that the top layer’s token representations can have a large receptive field, similar to CNN (Wu et al., 2018). Figure 3 illustrates the attention mask of the mixed attention layer of pretrained MLMs. It is worth noting that the keyvalue pairs consist of two parts: the source representation of the last layer (with green  evaluation. During inference, we set nucleus sampling hyperparameter topp=0.9, temperature T=1.0, and limit the maximum iteration steps to 6 for ROC and 8for WP. Results As shown in Table 2, For the ROC dataset, the RoBERTa base model obtains comparable performance with BART. However, the generation quality significantly decreases for the WP dataset, which involves much longer targets. Specifically, most of the generated results are made up of duplicated function words or punctuations, e.g., “it”,“to”,“the” , and “.”, etc, which makes themodel outputs unreadable and meaningless. One intuitive question is What causes the collapse problem in OpenLTG when using pretrained MLMs? 3.3 Analysis and Possible Improvements We show typical good case andbad case in Figure 2, which are randomly selected from the ROC and WP datasets respectively to demonstrate the generation process. For each iterative refinement step of bad case , the informative tokens will be replaced by the placeholder token “ <mask> ” and are replaced by the function words at the subsequent steps. Thus it is unable to generate fluent results likegood case . According to this observation, we try to provide some possible explanations for the aforementioned collapse issues: 1)The most intuitive reason is that the function words are often located at the front of the output distribution, which dominates the high probability region, causing the informative tokens hard to be sampled. . The output distribution trained with the ROC dataset contains more promptrelated tokens than WP, e.g., the “swim” and“water” in the top 50candidates of ROC output, as shown in Figure 2 (distribution histogram). Worse still, the function words dominate the high probability regions (from 35% to 45%) for the bad case and lead to terrible initialization at the first iteration step. 2)The iterative refinement mechanism depends on the token confidence of generated sequences, and it is easier for the lowconfidence but infor226Data Recurrent B1 B2 R1 R2 Dist Rep WP1 15.80 5.21 10.08 0.84 17.08 94.25 2 22.42 8.70 16.81 2.14 34.82 83.87 4 26.91 10.67 21.32 2.81 50.32 35.93 Table 3: The performance of different recurrent steps. mative tokens to be masked . In fact, the iterative refinement mechanism is designed for directed generation tasks, e.g., neural machine translation or summarization, which usually apply the argmax operation to sample results, and the evaluation of confidence is reasonable in different iterations. Nevertheless, we use the nucleus sampling strategy for inference in OpenLTG, which leads to the lowconfidence tokens with high priority being masked. 3)The massive absent context tokens suffer a more serious multimodality problem on long text generation in early iteration steps . As a result, the model is inclined to generate duplicated tokens due to the multimodal output distribution. Although iterative refinement can provide additional context to alleviate this issue, the model still cannot generate the expected results. The possible explanation is that the selfattention layer needs the context token as keyvalue pairs to calculate the token representation. Unfortunately, the massive uninformative mask tokens (“ <mask> ”) in context lead to model collapse steadily worsening in the following iteration steps. Thus, we utilize the recurrent generation mechanism for model training and inference to reduce the context dependency, which can also flexibly control the maximum length of the generated sequence (please refer to the Appendix B for more details about the model architectures and experiments). The results are shown in Table 3. We can observe that the model can gradually improve its performance as the recurrent steps increase, demonstrating that informative context dependency is the implicit reason for the model collapse. Improvements Based on the above analysis and findings, we categorize these critical factors into two types: the defects of attention mechanism andinappropriate inference strategies . In particular, we believe that each token should not pays attention to all context information, and most tokens only need the neighbor tokens’ information to represent the hidden states and predict the results. Therefore, we will change the selfattention mechanism of the pretrained MLMs so that each tokens can attend to the restricted neighbors. Besides, Figure 3: The overview of sliding window attention. we will adjust the confidence score of the output distributions to keep the informative tokens in subsequent iteration steps instead of being masked. 4 Method In this section, we propose two simple yet effective strategies for attention mechanism and inference to mitigate the model collapse problems: Dynamic Sliding Window Attention (DSWA) and Linear Temperature Decay (LTD). These designs do not break the paradigm of MLM so that it can flexibly adapt to the pretrained models. 4.1 Dynamic Sliding Window Attention We first introduced the sliding window mechanism (Beltagy et al., 2020) for the selfattention layer to adjust each token’s attention pattern, which also ensures that the top layer’s token representations can have a large receptive field, similar to CNN (Wu et al., 2018). Figure 3 illustrates the attention mask of the mixed attention layer of pretrained MLMs. It is worth noting that the keyvalue pairs consist of two parts: the source representation of the last layer (with green Analysis and Possible Improvements We show typical good case andbad case in Figure 2, which are randomly selected from the ROC and WP datasets respectively to demonstrate the generation process. For each iterative refinement step of bad case , the informative tokens will be replaced by the placeholder token “ <mask> ” and are replaced by the function words at the subsequent steps. Thus it is unable to generate fluent results likegood case . According to this observation, we try to provide some possible explanations for the aforementioned collapse issues: 1)The most intuitive reason is that the function words are often located at the front of the output distribution, which dominates the high probability region, causing the informative tokens hard to be sampled. . The output distribution trained with the ROC dataset contains more promptrelated tokens than WP, e.g., the “swim” and“water” in the top 50candidates of ROC output, as shown in Figure 2 (distribution histogram). Worse still, the function words dominate the high probability regions (from 35% to 45%) for the bad case and lead to terrible initialization at the first iteration step. 2)The iterative refinement mechanism depends on the token confidence of generated sequences, and it is easier for the lowconfidence but infor226Data Recurrent B1 B2 R1 R2 Dist Rep WP1 15.80 5.21 10.08 0.84 17.08 94.25 2 22.42 8.70 16.81 2.14 34.82 83.87 4 26.91 10.67 21.32 2.81 50.32 35.93 Table 3: The performance of different recurrent steps. mative tokens to be masked . In fact, the iterative refinement mechanism is designed for directed generation tasks, e.g., neural machine translation or summarization, which usually apply the argmax operation to sample results, and the evaluation of confidence is reasonable in different iterations. Nevertheless, we use the nucleus sampling strategy for inference in OpenLTG, which leads to the lowconfidence tokens with high priority being masked. 3)The massive absent context tokens suffer a more serious multimodality problem on long text generation in early iteration steps . As a result, the model is inclined to generate duplicated tokens due to the multimodal output distribution. Although iterative refinement can provide additional context to alleviate this issue, the model still cannot generate the expected results. The possible explanation is that the selfattention layer needs the context token as keyvalue pairs to calculate the token representation. Unfortunately, the massive uninformative mask tokens (“ <mask> ”) in context lead to model collapse steadily worsening in the following iteration steps. Thus, we utilize the recurrent generation mechanism for model training and inference to reduce the context dependency, which can also flexibly control the maximum length of the generated sequence (please refer to the Appendix B for more details about the model architectures and experiments). The results are shown in Table 3. We can observe that the model can gradually improve its performance as the recurrent steps increase, demonstrating that informative context dependency is the implicit reason for the model collapse. Improvements Based on the above analysis and findings, we categorize these critical factors into two types: the defects of attention mechanism andinappropriate inference strategies . In particular, we believe that each token should not pays attention to all context information, and most tokens only need the neighbor tokens’ information to represent the hidden states and predict the results. Therefore, we will change the selfattention mechanism of the pretrained MLMs so that each tokens can attend to the restricted neighbors. Besides, Figure 3: The overview of sliding window attention. we will adjust the confidence score of the output distributions to keep the informative tokens in subsequent iteration steps instead of being masked. 4 Method In this section, we propose two simple yet effective strategies for attention mechanism and inference to mitigate the model collapse problems: Dynamic Sliding Window Attention (DSWA) and Linear Temperature Decay (LTD). These designs do not break the paradigm of MLM so that it can flexibly adapt to the pretrained models. 4.1 Dynamic Sliding Window Attention We first introduced the sliding window mechanism (Beltagy et al., 2020) for the selfattention layer to adjust each token’s attention pattern, which also ensures that the top layer’s token representations can have a large receptive field, similar to CNN (Wu et al., 2018). Figure 3 illustrates the attention mask of the mixed attention layer of pretrained MLMs. It is worth noting that the keyvalue pairs consist of two parts: the source representation of the last layer (with green experiments in this paper are available at https://github.com/gchronis/features_ in_context . Figure 1: (top) Models are trained by using multiprototype embeddings in LLM space to predict gold feature vectors derived from psycholinguistic feature norms. (bottom) These same models are used to project contextual word embeddings to interpretable contextual feature space (model=B UCHANAN -PLSRMIL). 2019; McRae et al., 2005). By learning a mapping to these spaces, as illustrated in Figure 1, we attain contextsensitive, interpretable, realvalued lexicalsemantic features. After experimenting to determine best practices for contextualfeature projection, we use these features to explore whether contextual embeddings are sensitive to subtle semantic construals in different grammatical constructions. Specifically, we observe how even seemingly similar constructions can impart a different semantics on their component parts or ‘slot fillers’ (Trott et al., 2020; Goldberg, 2019). Consider the Article + Adjective + Numeral + Noun (AANN) construction: e.g., “a beautiful three days in London,” where the normally singular “a” precedes a plural noun and the adjective precedes the numeral (Solt, 2007; Dalrymple and King, 2019; Keenan, 2013). This construction often occurs with units or measure phrases (e.g., days, feet), but can also occur with nonmeasure nouns (e.g., “a lucky three students”). While it is tempting to think of “a lucky three students” as semantically equivalent to “three lucky242students,” it has a different construal . Specifically, the AANN construction is acceptable only when the noun behaves as a single collective unit and is, in effect, more semantically similar to a unit of measurement than it would be in the unmarked construction. Evidence for a difference in meaning between the two variants is seen in their divergent distributions. For example, the AANN construction is unavailable in contexts like (1) and (2) (#-ed cases; adapted from Solt, 2007). (1) The essay consisted of (a few eloquent paragraphs / # an eloquent few paragraphs) separated by pages of gibberish. (2) He played (five boring songs / # a boring five songs), but in between he played one really good one. The AANN construction cannot occur in contexts where the referent of the noun is split into noncontiguous parts. This distributional pattern is taken as evidence that the AANN construction construes its argument as a single, measurelike unit. In this paper, we study distributional evidence on a larger scale, using a contextualized large language model as a ‘compressed corpus’ that captures observed statistical regularities over utterances of many speakers. We analyze this compressed corpus by mapping embeddings to interpretable feature spaces based on psycholinguistic feature norms. When we do this for the embedding of the noun days in “I spent a beautiful three days in London,” we find the most salient difference with the “I spent three beautiful days in London” to be a higher value for features like measure and unit when it is in an AANN construction. We argue that this is because human speakers construe the AANN construction as being “measureish”, and that this construal is reflected in their language use in a way that the contextual language model can pick up. We conduct two case studies, one about AANNs and the other about grammatical subjecthood. Specifically, we show that a word in subject position is interpreted as more agentive than the very same word in object position (consistent with findings from psycholinguistics, e.g., Kako, 2006), and that a noun in the AANN construction is interpreted as more measurementlike than when in the canonical alternation. Our results demonstrate that construals can be inferred from statistical usage patterns. While we here use constructions with known construals, our positiveresults indicate that we may be able to analyze constructions where the construal is less clear in the theoretical literature. While feature norms have been used to interpret distributional semantic models (Baroni and Lenci, 2010; Herbelot and Vecchi, 2015; Fagarasan et al., 2015; Rosenfeld and Erk, 2023), we emphasize the linguistic value of reliable, reusable, interpretable semantic spaces, which we use to interrogate the semantic properties of language in use. The ability of our method to characterize subtle semantic differences using language models offers a point of connection between linguistically oriented deep neural network analysis (Baroni, 2021) and topics in formal linguistics. In particular, this work empirically demonstrates the potential alignment between LMs and featurebased theories of lexical semantics (as illustrated by Petersen and Potts, 2023). Our main goal is to use interpretable feature spaces for understanding the semantic construal of words in context, specifically the AANN construction and the transitive construction. In Section 2, we lay out our method for constructing interpretable feature spaces for tokens in context. Then, in Section 3, we evaluate the success of our method on a sense differentiation task, a homonym feature prediction task, and a qualitative analysis. The idea is that, if the method for mapping from embedding space to contextsensitive feature space is successful, we will predict unique semantic features for different senses. Having established and validated our method, we then turn to our key constructions in Section 4. 2 Methods The task is to learn a mapping from contextual word embedding space to an interpretable space defined by feature norms (Section 2.1), where every dimension corresponds to a semantic feature. We construct the training data by pairing feature norms with embeddings derived from contextual word vectors. We train models at the typelevel, e.g., to map the embedding vectors for the word ring to the set of feature norms for ring, as shown in the top half of Figure 1. But ultimately, we use the model to predict semantic features for individual tokens. That is, we project the token vector of a single occurrence of the word “ring” into the feature space learned at the typelevel, as shown in the bottom half of Figure 1.2432.1 Psycholinguistic feature norms We construct three semantic spaces, trained from three datasets of psycholinguistic feature norms. The McRae et al. (2005) feature norms comprise 541 concrete English nouns and 2,526 features. Participants were asked to list definitional properties of cue words. The features are full predicates; for example, a brush ‘has_bristles’ and is‘used_on_hair’ . The Buchanan et al. (2019) feature norms consist of over 4000 English words and 3,981 distinct features, from all openclass parts of speech, and include  evaluation established that models using BERTderived embeddings are just as good as staticMcRae Buchanan Binder MIL Vanilla MIL Vanilla MIL Vanilla PLSR .41 .39 .41 .42 .28 .26 FFNN .36 .36 .42 .40 .30 .30 PROP -.03 -.03 .10 .10 -.03 -.03 Table 1: Results of Sense Differentiation experiment. Pearson correlation of cosine similarities of predicted features vectors with WuPalmer similarity between senses. Data: pairs of tokens of the same noun lemma in SemCor. # Lemmas = 8021, # Tokenpairs = 1,045,966, p<0.0001 in all cases. embeddings for predicting semantic features. To evaluate our models on incontext feature prediction, we conduct two quantitative experiments: one on a sense differentiation task, one on a homonym disambiguation task, as well as a qualitative analysis for a representative word ( fire). The goal of this section is to explore whether the contextual feature norm method successfully captures contextual modulation of word meaning. For these experiments, we select the hyperparameters for each model that performed the best at typelevel feature prediction under 10fold crossvalidation (Appendix D). 3.1 Exp. 1: Sense Differentiation Tokenlevel evaluation is tricky because there are no existing datasets for incontext feature norms. Noting this obstacle, others utilize indirect methods like wordsense disambiguation and qualitative analysis, (Turton et al., 2020), or forego incontext evaluation (Chersoni et al., 2021). Turton et al. (2020) evaluate the Binder feature prediction model using the Words in Context Dataset (Pilehvar and CamachoCollados, 2019), which only labels token pairs as ‘same meaning’ or ‘different meaning’. We devise a sense differentiation experiment using the SemCor corpus, (Miller et al., 1994), which lets us do a more finegrained analysis in terms of close and distant polysemy. The logic of this experiment is that, if two senses of a word are semantically distant , we expect the feature vectors in projected space to also be distant. We test the quality of our predicted feature vectors by testing how well the cosine distance between vectors for polysemous words corresponds to the distance between their senses in WordNet (Fellbaum, 2010). To build this dataset, we collect examples of noun lemmas in the SemCor corpus, which is an245notated with WordNet senses for words in context. In SemCor, “Water is a human right,” is labeled right.n.02 ,an analysis (Baroni, 2021) and topics in formal linguistics. In particular, this work empirically demonstrates the potential alignment between LMs and featurebased theories of lexical semantics (as illustrated by Petersen and Potts, 2023). Our main goal is to use interpretable feature spaces for understanding the semantic construal of words in context, specifically the AANN construction and the transitive construction. In Section 2, we lay out our method for constructing interpretable feature spaces for tokens in context. Then, in Section 3, we evaluate the success of our method on a sense differentiation task, a homonym feature prediction task, and a qualitative analysis. The idea is that, if the method for mapping from embedding space to contextsensitive feature space is successful, we will predict unique semantic features for different senses. Having established and validated our method, we then turn to our key constructions in Section 4. 2 Methods The task is to learn a mapping from contextual word embedding space to an interpretable space defined by feature norms (Section 2.1), where every dimension corresponds to a semantic feature. We construct the training data by pairing feature norms with embeddings derived from contextual word vectors. We train models at the typelevel, e.g., to map the embedding vectors for the word ring to the set of feature norms for ring, as shown in the top half of Figure 1. But ultimately, we use the model to predict semantic features for individual tokens. That is, we project the token vector of a single occurrence of the word “ring” into the feature space learned at the typelevel, as shown in the bottom half of Figure 1.2432.1 Psycholinguistic feature norms We construct three semantic spaces, trained from three datasets of psycholinguistic feature norms. The McRae et al. (2005) feature norms comprise 541 concrete English nouns and 2,526 features. Participants were asked to list definitional properties of cue words. The features are full predicates; for example, a brush ‘has_bristles’ and is‘used_on_hair’ . The Buchanan et al. (2019) feature norms consist of over 4000 English words and 3,981 distinct features, from all openclass parts of speech, and include Experiments revealed that phraselevel dependency modelling with holographic composition can induce correct supertagging, achieving stateof-the262art performance in supertagging and parsing with a C&C parser ( Clark and Curran ,2007 ;Clark et al. , 2015 ), and further improved performance with a novel spanbased parsing algorithm.1 Additionally, we focused on the fact that the inverse operation of holographic composition iseasily available. This property can be applied to textinﬁlling tasks, predicting missing parts of sentences consistent with the rest syntactically andsemantically. This task is difﬁcult to accomplish using the existing neural architectures. The main contributions of this research can be summarized as follows: 1.We introduce HolE as a recursive compositional operator for explicit modelling of syntactic structures, enabling CCG to be treated as an operation between distributed representations. This modelling improves supertagging and parsing, achieving stateof-theart performance with a C&C parser. 2.We propose a novel spanbased parsing algorithm incorporating phraselevel representation from our model, achieving comparable performance to the current stateof-theart. 3.We propose an approach to compute phraselevel representations containing rich syntactic information while satisfying decomposability. We further demonstrate the applicability of decomposability to phraselevel textinﬁlling. 2  evaluation, we extracteddependencies from CCG derivation using the generate program of the C&C parser. V ariations in grammatical constraints caused programmaticextraction failure for some sentences, so their dependencies were replaced by C&C parser results. Furthermore, we implemented the skimmer mode for our spanbased parsing algorithm, along with the C&C parser, enabling the detection of dependencies between words, even if the parser is unable to parse the entire sentence. Consequently, our parser achieved 100% coverage. 5 Results 5.1 Supertagging Accuracy Table 1presents supertagging accuracy on development data for each training loss combination. First, we compared models with norm constraints on real space and found our proposed models to be statistically superior to the baseline in terms of supertagging accuracy. Moreover, the supertaggingperformance varied slightly compared to the model with norm constraints on the complex space, indicating a low impact of the type of norm constraint. Table 2shows the proposed supertagging model outperforming existing models, achieving a newstate of the art. This indicates the effectiveness of the proposed approach in inducing category assignments at the word level while considering phraselevel representations. 5.2 Parsing Performance The labeled Fscores of the current spanbased parser and C&C parser on the development dataare presented in Table 1. First, the model with norm constraints on the real space outperformed the baseline, even with the same C&C parser, due to improved supertagging. Furthermore, compared with C&C, the proposed spanbased parsing algorithm improved performance for the model with norm constraint on real space. However, the performance gap between C&C and the model with norm constraints on complex space is relatively small. This implies that models’ expressive power with the norm constraint on complex space is limitedcompared to real space. This could be due to representations being distributed on a ddimensional unit hypersphere in complex space, thus lacking norm information along each dimension. Model using proposed supertagging approach and C&C outperformed all existing models with the same parser (Table 2). Furthermore, the performance of the proposed spanbased parsing model is comparable to that of the current stateof-theart model of Clark (2021 ) using Transformers. Overall, results indicate recursive holographic compositions improve CCG parsing performance. 5.3 Replacement of Compositional Operator Examining performance gaps when employing alternative compositional operators in our analysis of the original and replaced sentences and showed that all the nonterminal symbols in the preand postreplaced phrases matched in our experiments demonstrate that LPT consistently sets stateof- theart performance on multiple lifelong learning settings of UIE, including taskincremental setting on seen tasks, fewshot adaptation, and zeroshot generalization on novel tasks1. 1  evaluation of KT extends to the novel tasks. In NLP community, large Pretrained Language Models (PLMs) have been widely applied in many downstream tasks. In order to lower computation and storage costs, recent popular lifelong learning techniques (Madotto et al., 2021; Ke et al., 2021a; Zhu et al., 2022; Wang et al., 2022c) try to solve the CF and KT leveraging parameterefficient finetuning (PEFT) methods (He et al., 2022a). In this work, we inherit this wisdom and also focus on parameterefficient methods for lifelong learning. Inspired by the lottery ticket hypothesis and the efficiency of prompt tuning, we propose a novel framework for lifelong UIE, named Lottery Prompt Tuning (LPT). Specifically, we adopt an encoderdecoder model architecture (Raffel et al., 2020) and reframe all types of IE tasks into a textto-structure format (Lu et al., 2022). First, we prepend a sequence of continuous prompt vectors to the input, which is shared across tasks. To continually learn a new IE task, we simultaneously learn the prompt vectors together with a taskaware binary prompt mask. The taskaware mask is devoted to pruning the shared prompt vectors and producing an optimal taskspecific pruned prompt,i.e., lottery prompt. To provide a pruning criterion for finding the lottery prompt online, we introduce a separate set of learnable parameters serving as the importance scores, which have the same shapes as the soft prompts. Hence, the lottery prompt can be easily found by selecting the parameters with the Topk%importance scores online, without iterative retraining and pruning procedure. To facilitate the forward knowledge transfer when learning a new task, the lottery prompt is permitted to selectively reuse the learned prompt parameters for the former tasks. Besides, the proposed LPT eliminates catastrophic forgetting and negative transfer by freezing the prompt parameters for the previous tasks during backpropagation. In the whole learning process, the PLM is kept frozen to maintain general knowledge. During inference, the same model can handle different tasks by inputting different lottery prompts, which is friendly for deployment. We show that our proposed framework effectively outperforms stateof-theart baselines on lifelong learning for UIE in terms of catastrophic forgetting prevention and knowledge transfer. Moreover, LPT closes the gap between continual learning and multitask learning. The efficacy of the proposed modules is thoroughly studied both empirically and analytically. In summary, this work makes three key contributions: •A challenging yet practical benchmark is proposed for lifelong UIE, where one UIE system should not only keep its performance on solving seen IE tasks, but also generalize well on novel IE tasks with few or even no examples. •We proposed Lottery Prompt Tuning (LPT), an extremely efficient prompt tuning framework for lifelong UIE that directly learns pruned prompts sequentially without an extra pruning stage. •Extensive experiments on the benchmark show that our approach outperformed baselines with higher parameter efficiency. 2 Related Work Lifelong Learning Lifelong Learning, also known as Continual Learning, aims to learn a sequence of tasks with one single model. Two main goals are demanded: catastrophic forgetting (CF) prevention and positive knowledge transfer (KT). The research in this area can be categorized into three folds: Regularization ,Rehearsal , and Architecture based methods. (a) Regularizationbased278methods (Li and Hoiem, 2017; Kirkpatrick et al., 2017; Ritter et al., 2018) ease the catastrophic forgetting issue by regularizing important parameters for learned tasks. These approaches usually need a tradeoff between learning new tasks and forgetting the old tasks. In NLP, it is studied (Han et al., 2020) to constrain the useful information from the huge amount of knowledge inside the PLMs. (b) Rehearsalbased methods methods reuse old examples from the previously learned tasks while learning new tasks. These examples are either derived from real training data of previous tasks (Rebuffi et al., 2017; LopezPaz and Ranzato, 2017; Mi et al., 2020), or generated by a pseudodata generator (Sun et al., 2019; Qin and Joty, 2021; Zhao et al., 2022). Although these methods work well, they are limited by data privacy or the quality of generated data. (c) Architecturebased methods tackle the continual learning problem by expanding new modules to the network over time (Veniat et al., 2020; Douillard et al., 2022) or isolating the network’s parameters for different tasks (Serra et al., 2018; Mallya and Lazebnik, 2018; Mallya et al., 2018; Wortsman et al., 2020; Geng et al., 2021; Kang et al., 2022). In NLP, in order to better take advantage of the PLMs, these methods usually are in conjunction with parameterefficient finetuning approaches, including adapter tuning (Houlsby et al., 2019) and prompt tuning (Lester et al., 2021a; Li and Liang, 2021; Liu et al., 2022b). AdapterCL (Madotto et al., 2021) trains a separate adapter for each task, leaving knowledge transfer out of consideration. Ke et al. (2021b,a); Ermis et al. (2022); Zhang et al. (2022) overcome this drawback by introducing capsule network (Sabour et al., 2017), distillation mechanism and adaptive compositional modules, respectively. For the latter, CPT (Zhu et al., 2022) learns a separate prompt with continual prompt initialization for each task. Wang et al. (2022c,b) propose to learn a prompt pool and then select the useful prompts to alleviate forgetting and potentially share knowledge across tasks. Dai et al. (2022) extend the idea to organize the prompt pools in a hierarchical way to guide the pretrained models in different granularities. In contrast, we here share a single copy of prompt parameters to instruct the PLMs, yet incrementally learn a taskaware prompt mask for each task whilst keeping the prompt parameters used by the previous tasks unchanged. This not only isolates the harmful prompt parameters that lead toforgetting but also shares useful prompt parameters for knowledge transfer. Lifelong learning in Information Extraction In IE areas, some efforts are paid for building IE systems to handle continual learning scenarios, including continual NER (Monaikul et al., 2021; Zheng et al., 2022), relation extraction (Cui et al., 2021; Qin and Joty, 2022; Wang et al., 2022a), and event detection (Yu et al., 2021; Liu et al., 2022a). However, they merely study continual learning on one single IE task. Very recently, UIE (Lu et al., 2022; Fei et al., 2022) regards general IE tasks as a textto-structure generation task, thus unifies all IE tasks with one model framwork. To a step further, our work studies a more challenging yet practical continual learning paradigm for UIE, where one universal IE system needs to solve different types of IE tasks across different domains incrementally. Lottery Ticket Hypothesis Frankle and Carbin (2018) propose the The Lottery Ticket Hypothesis (LTH) that an overparameterized network contains a subnetwork (lottery ticket) that, when initialized and trained in isolation, can match or exceed the test accuracy of the original network after training for at most the same number of iterations. The LTH has been widely explored in many fields of deep learning (Liu et al., 2018; Frankle et al., 2019; Gong et al., 2022; Yu et al., 2019) In NLP, researchers also explore the existence of winning tickets under transfer learning regimes for overparametrized pretrained language models across various tasks (Morcos et al., 2019; Desai et al., 2019). Chen et al. (2020); Prasanna et al. (2020) show the existence of winning tickets when finetuning BERT on downstream tasks. Liang et al. (2021) shows the existence of super tickets inside PLMs that can improve generalization. Xprompt (Ma et al., 2022) is the pioneer to explore the LTH in the context of prompt tuning by hierarchical structure pruning. However, Xprompt needs iterative retraining, pruning and rewinding to get the pruned prompts, which is impractical to perform during continual learning settings since it needs excessive computational time and costs. By contrast, our LPT does not require an explicit pruning stage and jointly learns prompt and taskrelated masks together, which accelerates convergence during continual learning. Moreover, our pruning is performed at the parameter level while Xprompt’s pruning is performed at the token and piece level.2793 Preliminary 3.1 Lifelong Learning Protocols Conventional continual learning is defined as training machine learning models on a continuum of data from a sequence of tasks. Here in our lifelong learning protocols for UIE, the incoming task on the task sequence can be of different types (e.g., entity extraction, relation extraction, event extraction, and aspectbased sentiment analysis.), or of the same type but potentially of different domains. An intuitive demonstration can be found in Figure 1. Formally, we define a sequence of tasksD={D1,···,DT}, where the kth task Dk=/braceleftbig/parenleftbig xk i,yk i/parenrightbig/bracerightbigNk i=1contains a set of data samples. For each data sample, the input xk iis constructed by the raw text tk iand a specific predefined schema sk i, while the desirable output yk i is structural information contained in the text xk i indicated by the schema sk i. Note that our approach is Rehearsalfree, meaning that data from the previous tasks can not be used anymore when training future tasks. The goal of a lifelong UIE model should perform well on all Ttasks after being trained with the samples of these tasks sequentially. Further, in the realistic scenario, it is usually expensive and impractical to acquire plenty of labeled data for a newly emerged task. To simulate this circumstance, we adapt the sequentially trained model on a set of nnovel novel tasks individually {Di}Nnovel i=1. Hence, we can access the model’s ability to accumulate previously learned knowledge for generalization to new tasks by evaluating the fewshot/zeroshot transferability of the lifelong model. 3.2 Generative UIE Framework In this section, we cast all IE tasks as text generation and model the UIE system in a textto-structure framework (Lu et al., 2022). In this generative framework, different IE structure generation is decomposed into two atomic operations, i.e., spotting and associating. Spotting indicates locating target information pieces from the sentence, e.g., the entity and the trigger word in the event. Associating means connecting spans by assigning them with specific semantic roles based on predefined schemas, such as the relation between entity pairs or the role between an event and its argument. Input the input xfor the UIE model is formulated as the concatenation of the raw sentence anda schemabased prompt in the form of: x= [s;t] =/bracketleftbig s1, s2, . . . , s |s|, t1, t2, . . . , t |t|/bracketrightbig =[[spot],SPOT 1,[spot],SPOT 2. . . , [asso],ASSO 1,[asso],ASSO 2. . . , [text], t1, t2, . . . , t |t|/bracketrightbig (1) SPOT irepresents the targeted spotting name in the IE tasks, e.g., “organization\" in the NER task; and ASSO irepresents the targeted association name, e.g., “work for” in the relation extraction task. Output the output text yis a unified Structured Extraction Language (SEL) that describes how the structural elements organize into the target structure, which can be represented as “{Spot Name: Info Span, (Asso Name: Info Span) (Asso Name: Info Span)}\" . The Spot Name andAsso Name are the target structure from the predefined schemas, while the Info Span refer to the text span mentioned in the raw text. Model We employ a Transformerbased encoderdecoder language model i.e., T5 (Raffel et al., 2020), as the model architecture for UIE. Given the schema and the raw sentence as input sequences x and the SEL as output sequences y, the model computes the conditional language model distribution of each token yiusing the chain rule of probability asp(yi|y<i, x). It finishes prediction when outputting the end signal [EOS] . The predicted SEL expression will be converted back into the extracted information record for evaluation. 4 analysis.), or of the same type but potentially of different domains. An intuitive demonstration can be found in Figure 1. Formally, we define a sequence of tasksD={D1,···,DT}, where the kth task Dk=/braceleftbig/parenleftbig xk i,yk i/parenrightbig/bracerightbigNk i=1contains a set of data samples. For each data sample, the input xk iis constructed by the raw text tk iand a specific predefined schema sk i, while the desirable output yk i is structural information contained in the text xk i indicated by the schema sk i. Note that our approach is Rehearsalfree, meaning that data from the previous tasks can not be used anymore when training future tasks. The goal of a lifelong UIE model should perform well on all Ttasks after being trained with the samples of these tasks sequentially. Further, in the realistic scenario, it is usually expensive and impractical to acquire plenty of labeled data for a newly emerged task. To simulate this circumstance, we adapt the sequentially trained model on a set of nnovel novel tasks individually {Di}Nnovel i=1. Hence, we can access the model’s ability to accumulate previously learned knowledge for generalization to new tasks by evaluating the fewshot/zeroshot transferability of the lifelong model. 3.2 Generative UIE Framework In this section, we cast all IE tasks as text generation and model the UIE system in a textto-structure framework (Lu et al., 2022). In this generative framework, different IE structure generation is decomposed into two atomic operations, i.e., spotting and associating. Spotting indicates locating target information pieces from the sentence, e.g., the entity and the trigger word in the event. Associating means connecting spans by assigning them with specific semantic roles based on predefined schemas, such as the relation between entity pairs or the role between an event and its argument. Input the input xfor the UIE model is formulated as the concatenation of the raw sentence anda schemabased prompt in the form of: x= [s;t] =/bracketleftbig s1, s2, . . . , s |s|, t1, t2, . . . , t |t|/bracketrightbig =[[spot],SPOT 1,[spot],SPOT 2. . . , [asso],ASSO 1,[asso],ASSO 2. . . , [text], t1, t2, . . . , t |t|/bracketrightbig (1) SPOT irepresents the targeted spotting name in the IE tasks, e.g., “organization\" in the NER task; and ASSO irepresents the targeted association name, e.g., “work for” in the relation extraction task. Output the output text yis a unified Structured Extraction Language (SEL) that describes how the structural elements organize into the target structure, which can be represented as “{Spot Name: Info Span, (Asso Name: Info Span) (Asso Name: Info Span)}\" . The Spot Name andAsso Name are the target structure from the predefined schemas, while the Info Span refer to the text span mentioned in the raw text. Model We employ a Transformerbased encoderdecoder language model i.e., T5 (Raffel et al., 2020), as the model architecture for UIE. Given the schema and the raw sentence as input sequences x and the SEL as output sequences y, the model computes the conditional language model distribution of each token yiusing the chain rule of probability asp(yi|y<i, x). It finishes prediction when outputting the end signal [EOS] . The predicted SEL expression will be converted back into the extracted information record for evaluation. 4 experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrievalaugmented methods and analyze why they work. 1 Experimental Setup Datasets. We conduct our experiments on two widely used documentlevel EAE datasets: RAMS (Ebner et al. ,2020 ) and WikiEvents ( Li et al. , 2021 ). RAMS provides 9,124 annotated examples from news based on 139 event types and 65roles. WikiEvents provides 246 annotated documents based on 50 event types and 59 roles.297ModelsRAMS WikiEventsPLM ArgI ArgC ArgI ArgC HeadC Multilabel classiﬁcationbased Models BERTCRF ( Shi and Lin ,2019 )∗- 40.3 - 32.3 43.3 BERTbase PAIE ( Ma et al. ,2022 )∗54.7 49.5 68.9 63.4 66.5 BARTbase 56.8 52.2 70.5 65.3 68.4 BARTlarge QAbased Models EEQA ( Du and Cardie ,2020 )∗46.4 44.0 54.3 53.2 56.9 BERTbase 48.7 46.7 56.9 54.5 59.3 BERTlarge DocMRC ( Liu et al. ,2021 )∗- 45.7 - 43.3 - BERTbase Generationbased Models BARTGen ( Li et al. ,2021 )∗50.9 44.9 47.5 41.7 44.2 BARTbase 51.2 47.1 66.8 62.4 65.4 BARTlarge T5baseline‡45.1 37.3 44.8 39.1 39.3 T5base 45.9 40.3 62.7 41.0 53.7 T5large Our Models using Retrievalaugmented Generation Setting 1: ContextConsistency Retrieval 52.2 44.9 59.8 40.4 58.7 T5base 53.9 47.9 66.8 50.9 63.4 T5large Setting 2: SchemaConsistency Retrieval 45.9 38.6 53.4 39.7 43.0 T5base 49.1 41.0 64.4 53.8 61.8 T5large Setting 3: Adaptive Hybrid Retrieval 53.3 46.3 61.4 46.1 62.5 T5base 54.6 48.4 69.6 63.4 68.4 T5large Table 1: Experimental results on RAMS and WikiEvents.∗means the results from ( Ma et al. ,2022 ), and‡denotes the results from our implemented models for a fairer comparison. We highlight the SOTA results (classiﬁcationbased evaluation for analyzing various retrieval settings and observe that given a document, (1) contextconsistency retrieval(Setting 1 ) helps the model identify the argument span more accurately than Setting 2. This suggests that indistribution demonstration contextscan contribute to performance gains by improving the ability to recognize argument spans; (2) schemaconsistency retrieval ( Setting 2 ) makes the generated role labels more accurate than Setting 1, which indicates that conditioning on the label space contributes to better performance by alleviating the difﬁculty of learning the complex event pattern; and (3) adaptive hybrid retrieval ( Setting 3 ) has achieved stateof-theart (SOTA) performance among all generationbased baselines, indicating that this setting can generate diverse and faithfulpseudo demonstrations with consistency in both input space and label space. Overall, the contributions can be summarized as follows: •We are the ﬁrst to explore how to design the retrieval strategy for documentlevel EAE from the input and label distribution views. And our introduced retrieval strategies can recall demonstrations that can be helpful to demonstrate how the model should solve the task. •We further propose a novel adaptive hybrid retrieval augmentation paradigm that adaptively samples pseudo demonstrations from continuous space for each training instance to improve the analogical capability of the model. •Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrievalaugmented methods. We also conducted additional analytical experiments to discuss the reasons why different settings affect performance. 2 Methodology Problem Deﬁnition. We formulate documentlevel EAE in the manner of Ebner et al. (2020 ): given a document x={w1,w2,...,w|x|}, it contains a set of described events E. Each event e∈E has its event type tand designated by a trigger (a text span in x). Each event type tspeciﬁes a role setRt. The event schema eis made up of event type and its associated role set. The task aims to extract all (a,r)pairs for each e∈E, wherea∈x is an argument—a text span in xandr∈R tis the role thatatakes.294Training corpus Encoder DecoderEncoderEncoder Decoder topkTraining corpus Input Sequence Target SequenceEncoderEncoder DecoderDecoderEncoderEncoder Decoder topkRetrieved Doc (a) ContextConsistency RetrievalAugmented DocEAETraining corpusEncoder DecoderEncoderRetrieved Label topk Target SequenceInput Sequence (b) SchemaConsistency RetrievalAugmented DocEAE TransportPerson Preventer Transporter Origin Destination [SEP] ... , the FSB special forces detained 53 young men , ..., the <tgr> arrests <tgr> made at the illegal prayer hall ... in Syria Input Sequence the FSB [ Preventer ] 53 young men [ Transporter ] the illegal prayer hall [ Origin ] Syria [ Destination ] Target SequenceEncoder Decoder Training corpus Retrieved Doc EncoderGaussian Sampling topk Repeat k times (c) Adaptive Hybrid RetrievalAugmented DocEAE Figure 2: An illustration of our proposed three retrievalaugmented methods. Subﬁgures (a), (b), and (c) refer to the three retrievalaugmented methods, respectively. xrdenotes the retrieved document, while yrmeans the retrieved event label. hdis the representation of the retrieved kdiscrete demonstrations, vis the sampled kpseudo demonstrations. The gray dashed lines in (a) and (b) denote that two encoders share parameters, as does (c). For retrievalaugmented documentlevel EAE, we ﬁrst retrieve the topkpotentially helpful demonstrations (discrete or continuous), then fuse them into the decoder to generate role records (a sequence of (a,r)pairs). In the following, we ﬁrst introduce how to reformulate documentlevel EAE as RetrievalAugmented Generation (RAG), then describe various retrieval settings. 2.1 Basic RAG Architecture We adopt the T5 model ( Raffel et al. ,2022 ), an encoderdecoder pretrained model, as a backbone. The encoderdecoder LM models the conditional probability of selecting a new token y(i)given the previous tokens y(<i)and the encoder input [e;x] during the generation process. As a result, the total probability p(y|x,e)of generating the output y given the input [e;x]is calculated as: p(y|x,e)=|y|/productdisplay i=1p/parenleftBig y(i)|y(<i),x,e/parenrightBig , (1) where the input sequence is the concatenation of the document context and its event schema, constructed as <s> event schema [SEP] document context </s> . The output yis the role record, presenting by the concatenation of each argument and its event role, i.e., <s> arg1role 1...argnrole n</s> .In this paper, we decompose the modeling of p(y|x,e)into two steps: retrieval and prediction . Given a query document x, we ﬁrst retrieve topk potentially helpful demonstrations dfrom training corpusDtrain. We model this as sampling from a distribution p(d|x). Then we use siamese network structures to obtain meaningful embeddings for input sequence [e;x]and demonstration d: he,hx=T5Encoder ([e;x]), hd=T5Encoder (d).(2) Then, we condition on both the retrieved d and the original input [e;x]to generate the outputy—modeled as p(y|d,x,e). Speciﬁcally, we integrate kdemonstration embeddings hd= {hd(1),hd(2),...,hd(k)}into crossattention module in all decoder layers by concatenating them to the encoder outputs and feed them all to decoder: y=T5Decoder (<bos>; [hd;he;hx]), (3) where <bos> is the beginning token of decoder, [hd;he;hx]denotes the encoder outputs we constructed for decoder input. In Setting 3, we use [v;he;hx]instead. To obtain the overall likelihood of generating y, we treat das a latent variable, yielding: p(y|x,e)=p(y|d,x,e)p(d|x). (4)295Figure 3: The geometric diagram of the proposed Gaussian sampling. ¯he,¯hxand ¯hd(i)are the representations of the event schema, the document and the ith discrete demonstrations, ¯he=meanpooling (he),¯hx= meanpooling (hx),¯hd(i)=meanpooling (hd(i)).T o sample pseudo demonstrations from event semantic region (the light blue intersection), we formalize v(i)=¯he+ω(i)⊙b(i.e., the blue dashed arrow) as a pseudo demonstration, in which the bias vector b=¯hx−¯he, scale vector ω(i)∈(1−r(i)/R,1), r(i)=||¯hx−¯hd(i)||,R=||¯hx−¯he||. 2.2 Demonstration Retrieval Design The main challenge of demonstration retrieval is to design an appropriate retrieval strategy to recall demonstrations that can be helpful to demonstrate how the model should solve the task. In this part, we explore various retrieval settings. As shown in Figure 2, we categorize the retrieval setting into three categories: (1) ContextConsistency Retrieval; (2) SchemaConsistency Retrieval; and (3) Adaptive Hybrid Retrieval. The goal of all retrieval settings in this part is to ﬁnd kdemonstrations (whether discrete or continuous). Setting 1: ContextConsistency Retrieval Since similar documents cannot guarantee the same distribution of event labels, Setting 1 aims to answer whether it makes sense to pursue xrto be similar to xin the retrieval process. Given a query document x, we retrieve the instance document xr from the training corpus Dtrainthat is the topkrel- evant to the original input document, as discretedemonstrations d. For retrieval, we use SBERT (Reimers and Gurevych ,2019 ) to retrieve semantically similar documents xr∈D train. Setting 2: SchemaConsistency Retrieval To explore whether conditioning on the label space contributes to performance gains, Setting 2 satisﬁes event schema consistency and aims to alleviate the difﬁculty of learning the complex event pattern of y. Given the event label yof input as query, weAlgorithm 1: Gaussian Sampling Input: The embeddings of schema, document and discrete demonstrations, i.e. ¯he,¯hxand ¯hd={¯hd(1),¯hd(2),..., ¯hd(k)} Output: A set of pseudo demonstrations v={v(1),v(2),...,v(k)} 1Normalizing the importance of each element in b=¯hx−¯he:Wr=|b|−min(|b|) max(|b|)−min(|b|) 2Initialize i←0 3whilei≤(k−1)do 4i←i+1 5r(i)=||¯hx−¯hd(i)||,R=||¯hx−¯he|| 6 Use reparametrizetion to calculate the current scale vector: ω(i)∼N/parenleftBig 1−r(i)/R+1 2,diag/parenleftbig W2 r/parenrightbig/parenrightBig . 7 First sample a noise variable /epsilon1fromN(0,1) 8 Then transform it to ω(i)=μ+/epsilon1·σ, where μ=1−r(i)/R 2,σ=Wr. 9 Calculate the current sample: v(i)=¯he+ω(i)⊙b 10 v←v∪v(i) 11end retrieve (also via SBERT) the instance label yr that is the topkrelevant to the input label from the training corpus Dtrain. During the inference, the query is the event schema eof test sample. Setting 3: Adaptive Hybrid Retrieval To ﬁnd the ideal demonstration that has equal distribution with input document in both input and label space to guide the model, we propose a novel adaptive hybrid retrieval strategy to sample pseudo demonstrations from continuous space as depth cues to improve the analogical capability of model. Given an instance document x, we ﬁrst retrieve topkhelpful documents from the training corpus Dtrain. Conditioning on retrieved kdiscrete demonstrations, we adaptively determine kevent semantic regions in continuous space for each training instance. Then we sample kpseudo demonstrations fromkevent semantic regions. Event Semantic Region. We treat points in the event semantic region as the critical states of eventsemantic equivalence. Speciﬁcally, in order to consider both context and event schema consistency, we ﬁrst determine the adjacent region of document and event schema by setting their adjacent radii (the orange circle and purple circle in Figure 3). Furthermore, we deﬁne the intersection of their adjacent regions as an event semantic region /uprise(¯he,¯hx) (the light blue region in Figure 3), which describes accurate alternatives in consistency with original296context and event semantic meaning. Here we have kdiscrete demonstration embeddings ¯hdforkadjacent radii r, which determines kevent semantic regions. For each event semantic region, we perform the following Gaussian sampling. Gaussian Sampling. To obtain diverse and faithful pseudo demonstrations from the event semantic region for the training instance x, we apply a Gaussian sampling strategy ktimes to sample a cluster of vectors from kevent semantic regions. As shown in Figure 3, we ﬁrst use scale vector ω(i)to transform the bias vector b=¯hx−¯he asω(i)⊙b, where⊙is the elementwise product operation. Then, we construct a novel sample v(i)=¯he+ω(i)⊙bas a pseudo demonstration. As a result, the goal of the sampling strategy turns into ﬁnding a set of scale vectors, i.e. ω={ω(1),ω(2),...,ω(k)}. Intuitively, we can assume that ω(i)follows a distribution with Gaussian forms, formally: ω(i)∼N/parenleftbigg1−r(i)/R+1 2,diag/parenleftbig W2 r/parenrightbig/parenrightbigg , (5) whereWr=|b|−min(|b|) max(|b|)−min(|b|)normalizes the importance of each dimension in b, the operation |·| takes the absolute value of each element in vector, which indicates the larger the value is, the more informative it is. μ=1−r(i)/R+1 2constrains the sampling range to event semantic region. Since sampling is a nondifferentiable operation that truncates the gradient, here we usea reparametrization trick to construct N(1− r(i)/R 2,diag(W2 r)). We ﬁrst sample a noise variable/epsilon1from standard normal distribution N(0,1). Then, instead of writing ω(i)∼N (μ,σ2): ω(i)=μ+/epsilon1·σ, (6) where/epsilon1∼N (0,1),μ=1−r(i)/R 2,σ=Wr. Now the gradient is inside the expectation. We ﬁnally sample kpseudo demonstrations vfromk event semantic regions to augment the text generation, that is v={v(1),v(2),...,v(k)}, where v(i)∼/uprise(¯he,¯hx).kis the hyperparameter of the number of sampled vectors, which is determinedby the number of discrete demonstrations. For a clearer presentation, Algorithm 1summarizes the sampling process.Algorithm 2: Decoding the output Input: role record y:<s> arg1role 1...argnrole n</s> . Output: (arg,role )pairs. 1Initialize arg list←[] 2foryi∈ydo 3 /* Here consider multievent scenario, separated by [SEP] */ 4 ifyi/negationslash=[SEP] then 5 ifyi/∈role list then 6 append yitoarg list 7 else 8 role←yi 9 argument ←arg list 10 get a (arg,role )pair 11 arg list←[] 12 end 13 else 14 eventindex ←eventindex +1 15 arg list←[] 16 end 17end 2.3 Training and Inference The trainable parameters of the model are only the encoderdecoder LM, which is denoted as θ. Given a training dataset Dtrain =/braceleftbig (x1,y1),...,/parenleftbig x|Dtrain|,y|Dtrain|/parenrightbig/bracerightbig , where each instance is a (document, role records) pair, the learning objective is a negative loglikelihood function: L=−/summationdisplay (x,y)∈D trainlogp(y|x,d,e,θ). (7) After generating role records, we need to decode it back into (argument, role) pairs to calculate speciﬁc evaluation metrics. The detailed decoding process is in Algorithm 2. 3 Experiments We evaluate our model’s performance on two commonly used documentlevel EAE benchmarks and compare it to prior works. Then we conduct additional analytical experiments on how the demonstration retrieval design affects performance. 3.1 Experimental Setup Datasets. We conduct our experiments on two widely used documentlevel EAE datasets: RAMS (Ebner et al. ,2020 ) and WikiEvents ( Li et al. , 2021 ). RAMS provides 9,124 annotated examples from news based on 139 event types and 65roles. WikiEvents provides 246 annotated documents based on 50 event types and 59 roles.297ModelsRAMS WikiEventsPLM ArgI ArgC ArgI ArgC HeadC Multilabel classiﬁcationbased Models BERTCRF ( Shi and Lin ,2019 )∗- 40.3 - 32.3 43.3 BERTbase PAIE ( Ma et al. ,2022 )∗54.7 49.5 68.9 63.4 66.5 BARTbase 56.8 52.2 70.5 65.3 68.4 BARTlarge QAbased Models EEQA ( Du and Cardie ,2020 )∗46.4 44.0 54.3 53.2 56.9 BERTbase 48.7 46.7 56.9 54.5 59.3 BERTlarge DocMRC ( Liu et al. ,2021 )∗- 45.7 - 43.3 - BERTbase Generationbased Models BARTGen ( Li et al. ,2021 )∗50.9 44.9 47.5 41.7 44.2 BARTbase 51.2 47.1 66.8 62.4 65.4 BARTlarge T5baseline‡45.1 37.3 44.8 39.1 39.3 T5base 45.9 40.3 62.7 41.0 53.7 T5large Our Models using Retrievalaugmented Generation Setting 1: ContextConsistency Retrieval 52.2 44.9 59.8 40.4 58.7 T5base 53.9 47.9 66.8 50.9 63.4 T5large Setting 2: SchemaConsistency Retrieval 45.9 38.6 53.4 39.7 43.0 T5base 49.1 41.0 64.4 53.8 61.8 T5large Setting 3: Adaptive Hybrid Retrieval 53.3 46.3 61.4 46.1 62.5 T5base 54.6 48.4 69.6 63.4 68.4 T5large Table 1: Experimental results on RAMS and WikiEvents.∗means the results from ( Ma et al. ,2022 ), and‡denotes the results from our implemented models for a fairer comparison. We highlight the SOTA results (classiﬁcationbased Analysis Impact of the input space. To explore the reason why contextconsistency affects performance, we additionally experiment with two variants of the document (random documents and outof- distribution documents) on RAMS and WikiEvents. Speciﬁcally, “random documents” means that we randomly choose a set of kdocuments from their own training set as the demonstrations. “Outof-distribution documents” means that we randomlychoose a set of kdocuments from each other’s training set as the demonstrations. Figure 4shows that using outof-distribution documents as references signiﬁcantly drops the performance, and using random documents is better than no demonstrations. Setting 1 improves ArgC F1 by about 6.0% and 11.8% over the “random documents” and no demonstrations. This is likely because using the indistribution text as the context makes the task closer to language modeling since the LM always conditions on the indistribution text during training. Furthermore, using indistribution with similar text as context can further improve performance.29944454647 35 1 0 20ArgC F1 (%) Number of demonstrationsRAMS 101520253035404550 35 1 020ArgC F1 (%) Number of demonstrationsWikiEvents Figure 5: Performance of Setting 3 on both datasets as a function of the number of demonstrations. Impact of the label space. To explore the reason why schema consistency affects performance, we experiment with two variants of Setting 2 (random labels and random English words) on RAMS and WikiEvents. Speciﬁcally, “random labels” means that we randomly choose a set of klabels from their own training set as the demonstrations. “Random English words” means that we randomly choosea set of English words from https://pypi.org/ project/englishwords/ (consists of 61,569 words) as the demonstrations. From Figure 4we can see that the performance gap between usingrandom/topklabels (within the label space) and using random English words is signiﬁcant. Setting 2 improves ArgC F1 by about 0.65% and 2.5% over “random labels” and no demonstrations. This indicates that conditioning on the label space can alleviate the difﬁculty of learning the complex event pattern, which is why performance improves. Argument span prediction accuracy. Argument span prediction accuracy in Table 2illustrates the ArgI precision of both datasets. As expected, Setting 1 identiﬁes the argument span more accurately than Setting 2, and the gap in predictionaccuracy is as large as 25.5%. This indicates that indistribution demonstration contexts can improve the ability to recognize argument spans and contribute to performance gains. Argument role prediction accuracy. We also evaluate the capability to generate golden argument role in target sequence. From Table 2we can see that Setting 2 generates role labels more accurately than Setting 1, and the gap in prediction accuracy is 14.6%. This suggests that schemaconsistency retrieval alleviates the difﬁculty of learning the complex event pattern, and conditioning on the label space contributes to better performance. Impact of the number of demonstrations k. Figure 5illustrates how the hyperparameters k(a) RAMS (b) WikiEvents Figure 6: ArgC F1 scores with different training data ratios on both benchmarks. affect the extraction performance. We observe that gradually increasing the number of demonstrations signiﬁcantly improves ArgC F1 in RAMS, but not in WikiEvents. We conjecture that the reason is that the averaged context length (about 900 words) in WikiEvents is too long, which affects the original input representation in the crossattention module. 3.4 Fewshot Setting To conduct detailed comparisons between different augmentation methods, we asymptotically increase the training data to analyze the performance ofthem on both datasets. Figure 6shows the performance of them and T5baseline with partialtraining samples. It demonstrates our approach achieves comparable performance with the T5baseline model with only ~20% of training data, which indicates that our approach has great potential to achieve good results with very few data. 4 Related Work Documentlevel Event Argument Extraction The goal of documentlevel EAE is to extract arguments from the whole document and assign them to right roles. On the task level, most of these works fall into three categories: (1) multilabel classiﬁcationbased models (2) QAbased models (3) generationbased models. Speciﬁcally, Zhang et al. (2020 );Xu et al. (2021 );Huang and Jia (2021 );Ren et al. (2022 );Ma et al. (2022 );Xu et al.300(2022 ) ﬁrst identiﬁed argument spans and then ﬁll each with a speciﬁc role via multilabel classiﬁcation; Du and Cardie (2020 );Liu et al. (2021 );Wei et al. (2021 ) formulated documentlevel EAE as an question answering (QA) or machine reading comprehension (MRC) problem; Li et al. (2021 ) designed speciﬁc templates for each event type and frames EAE as conditional generation. Above methods conduct experiments on WikiEvents ( Li et al. ,2021 ), RAMS ( Ebner et al. ,2020 ), and Chinese ﬁnancial dataset ( Zheng et al. ,2019 ). RetrievalAugmented Text Generation RAG has recently been successfully applied to manyNLP tasks, e.g., dialogue response generation,machine translation, and information extraction. These methods retrieve additional knowledge from various corpora to augment text generation, which includes three major components: the retrieval source, retrieval strategy, and integration methods. Meanwhile, leveraging additional knowledge as the augmentation signal is a natural way to resolve the information insufﬁciency issue for information extraction. For example, Lee et al. (2022 ) proposed two demonstration retrieval methods for named entity recognition. Zhang et al. (2021 ) used the opendomain knowledge in Wikipedia as retrieval source for distantly supervised relation extraction. Du and Ji (2022 ) applied SBERT ( Reimers and Gurevych ,2019 ) to retrieve the most relevant example for event extraction. 5 Conclusion In this paper, we explore how to design retrievalaugmented strategy for documentlevel EAE from the input and label distribution views. And our introduced retrieval strategies can recall demonstrations that can be helpful to demonstrate howthe model should solve the task. We further propose a novel adaptive hybrid retrieval augmentation paradigm to generate the reference vectors as depth cues to improve the analogical capability ofmodel. Through extensive experiments on RAMS and WikiEvents datasets, we demonstrate the validity of our newly introduced retrievalaugmented models. In the future, we plan to adapt our experiments on various tasks demonstrate the strong performance of WeCheck, achieving an average absolute improvement of 3.3%on the TRUE benchmark over 11B stateof-theart methods using only 435M parameters. Furthermore, it is up to 30× faster than previous evaluation methods, greatly improving the accuracy and efficiency of factual consistency evaluation.1 1  evaluation models on synthetic texts or directly transfer from other related tasks, such as question answering (QA) and natural language inference (NLI). Bias in synthetic text or upstream tasks makes them perform poorly on text actually generated by language models, especially for general evaluation for various tasks. To alleviate this problem, we propose a weakly supervised framework named WeCheck that is directly trained on actual generated samples from language models with weakly annotated labels. WeCheck first utilizes a generative model to infer the factual labels of generated samples by aggregating weak labels from multiple resources. Next, we train a simple noiseaware classification model as the target metric using the inferred weakly supervised information. Comprehensive experiments on various tasks demonstrate the strong performance of WeCheck, achieving an average absolute improvement of 3.3%on the TRUE benchmark over 11B stateof-theart methods using only 435M parameters. Furthermore, it is up to 30× faster than previous evaluation methods, greatly improving the accuracy and efficiency of factual consistency evaluation.1 1 Analysis To analyse how each module and settings work, we conduct analysis experiments on each module and settings of WeCheck. Training Mechanism We first study how the mechanisms proposed in §2 affect the overall framework by removing or replacing them. The results are reported in Table 2. Most important of all, by removing the NLIwarmup before weak supervision training, the performance drops significantly on each task and drops an average of 19.3% on each dataset. This proves that NLI, as an easier312Sum. Dial. Para. Ave WeCheck 83.0 86.2 89.6 84.8 w/oNLIwarmup 67.8 75.7 50.7 68.5 w/oNoise Filter 81.6 85.3 78.2 83.7 w/Hard Label 82.8 86.0 89.5 84.6 Table 2: Ablation study of different settings of WeCheck on summarization (Sum.), dialogue (Dial.) and paraphrase (Para.). Sum. Dial. Para. Sum. Dial. Para. Ave 77.2 85.4 85.4 80.3 ✓ 83.4 85.2 89.2 84.6 ✓ 72.7 84.2 84.2 77.8 ✓ 77.2 86.7 92.1 81.8 ✓ ✓ ✓ 83.0 86.2 89.6 84.8 Table 3: Analysis on the effects of different task data. The left block indicates whether using a type of task data while the right block is the corresponding performance. and closely related task, provides a much better initialization for training with weak supervision. For noiseaware finetuning, we study how filtering potential noisy samples (Eq. 7) and the probabilistic label (Eq. 6) affect the overall performance. After removing noise filtering (w/o Noise Filter in Table 2), the performance drops around 12 points in each task and dataset in average. By replacing the probabilistic labels into hard labels (w/ Hard Label in Table 2), we observe around 0.10.2 drops in performance. This implies how to filter potential noisy samples is crucial in noise aware finetuning, and probabilistic labels also slightly help. Effects of Task We also analyse how each bootstrapped task affect WeCheck. In Table 3, the left block rows indicate whether a type of task samples are used for training, and the right block rows are the corresponding performance. The first row is the results of NLIwarmup which does not use any task data for training. The second to forth rows separately train on summarization, dialogue, and paraphrase examples. The last row reports the default settings of WeCheck, which jointly train with all three task samples. From the results, we can conclude that, joint training on all tasks leads to a better performance on the comprehensive evaluation across tasks. For single task evaluation except dialogue, training using only the target task examples leads to better performance on this task than joint training. In horizontal comparisons of single taskperformance, we observe that summarization examples contribute most to the overall performance, improving the performance of checking summarization and paraphrase by 6.2 and 3.8 points. Paraphrase examples benefit evaluating paraphrase and dialogue by 6.7 and 1.3 points. Dialogue samples worsen the performance of WeCheck. We suppose that is because these samples are boostrapped from relative weak dialogue models, MemNet and dodecaDialogue, which are not even pretrained models. Thus, dialogue samples have no contributions to NLIwarmup. By contrast, the summarization samples, which are the most difficult type for checking, benefit most to the overall performance. Computational Efficiency We analyze the computational efficiency of WeCheck by comparing with other metrics based on different architectures. As reported in Table 4, we select three other representative metrics: SC ZSbased on sentencelevel NLI, FactCC based on documentlevel NLI, and QAFact based on QAQG framework. All these methods are tested on the TRUE benchmark with a single NVIDIA 32G V100 GPU and we report the relative time cost of each experiments further verify the significance of our model and the AMR. 2 Parsed Structures We perform some experiments and discussions for the characteristics of AMR compared to parsing structures already used for the ABSA task and how these characteristics affect our APARN . Humandefined Structures Dependency trees and AMRs are parsed based on humandefined syntactic and semantic rules, respectively. Each word in a sentence becomes a node of the dependency tree, but in the AMR, relational words like function words and auxiliary words are represented as edges, while concept words like nouns and verbs are refined into nodes in the graph. With AMR aligning, we can map concept words in sentences to nodes in the graph and establish relations between them, while relation words are isolated. To estimate the impact of dependency trees and AMRs in the ABSA task, we calculate the average distance between aspect words and opinion words in different parsed structures on the Restaurant dataset, called aspectopinion distance (AOD). We also calculate the average distance between aspect words and all context words called aspectcontext distance (ACD), and divide AOD by ACD as relative aspectopinion distance (rAOD). The distance between aspect words and isolated words323is treated as sentence length. According to the result shown in Table 1, both dependency trees and AMRs have similar AOD smaller than original sentences, which indicates their benefits to capture relations about aspects. Due to the elimination of isolated words, the rAOD of AMRs is much less than dependency trees, which means smaller scope and easier focus. About 2.13 %of opinion words are wrongly isolated, making the AOD of AMR (all words) a little bigger. But this is acceptable considering the improvement of rAOD and partially repairable by information from original sentences. The above analysis is for graph skeletons, and we also explore the impact of edge labels of two structures in the ABSA task. Figure 2 compares the distribution of edge labels in aspectopinion paths with the distribution of all edge labels. These distributions are clearly different, both in dependency trees and AMRs, which implies that edge labels can also help the ABSA task, especially in AMRs. Based on these characteristics, we design the outer product sum module for APARN to mix sentence information into the graph, and design the path aggregator to collect graph skeleton and edge label information in AMRs. Datadriven Structures Some existing studies use structures produced by datadriven models in the ABSA task (Chen et al., 2020; Dai et al., 2021; Chen et al., 2022) and exhibit different effects from humandefined structures. Therefore, we design a relationenhanced selfattention mechanism for APARN to integrate the graph information obtained by the path aggregator with the information from the pretrained model. 3 Proposed Model The overall architecture of our proposed model APARN is illustrated in Figure 3. It consists of 3 parts: AMR preprocessing, path aggregator and relationenhanced selfattention mechanism. In the ABSA task, a sentence s={w1, w2, ..., w n}and a specific aspect term a={a1, a2, ..., a m}are given to determine the corresponding sentiment polarity class ca, where ais a subsequence of sandca∈ {Positive, Neutral, Negative }. Many existing works use syntactic dependency trees to establish explicit or implicit connections between aspects and contexts. However, we believe that the sentiment analysis task is essentially about the meanings of sentences, so semantic structures like AMRs are more favorable for this task.In addition, AMRs are more concise than dependency trees, making it easier to extract valuable information in training but more difficult to preprocess before training. We have to conduct a series of steps including: AMR parsing, AMR aligning and AMR embedding. Preprocessed AMRs still have errors and unsuitable parts for the task, so we design the path aggregator and the relationenhanced selfattention mechanism to perform joint representation refinement and flexible feature fusion on the AMR graph and the original sentence. Next, we elaborate on the details of our proposed APARN , including AMR preprocessing and embedding, the path aggregator and the relationenhanced selfattention mechanism. 3.1 AMR Preprocessing and Embedding Parsing As we determine to employ the semantic structure AMR as an alternative of the syntactic structure dependency tree to better perform the semantic task ABSA, the first step is parsing the AMR from the input sentence. We choose the offthe-shelf parser SPRING (Bevilacqua et al., 2021) for high quality AMR outputs. Aligning Next, we align the AMR by the aligner LEAMR (Blodgett and Schneider, 2021). Based on the alignments, we manage to rebuild AMR relations between words in the sentence and get the transformed AMR with words as nodes. Embedding After aligning, we now have transformed AMRs, which can also be called sentences with AMR relations. Then we need to obtain their embeddings for later representation learning by the model. For words in the sentence, also as the nodes in the AMR, we utilize BERT as an encoder to get contextual embeddings H={h1, h2, ..., h n}like lots of previous works. For the edges in the AMR, we represent the relations between nodes as an adjacency matrix R={rij|1≤i, j≤n}, where rij is the embedding of the edge label between word wiand word wj. If there is no edge between wiand wjin the AMR, we assign a “none” embedding to rij. Edge label embeddings are also obtained from the pretrained model. 3.2 Path Aggregator Path aggregator receives the mix of AMR embeddings R∈Rdr×n×nand sentence embeddings H∈Rdw×n, where dranddwdenote the dimensions of relation and word embeddings, respectively. Path aggregator outputs the relational fea324But the staffwas so horrible BERT Encoder contrast -01 horrible person so staff01ARG2 ARG2ofdomain degreeBut the staff so horriblewasARG2of domain degree ARG2But the staff was so horribleBut the staff was so horrible But the staff was so horrible 𝜎 𝑊ଵௗೢ×ௗೝ 𝑊ொௗೢ×ௗೢ 𝑊௄ௗೢ×ௗೢ 𝑊௏ௗೢ×ௗ౭𝑊ଶௗೢ×ௗೝ dom aindeg reeARG 2 ARG 2of 𝑑௪ 𝑛௛௘௔ௗ𝑛௛௘௔ௗ √ Negative Neutral Positive But so horrible degreeARG2But soPath Aggregator RelationEnhanced SelfAttention𝑑୰ 𝑹 𝑯𝑨𝑨𝑮𝑮 𝑨𝑹𝑨 : Elementwise Sum : Outer ProductAMR Aligning AMR Parsing AMR PreprocessingFigure 3: The overall architecture of APARN . ture matrix RAGG={rAGG ij∈Rdr|1≤i, j≤n}. This process integrates and condenses information from two different sources, AMRs and sentences, making semantic knowledge more apparent but parsing errors less influential. Outer Product Sum We first add the outer product of two independent linear transformation of sentence embeddings Hto the original AMR embeddings Rto obtain sequenceenhanced relation embeddings RS∈Rdr×n×n. On the one hand, as the outer product of His the representation of word relations from the sentence perspective, its combination with the AMR embeddings Rcould enlarge the information base of the model to improve the generalization, also cross validate important features to improve the reliability. On the other hand, AMR embeddings Ris usually quite sparse. The outer product sum operation ensures the basic density of the feature matrix and facilitates the subsequent representation learning by avoiding the fuzziness and dilution of numerous background “none” relations to the precious effective relations. Path Aggregation Next, we perform the path aggregation on RS={rS ij|1≤i, j≤n}to calculate RAGG={rAGG ij|1≤i, j≤n}as: r′S ij= LayerNorm( rS ij), (1) gin ij, gout ij= sigmoid(Linear( r′S ij)), (2) aij, bij=gin ij⊙Linear( r′S ij), (3) rout ij= Linear(LayerNorm(/summationdisplay kaik⊙bkj)),(4)rAGG ij =gout ij⊙rout ij. (5) The path aggregation has distinctive effect on both local and global dissemination of features. From the local view, the path aggregation covers all the 2hop paths, so that it is very sensitive to neighborhood features, including the features around the aspect term which are really important for the ABSA task. From the global view, information in any long path can be summarized into the representation between the start and the end by several twoin-one operations in enough times of path aggregations. In other words, path aggregations make the features in matrix more inclusive and finally attain global features. In practice, because the ABSA task focuses more on the neighboring information and the BERT encoder with attention mechanisms has made the feature comprehensive enough, a single path aggregation can achieve quite good results. Additionally, we also introduce a gating mechanism in the path aggregation to alleviate the disturbance of noise from insignificant relations. Finally, the output of path aggregation RAGGis transformed into the relational attention weight matrix AAGG={aAGG ij|1≤i, j≤n}by a linear transformation for subsequent calculation. 3.3 RelationEnhanced SelfAttention The classic selfattention (Vaswani et al., 2017) computes the attention weight by this formula: A=softmax/parenleftbiggQWQ×(KW K)T √ d/parenrightbigg ,(6) where QandKare input vectors with ddimensions, while WQandWKare learnable weights325ModelRestaurant Laptop Twitter MAMS Accuracy MacroF1 Accuracy MacroF1 Accuracy MacroF1 Accuracy MacroF1 BERT (Devlin et al., 2019) 85.62 78.28 77.58 72.38 75.28 74.11 80.11 80.34 DGEDT (Tang et al., 2020) 86.30 80.00 79.80 75.60 77.90 75.40 - - RGAT (Wang et al., 2020) 86.60 81.35 78.21 74.07 76.15 74.88 - - TGCN (Tian et al., 2021) 86.16 79.95 80.88 77.03 76.45 75.25 83.38 82.77 DualGCN (Li et al., 2021) 87.13 81.16 81.80 78.10 77.40 76.02 - - dotGCN (Chen et al., 2022) 86.16 80.49 81.03 78.10 78.11 77.00 84.95 84.44 SSEGCN (Zhang et al., 2022b) 87.31 81.09 81.01 77.96 77.40 76.02 - - APARN (Ours) 87.76 82.44 81.96 79.10 79.76 78.79 85.59 85.06 Table 2: Results on four public datasets. Best performed baselines are underlined . All models are based on BERT. with the same size of Rd×d. In our relationenhanced selfattention, we added AAGG, the relational attention weight matrix from AMR into the original attention weight, which can be formulated as: AR=softmax/parenleftbiggHW Q×(HW K)T √dw+AAGG/parenrightbigg ,(7) where input vectors WandQare both replaced by the BERT embeddings Hwithdwdimensions. WithAAGG, attention outputs are further guided by the semantic information from AMRs, which improves the efficient attention to semantic keywords. In addition, similar to path aggregator, we also introduced the gating mechanism into the relationenhanced selfattention as follows: G=sigmoid (HW G), (8) HR= (HW V)AR⊙G, (9) where WGandWVare trainable parameters and Gis the gating matrix. Considering the small proportion of effective words in the whole sentence, the gating mechanism is conducive to eliminating background noise, making it easier for the model to focus on the more critical words. Finally, with all these above calculations including relationenhanced selfattention and gating mechanism, we obtain the relationenhanced aspect representation HR a={hR a1, hR a2, ..., hR am}for subsequent classification. 3.4 Model Training The final classification features are concatenated by the original BERT aspect representation Ha=mean{ha1, ha2, ..., h am}and the relationenhanced aspect representation HR a. Hfinal a = [Ha, HR a]. (10)It is passed through a fully connected softmax layer and mapped to probabilities over three sentiment polarities. p(a) =softmax (WpHfinal a +bp). (11) We use crossentropy loss as our objective function: LCE=−/summationdisplay (s,a)∈D/summationdisplay c∈Cyc alogpc(a), (12) where yis the ground truth sentiment polarity, D contains all sentenceaspect pairs and Ccontains all sentiment polarities. 4 Experiments In this section, we first introduce the relevant settings of the experiments, including the datasets used, implementation details and baseline methods for comparison. Then, we report the experimental results under basic and advanced settings. Finally, we select several representative examples for model analysis and discussion. 4.1 Datasets and Setup Our experiments are conducted on four commonly used public standard datasets. The Twitter dataset is a collection of tweets built by Dong et al. (2014), while the Restaurant and Laptop dataset come from the SemEval 2014 Task (Pontiki et al., 2014). MAMS is a largescale multiaspect dataset provided by Jiang et al. (2019). Data statistics are shown in Appendix A.1. In data preprocessing, we use SPRING (Bevilacqua et al., 2021) as the parser and LEAMR (Blodgett and Schneider, 2021) as the aligner. APARN uses the BERT of bertbase-uncased version with max length as 100 and the relationenhanced selfattention mechanism uses 8 attention heads. We reported accuracy and MacroF1 as results which326are the average of three runs with different random seeds. See Appendix A.2 for more details. 4.2 Baseline Methods We compare APARN with a series of baselines and stateof-theart alternatives, including: 1)BERT (Devlin et al., 2019) is composed of a general pretrained BERT model and a classification layer adapted to the ABSA task. 2)DGEDT (Tang et al., 2020) proposes a dual transformer structure based on dependency graph augmentation, which can simultaneously fuse representations of sequences and graphs. 3)RGAT (Wang et al., 2020) proposes a dependency structure adjusted for aspects and uses a relational GAT to encode this structure. 4)TGCN (Tian et al., 2021) proposes an approach to explicitly utilize dependency types for ABSA with typeaware GCNs. 5)DualGCN (Li et al., 2021) proposes a dual GCN structure and regularization methods to merge features from sentences and dependency trees. 6)dotGCN (Chen et al., 2022) proposes an aspectspecific and languageagnostic discrete latent tree as an alternative structure to dependency trees. 7)SSEGCN (Zhang et al., 2022b) proposes an aspectaware attention mechanism to enhance the node representations with GCN. 4.3 Main Results Table 2 shows the experimental results of our model and the baseline models on four datasets under the same conventional settings as Li et al. (2021), where the best results are in bold and the second best results are underlined. Our APARN exhibits excellent results and achieves the best results on all 8 indicators of 4 datasets with an average margin more than one percent, which fully proves the effectiveness of this model. Comparing the results of different datasets, we can find that the improvement of APARN on the Twitter dataset is particularly obvious. Compared to the best baselines, the accuracy rate has increased by 1.65 %and the MacroF1 has increased by 1.79 %. The main reason is the similarity of the Twitter dataset to the AMR 3.0 dataset, the training dataset for the AMR parser we used. More than half of the corpus of the AMR 3.0 dataset comes from internet forums and blogs, which are similar to the Twitter dataset as they are both social media. As a result, the AMR parser has better output on the Twitter dataset, which in turn enables the model to /uni00000027/uni00000036/uni00000027/uni00000038/uni00000034/uni00000011/uni00000027/uni00000033/uni00000038 /uni00000027/uni00000036/uni00000027/uni00000038/uni00000034/uni00000011/uni0000002a/uni0000002b/uni00000036 /uni0000003a/uni00000013/uni0000002d/uni00000029/uni00000034/uni00000011/uni00000027/uni00000033/uni00000038 /uni0000003a/uni00000013/uni0000002d/uni00000029/uni00000034/uni00000011/uni0000002a/uni0000002b/uni00000036/uni0000001d/uni00000016/uni0000001d/uni00000018/uni0000001d/uni0000001a/uni0000001d/uni0000001c/uni0000001d/uni0000001e/uni0000001e/uni00000016/uni0000001e/uni00000018/uni00000027/uni00000049/uni00000049/uni0000005b/uni00000058/uni00000047/uni00000049/uni0000005f/uni0000001d/uni0000001c/uni00000014/uni0000001f/uni0000001b /uni0000001d/uni0000001c/uni00000014/uni00000017/uni0000001c /uni0000001d/uni0000001b/uni00000014/uni0000001b/uni00000018/uni0000001d/uni0000001b/uni00000014/uni0000001c/uni0000001e/uni0000001d/uni0000001f/uni00000014/uni0000001d/uni0000001c /uni0000001d/uni0000001d/uni00000014/uni00000017/uni00000016/uni0000001d/uni0000001d/uni00000014/uni00000019/uni0000001a /uni0000001d/uni0000001c/uni00000014/uni0000001a/uni0000001b/uni0000005d/uni00000015/uni00000055/uni00000003/uni0000004b/uni0000004a/uni0000004d/uni0000004b/uni00000003/uni00000052/uni00000047/uni00000048/uni0000004b/uni00000052/uni00000059 /uni0000005d/uni00000014/uni00000003/uni0000004b/uni0000004a/uni0000004d/uni0000004b/uni00000003/uni00000052/uni00000047/uni00000048/uni0000004b/uni00000052/uni00000059Figure 4: Accuracy of APARN and TGCN on Twitter dataset with different parsed structures and edge labels. extract more valuable features from it and leads to a considerable improvement. This difference among datasets also reflects the effectiveness of semantic information from AMR for the ABSA task. 4.4 Comparative Experiments We conduct comparative experiments to analyse the impact of models ( APARN and TGCN), parsed structures (AMR and dependency tree), and edge labels (with and without). TGCN is selected instead of more recent models because they lack the ability to exploit edge labels and cannot receive AMRs as input. AMRs are the same as the basic experiments and dependency trees are parsed by Stanford CoreNLP Toolkits (Manning et al., 2014). “Without edge labels” means all labels are the same placeholder. The results are shown in Figure 4. From the perspective of models, APARN consistently outperforms TGCN in any parsed structure and edge label settings, demonstrating the effectiveness of our APARN . From the perspective of parsed structures, AMRs outperform dependency trees in most model and edge label settings, except for the case of TGCN without edge labels. The reason may be that the AMR without edge labels is sparse and semantically ambiguous, which does not match the design of the model. From the perspective of edge labels, a graph with edge labels is always better than a graph without edge labels, whether it is an AMR or a dependency tree, whichever the model is. We can also notice thatAPARN has a greater improvement with the addition of edge labels, indicating that it can utilize edge labels more effectively. Besides, with the addition of edge labels, experiments using AMR have improved more than experiments using depen327ModelRestaurant Laptop Twitter MAMS Accuracy MacroF1 Accuracy MacroF1 Accuracy MacroF1 Accuracy MacroF1 APARN 87.76 82.44 81.96 79.10 79.76 78.79 85.59 85.06 −Outer Product Sum 86.15 80.13 79.45 76.34 76.22 74.75 82.93 82.30 −Path Aggregator 87.04 81.61 79.20 75.67 76.66 74.90 83.16 82.61 −Relation in SelfAttention 87.49 81.82 80.36 77.87 76.81 75.49 83.73 83.08 −Gate in SelfAttention 85.61 78.49 79.81 77.42 77.55 76.06 83.96 83.15 Table 3: Ablation experimental results of our APARN . dency trees, indicating that edge labels of the AMR contain richer semantic information and are more valuable for sentiment analysis, which is consistent with previous experiments in Figure 2. 4.5 Further Analysis Ablation Study To analyze the role of each module, we separately remove four key components of APARN . Results on four datasets are represented in Table 3. According to the results, each of the four components contributes significantly to the performance ofAPARN . Removing Outer Product Sum results in a significant drop in performance, illustrating the importance of promoting consistency of information from sentences and AMRs. Removing Path Aggregator is worse than removing Relation in SelfAttention, indicating that unprocessed AMR information can only interfere with the model instead of being exploited by the model. Comparing the results in different datasets, we can find that the model depends on information from sentences and AMRs differently on different datasets. On the Restaurant dataset, removing the Relation in SelfAttention component has less impact, while on the Twitter dataset, removing this component has a greater impact. This means the model utilizes sentence information more on the Restaurant dataset and AMR information more on the Twitter dataset. This is also consistent with the analysis of the main results: the AMR of Twitter dataset has higher quality due to the domain relatedness with the training dataset of the AMR parser, which in turn makes the model pay more attention to the information from the AMR on this dataset. AMR Parser Analysis We conduct experiments using AMRs from different parsers on Twitter dataset, as displayed in Figure 5. In addition to the SPRING parser mentioned before, we try two other parsers from Zhang et al. (2019b) and Cai and Lam (2020). These parsers achieve 76.3, 80.2 75 77 79 81 83 85 AMR Parsing Smatch7677787980ABSA AccuracyZhang et al. (2019b)Cai and Lam (2020)SPRINGFigure 5: Accuracy of APARN on Twitter dataset with AMR from different parsers. Sentence Length <15 1524 2534 >35 w/o Path Aggregator 88.25 85.43 83.92 83.96 w. Path Aggregator 89.40 87.15 86.64 86.71 Relative Improvement +1.30 %+2.01 %+3.24 %+3.28 % Table 4: Accuracy of APARN with and without path aggregator for sentences of different lengths in the Restaurant dataset. and 84.3 Smatch score for AMR parsing task on AMR 2.0 dataset, which can be regarded as the quality of their output. From the figure, it is clear that the accuracy of ABSA task shows positive correlation with the Smatch score, which proves the positive effect of AMRs in the ABSA task and the importance of the high quality AMR. Sentence Length Study Table 4 compares the accuracy of APARN with and without path aggregator for sentences of different lengths in the Restaurant dataset. According to the table, we can see that the model achieves higher accuracy on short sentences, while the long sentences are more challenging. In addition, the model with the path aggregator has a larger relative improvement on long sentences than short sentences, indicating that the path aggregator can effectively help the model capture longdistance relations with AMR.328the atmosphere was crowded but it was a great bistrotype vibe BERT +AMR so if you want a nice ， enjoyable meal at montparnasse ， go early for the pretheater prixfixe BERT +AMRi ordered the smoked salmon and roe appetizer and it was off flavor BERT +AMRFigure 6: Visualization of aspect terms’ attention to the context in three cases. Aspect terms are highlighted in blue. 4.6 Case Study As shown in Figure 6, we selected three typical cases to visualize the aspect terms’ attention to the context before and after adding information from the AMR, respectively. From the first two examples, we can notice that the model focuses on the copula verb next to the opinion term without the AMR. While with the information from the AMR, the model can capture opinion terms through the attention mechanism more accurately. In the third example, without the AMR, the model pays more attention to words that are closer to the aspect term. With the semantic information from AMR, the model can discover opinion terms farther away from aspect terms. These cases illustrate that the semantic structure information of AMR plays an important role in making the model focus on the correct opinion words. It also shows that the structure of our APARN can effectively utilize the semantic structure information in AMR to improve the performance in the ABSA task. 5 Related Work Aspectbased Sentiment Analysis Traditional sentiment analysis tasks are usually sentencelevel or documentlevel, while the ABSA task is an entitylevel and finegrained sentiment analysis task. Early methods (Jiang et al., 2011; Kiritchenko et al., 2014) are mostly based on artificially constructed features, which are difficult to effectively model the relations between aspect terms and its context. With the development of deep neural networks, many recent works (Wang et al., 2016; Tang et al., 2016; Chen et al., 2017; Fan et al., 2018; Gu et al., 2018; Du et al., 2019; Liang et al., 2019; Xing et al., 2019) have explored applying attention mechanisms to implicitly model the semantic relations of aspect terms and identify the key opinionterms in the context. Another trend in ABSA studies is the explicit use of dependency trees. Some works (He et al., 2018; Zhang et al., 2019a; Sun et al., 2019; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020; Phan and Ogunbona, 2020; Li et al., 2021; Xiao et al., 2021) extend GCN, GAT, and Transformer backbones to process syntactic dependency trees and develop several outstanding models. These models shorten the distance between aspect terms and opinion terms by dependency trees and alleviate the longterm dependency problem. Recent studies have also noticed the limitations of dependency trees in the ABSA task. Wang et al. (2020) proposes the reshaped dependency tree for the ABSA task. Chen et al. (2020) propose to combine dependency trees with induced aspectspecific latent maps. Chen et al. (2022) further proposed an aspectspecific and languageindependent discrete latent tree model as an alternative structure for dependency trees. Our work is similar in that we also aim at the mismatch between dependency trees and the ABSA task, but different in that we introduce a semantic structure AMR instead of induced trees.   analysis (ABSA) is a finegrained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantictask. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named experiments, we prove that the proposed text adversarial purification algorithm can successfully serve as defense against strong attacks such as Textfooler and BERTAttack. Experiment results show that the accuracy under attack in baseline defense methods is lower than random guesses, while after text purification, the performance can reach only a few percent lower than the original accuracy when the candidate range of the attack is limited. Further, extensive results indicate that the candidate range of the attacker score is essential for successful attacks, which is a key factor in maintaining the semantics of the adversaries. Therefore we also recommend that future attacking methods can focus on achieving successful attacks with tighter constraints. To summarize our contributions: (1) We raise the concern of defending substitutionbased adversarial attacks without acknowledging the form of the attacks in NLP tasks. (2) To the best of our knowledge, we are the first to consider adversarial purification as a defense against textual adversarial attacks exemplified by strong wordsubstitution attacks and combine text adversarial purification with pretrained models.(3) We perform extensive experiments to demonstrate that the adversarial purification method is capable of defending strong adversarial attacks, which brings a new perspective to defending textual adversarial attacks. 2 Related Work 2.1 Adversarial Attacks in NLP In NLP tasks, current methods use substitutionbased strategies (Alzantot et al., 2018; Jin et al., 2019; Ren et al., 2019) to craft adversarial examples. Most works focus on the scorebased blackbox attack, that is, attacking methods know the logits of the output prediction. These methods use different strategies (Yoo et al., 2020; Morris et al., 2020b) to find words to replace, such as genetic algorithm (Alzantot et al., 2018), greedysearch (Jin et al., 2019; Li et al., 2020) or gradientbased methods (Ebrahimi et al., 2017; Cheng et al., 2019) and get substitutes using synonyms (Jin et al., 2019; Mrkši ´c et al., 2016; Ren et al., 2019) or language models (Li et al., 2020; Garg and Ramakrishnan, 2020; Shi et al., 2019). 2.2 Adversarial Defenses We divide the defense methods for wordsubstitution attacks by whether the defense method requires knowledge of the form of the attack. When the candidate list is known, recent works introduce defense strategies that incorporate the candidates of the words to be replaced as an augmentation. Jin et al. (2019); Li et al. (2020); Si et al. (2020) uses generated adversaries to augment the classifier for better defense performances; Jia et al. (2019); Huang et al. (2019) introduce a certified robust model to construct a certified space within the range of a candidate list therefore the substitutions in the candidate list cannot perturb the model. Zhou et al. (2020); Dong et al. (2021) construct a convex hull based on the candidate list which can resist substitutions in the candidate list. To defend unknown attacks, NLP models can incorporate gradientbased adversarial training strategies (Miyato et al., 2016; Madry et al., 2019) since recent works (Ebrahimi et al., 2017; Cheng et al., 2019; Zhu et al., 2019; Li and Qiu, 2020) show that gradientbased adversarial training can also improve defense performances against wordsubstitution attacks.339Adversarial Image (Gibbon) Purified Image (Panda) a continuous image reconstruct process from a noisy image e.g. using Diffusion Models/ Scorebased Models I possess the good common logical sense to realize... it is something I like... Adversarial Text (Negative) ... I [MASK] the good common logical sense to [MASK]... it is something I like... I possess the good common logical [MASK] to realize... it is [MASK] I like... ... ... MLMI know the good common logical sense to sense... it is something I like... Purified TextsI possess the good common logical feel to realize... it is something I like... (Positive) I possess the good common logical [MASK] to [MASK]... it is something I like... I possess the good common logical feel to realize... it is something I like... Figure 1: Text Adversarial Purification Process: Compared with Image Purification, we use masked language models to recover noisy texts to purify adversarial texts as a defense against wordsubstitutions attacks. 2.3 Adversarial Purification Adversarial purification is a defense strategy that uses generative models to purify adversarial inputs before making predictions, which is a promising direction in adversarial defense. Samangouei et al. (2018) uses a defensive GAN framework to build clean images to avoid adversarial attacks. Energybased models (EBMs) are used to purify attacked images via Langevin dynamics (LeCun et al., 2006). Scorebased models (Yoo et al., 2020) is also introduced as a purification strategy. Recent works focus on exploring diffusion models as the purification model in purifying the attacked images (Nie et al., 2022). Though widely explored, adversarial purification strategy is less explored in the NLP field. 3 Text Adversarial Purification 3.1  evaluation defined by Si et al. (2020). The afterattack accuracy measures the actual defense ability of the system under adversarial attacks. 1https://datasets.imdbws.com/ 2https://www.kaggle.com/amananandrai/agnews- classificationdataset4.3 Victim Models and Defense Baselines The victim models are the finetuned pretrain models exemplified by BERT and RoBERTa, which we implement based on Huggingface Transformers3 (Wolf et al., 2020). As discussed above, there are few works concerning adversarial defenses against attacks without knowing the candidates in NLP tasks. Moreover, previous works do not focus on recent strong attack algorithms such as Textfooler (Jin et al., 2019), BERTinvolved attacks (Li et al., 2020; Garg and Ramakrishnan, 2020) Therefore, we first list methods that can defend against adversarial attacks without accessing the candidate list as our baselines: AdvTrain (AdvHotFlip) : Ebrahimi et al. (2017) introduces the adversarial training Analysis 4.6.1 Ablations As we design an adversarial purification algorithm with masked language models and propose a multiplerecovering strategy, we aim to explore which process helps more in the purification defense system. Plus, we combine classifiers within the purification model so it is also important to explore whether such a combination is helpful. For each type of purification Experiments on two uniﬁedannotated event datasets indicate that S PEECH is predominant in event detection and eventrelation extraction tasks. 1   Analysis 5.1 Analysis On EnergyBased Modeling We list some values of energy loss deﬁned in Eq (5), (8)and(10) when training respectively for token, sentence and document, as presented in Figure 3. The values of tokenlevel energy loss are observably larger than those at the sentence and document levels. This can be attributed to the fact that the energy loss is related to the quantity of samples, and a single document typically contains much more tokens than sentences or sentence pairs. All three levels of energy loss exhibit a gradual decrease over the course of training, indicating that SPEECH , through energybased modeling, effectively minimizes the discrepancy between predicted results and ground truth. The energy functions for token, sentence and document deﬁned in Eq (4),(7)and (9), reﬂect that the implementation of energybased modeling in SPEECH is geared towards enhancing compatibility between input/output pairs. The graduallydecreasing energy loss demonstrates that SPEECH can model intricate event structures at the token, sentence, and document levels through energybased optimization, thereby improving the outcomes of structured prediction. 0 5 10 15 200255075100125150175200Loss for Energy tokenlevel 0 5 10 15 20 Iteration Step02468Loss for Energy sentencelevel documentlevel Figure 3: Illustration of loss for energy. 5.2 Case Study: EnergyBased Hyperspheres As seen in Figure 4, we visualize the event class embedding of “Attack” and 20 event mention embeddings as generated by both SPEECH and SPEECH without energy functions. We observe thatforSPEECH with energybased modelling, the instances lie near the surface of the corresponding hypersphere, while they are more scattered when not equipped with energybased modeling, which subsequently diminishes the performance of event classiﬁcation. This observation suggests that SPEECH derives signiﬁcant beneﬁts from modeling with energybased hyperspheres. The visualization results further demonstrate the effectiveness of SPEECH equipped with energybased modeling. 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.001.00 0.75 0.50 0.25 0.000.250.500.751.001.0 0.5 0.00.51.0 SPEECH SPEECH w/o energy Figure 4: Visualization of an example event class. 5.3 Error Analysis We further conduct error analysis by a retrospection of experimental results and datasets. (1) One typical error relates to the unbalanced data distribution. Considering every event type and eventrelation contain different amount of instances, uniﬁed modeling with energybased hyperspheres may not always be impactful. (2) The second error relates to the overlapping event mentions among event types, meaning that the same sentence may mention multiple event types. As ONTOEVENT -DOCcontains many overlappings, it might be the reason for its mediocre performance on ED. (3) The third error relates to associations with eventcentric structured prediction tasks. As trigger classiﬁcation is closely related to event classiﬁcation, wrong prediction of tokens will also inﬂuence classifying events. 6 Conclusion and Future Work In this paper, we propose a novel approach entitled SPEECH to tackle eventcentric structured prediction with energybased hyperspheres. We represent event classes as hyperspheres with token, sentence and documentlevel energy, respectively for trigger classiﬁcation, event classiﬁcation and event relation extraction tasks. We evaluate SPEECH on two eventcentric structured prediction datasets, and experimental results demonstrate that SPEECH is able to model manifold event structures with dependency and obtain effective event representations. In the future, we intend to enhance our work by modeling more complicated structures and extend it to other structured prediction tasks.358Acknowledgements We would like to express gratitude to the anonymous reviewers for their kind comments. This work was supported by the Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent experiments, we combine the output classes of hateful and offensive into one resulting in„8k/1k/1k hateful samples and „6k/781/782 nonhateful samples for train/validation/test respectively. Additionally, we utilize the accompanying rationales for ruleset construction.367Jigsaw5is a largescale dataset of Wikipedia comments labeled by human raters for toxic behavior. The defined types of toxicity are “toxic”, “severe toxic”, “obscene”, “threat”, “insult”, and “identity hate”. Each comment can have any one or more of these labels. In total, it contains „230k samples. In our experiments, we define examples of the “identity hate” class as hateful and the rest as nonhateful resulting in a dataset of 1405/100/712 hateful samples and „158k/1k/63k nonhateful examples for train/validation/test respectively. Contextual Abuse Dataset (CAD) (Vidgen et al., 2021) is annotated dataset of „25k Reddit entries labeled across six conceptually distinct primary categories of “Identitydirected”, “Persondirected”, “Affiliation directed”, “Counter Speech”, “Nonhateful Slurs”, and “Neutral”. In our experiment, we define examples of the “identitydirected” class as hateful and treat the remaining examples as nonhateful resulting in a dataset of 1353/513/428 hateful samples and „12k/4k/4k nonhateful samples for train/validation/test. 3.2 Ruleset Construction Hate+Abuse List We utilize a ruleset targeting identity hate which we’ll refer to as Hate+Abuse List. It consists of a list of ngrams representing harmful language such as slurs or hate verbs. Hate+Abuse List is similar to the publically available bad word lists commonly found online. We treat each ngram entry in Hate+Abuse List as its own rule that proposes a positive label if the ngram is in the input text. In total, Hate+Abuse List consists of 2957 distinct identity hate rules. HateXplain Rationale Ruleset Using the labeled annotator rationales included in the HateXplain dataset, we programmatically generate a Ruleset for HateXplain. To do so, we extract 1, 2, and 3gram substrings from the annotator rationales and cluster them by annotatoridentified target demographic groups. We then take the top N ngrams per each demographic group and automatically create rules for each of them. This results in rules similar in nature to our Hate+Abuse List. Using a default cluster size of 100 across the 25 target categories defined in HateXplain, we generated a total of 670 distinct rules for HateXplain. 5https://www.kaggle.com/competitions/ jigsawtoxic-commentclassification% 2DchallengeContextual Abuse Rationale Ruleset Similar to our derived HateXplain ruleset we programmatically generate a Ruleset for the Contextual Abuse Dataset using annotatorlabeled rationales. Following the identical process outlined before, this results in a total of 2712 distinct rules for CAD. Exemplar Selection For each dataset we complete our Ruleset construction by pairing each rule with accompanying exemplars. To achieve this, we first run our Ruleset on the dataset trainset and extract instances for which a rule correctly fires. For each rule that correctly fires, we then randomly select N instances to act as the exemplars. Additionally, to restrict potentially overgeneralized rules we enforce the condition that no two rules can be mapped to the same exemplar. Unless stated otherwise, we report results using just one exemplar per rule in our experiments. 3.3 Unsupervised Setting In addition to evaluating RBE in supervised settings, we investigate the applicability of RBE in unsupervised settings where no labeled data is present. In this setting, we are presented with a large unlabeled corpus Tand a given ruleset R. This setting is particularly challenging due to the inherent generalization problem of rules. Loosely applying rules as is in this setting results in the model overfitting to the distribution of the ruleset as seen in Table 3. To combat this issue, we design three different semantic clusteringbased strategies for determining rule quality in an unsupervised setting: Mean ,Concat , and Distance clustering. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first encode the entire corpus Tusing a pretrained sentence embedding model EΘ. In our case, we use a finetuned version of MPNet (Song et al., 2020) from the Sentence Transformers library. After receiving our encoded corpus EΘpTq, for the Mean andConcat , we construct a rule embedding ri Θfor each rule riin the ruleset. In the Mean strategy, this is obtained by taking the mean of all rule exemplars µpri Θq“p1 mřm iei mq. For Concat , this is calculated by concatenating all rule exemplars µpriq“EΘpei 1}...}ei mqand encoding the concatenated representation. Once ri Θis constructed, we then label each text in the corpus whose cosine similarity is within the threshold k:368Content Moderation Using Rules (Fully Supervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`0.808 0.841 0.824 0.787 0.459 0.729 0.563 0.987 0.445 0.421 0.433 0.893 MPNet^0.795 0.854 0.823 0.783 0.510 0.674 0.581 0.989 0.519 0.417 0.463 0.906 Rule By Example`△0.758 0.903 0.824 0.771 0.581 0.625 0.602 0.991 0.416 0.478 0.445 0.885 Rule By Example^△0.790 0.891 0.837 0.795 0.508 0.746 0.604 0.989 0.484 0.468 0.476 0.900 Rule By Example`˚0.738 0.912 0.816 0.756 - - - - - - - - Rule By Example^˚0.779 0.893 0.832 0.786 - - - - - - - - Rule By Example`;- - - - - - - - 0.512 0.378 0.435 0.905 Rule By Example^;- - - - - - - - 0.508 0.448 0.476 0.905 Table 1: Experiment Results in Fully Supervised Setting on hate speech classification datasets.`Uses BERT (Devlin et al., 2018) as the base model.^Uses MPNet (Song et al., 2020) as the base model.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset.;Uses CAD Ruleset. Note: The HateXplain Ruleset and Contextual Abuse Dataset (CAD) Ruleset are only applicable to their respective datasets. fptiq“# 1,ifsimpri Θ, EΘptiqqěk 0,otherwise(3) In contrast to the Mean and Concat strategies, the Distance strategy takes a rule elimination approach. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first noisily label the entire corpus using the ruleset Ri:xtÑ t1,Hu such that each rule is paired with a cover set R“tpr1, e1, c1q, ...,prn, en, cnquwhere ciis the set of texts in covered by ri. Next, for each rule, we encode text in its cover set EΘpciqand calculate the average cosine distance between each embedding and its neighboring examples in ci. avgDistpEΘpciqq“1 nnÿ idistpci j, ci j´1q(4) Lastly, once the average distance for each rule is calculated, using the defined threshold k, we flip any weakly labeled examples in the cover set if the average distance for that rule is above the threshold k: fptiq“# 1,ifavgDistpriqěk 0,otherwise(5) 4 Results and Discussion We analyze the results of our experiments, detail our insights, and discuss the implications of applying RBE for explainable hate speech detection.Evaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1. Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric. 4.1 Supervised Performance Table 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformerbased models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1)Ruleset Size and Alignment - As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown369Rule By Example Rule Grounding Dataset Text Fired Rules Exemplar HateXplain fully agree every personal interaction with these queers reinforces what i already knew severe mental illness and obnoxious to bootif contains(\"queers\") Ñ 1,0yes but queers are too self righteous to let them be included originally the gay and pedophile communities were working together nambla was started by gay men who liked boys now all subsets of pedophiles are members assuming nambla still exists Jigsaw Why do they put so many gay people on the damn show since when it was okay to be gay.if contains(\"so\" ^”gay”qÑ1,0stop reverting my edit your so f**** gay get a f**** life your f**** or go get laid or something CAD What a little b**** if contains(\"b****\") Ñ1,0 Nope, today is tuna b**** Table 2: Example of Rulegrounding explanations. This table shows examples of traced model predictions produced by RBE. By displaying the rules and exemplars responsible, rule authors and users are better able to understand model predictions and can automatically adjust their ruleset to further improve model performance. by the high recall score of the HateXplain Rationale Ruleset in Table 1. Additionally, when applied to the HateXplain dataset, the HateXplain Rationale Ruleset produces a total of 577 rules compared to the 377 rules derived from the Hate+Abuse Ruleset, allowing for more rule representations for the model to contrast against. 2)Embedding Initialization - Out of the box, pretrained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful information after downstream finetuning. This is shown by the BERT performance in Table 1. However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any finetuning, BERT embeddings obtain a precision, recall, and F1score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pretrained sentence embedder, such as MPNet, with a pretrain task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned. On Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward nonhateful examples and thus more representative of the realworld case of content moderation where most content is considered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1, on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE’s reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting overgeneralized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines. Outof-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an outof-domain ruleset. This provides an example of outof-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data. 4.2 Interpretability In addition to its improved performance, another advantage of RBE lies in its ability to perform Rulegrounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rulegrounding examples extracted from each of our tested datasets. By nature, Rulegrounding enables two main features in RBE: 1)Customizability/Ruleset Adaptation : Given the vast reach of online applications, content mod370Content Moderation Using Rules (Unsupervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`˚0.606 0.990 0.752 0.613 - - - - - - - - BERT`△0.747 0.717 0.732 0.688 0.234 0.461 0.310 0.977 0.587 0.205 0.303 0.909 BERT`;- - - - - - - - 0.107 0.865 0.191 0.290 MPNet`˚0.611 0.991 0.756 0.621 - - - - - - - - MPNet`△0.652 0.850 0.738 0.641 0.247 0.501 0.331 0.977 0.642 0.199 0.304 0.912 MPNet`;- - - - - - - - 0.111 0.840 0.196 0.335 Rule By Example (Distance)˚0.614 0.983 0.756 0.623 - - - - - - - - Rule By Example (Distance)△0.629 0.955 0.758 0.639 0.358 0.284 0.317 0.986 0.280 0.322 0.299 0.854 Rule By Example (Distance);- - - - - - - - 0.166 0.522 0.252 0.701 Rule By Example (Concat)˚0.621 0.950 0.751 0.626 - - - - - - - - Rule By Example (Concat)△0.612 0.985 0.755 0.621 0.189 0.052 0.081 0.987 0.175 0.437 0.250 0.747 Rule By Example (Concat);- - - - - - - - 0.178 0.437 0.253 0.750 Rule By Example (Mean)˚0.612 0.983 0.754 0.620 - - - - - - - - Rule By Example (Mean)△0.636 0.944 0.760 0.646 0.188 0.124 0.149 0.984 0.294 0.273 0.283 0.866 Rule By Example (Mean);- - - - - - - - 0.189 0.411 0.259 0.772 Unsupervised PreTraining Rule By Example (Mean)△0.641 0.954 0.767 0.656 0.166 .626 0.262 0.961 0.260 0.320 0.287 0.846 Rule By Example (Distance)△0.617 0.968 0.753 0.624 0.203 0.465 0.283 0.974 0.484 0.236 0.317 0.902 Table 3: Unsupervised Performance across all clustering strategies.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset. ;Uses CAD ruleset. Note: The HateXplain Ruleset is not applicable to Jigsaw and Contextual Abuse Dataset (CAD). eration systems need to be easily adaptable to everemerging trends of hateful content. Particularly in online social settings, expert users of these platforms continually find new and interesting ways to bypass moderation systems. Additionally, new terminologies and slang are being introduced every day. RBE is seamlessly capable of addressing these concerns by facilitating ruleguided learning. By defining a new rule and adding at least one exemplar, RBE is able to capture emerging content without the need for retraining. Additionally, users of RBE can easily modify existing rules that may be too broad and add additional exemplars to further refine predictions in a controllable manner. 2)Prediction Transparency : By facilitating model interpretations via rulegrounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate. 4.3 Unsupervised Performance Table 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE’s Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain andCAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy’s rule elimination objective. As opposed to the Mean andConcat strategies which focus on deriving rule representations in a selfsupervised manner, the Distance strategy instead focuses on eliminating overgeneralized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives. For Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE’s overreliance on the ruleset in this setting, particularly for the Mean andConcat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is overgeneralized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing selfsupervised model pretraining (Gao et al., 2021) on the target corpus circumvents this trend for the Mean andConcat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD. 5 Related Work There has been active work on detecting hate speech in language (Poletto et al., 2021; AlMakhadmeh and Tolba, 2020; Schmidt and Wie371gand, 2017). Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023). However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021), thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this Experimental Setup Training We train all models with AdamW optimizer and weight decay of 0.01 on all data. We employ early stopping with a ceiling of 10 epochs, a learning rate of 2e5, batch size of 8, and linear learning rate warmup over the first 10% steps with a cosine schedule. Our models are trained with NVIDIA Tesla V100 32GB GPUs using Azure Machine Learning Studio. We preprocess data and train all models with different random seeds over multiple runs. Our implementation of RBE is based on Huggingface Transformers (Wolf et al., 2020) and Sentence Transformers (Reimers and Gurevych, 2019). RBE utilizes two Bertbased networks consisting of 110 million parameters each. Approximately 2,000 GPU hours were required to train all hyperparameter variations of RBE plus the Bert baseline across all 3 test sets. Baselines We evaluate our training algorithms in both supervised and unsupervised settings. We compare against the baselines of applying logical rules as is and the current SOTA approach of training transformerbased sequence classifiers (Mathew et al., 2020). 3.1 Datasets We evaluate RBE across three datasets on the task of hatespeech classification. Across each dataset, we frame the problem as a binary classification task of detecting whether a given text is hateful or nonhateful. We augment each dataset with rulesets that we manually curate. More information on each dataset and ruleset is provided below. HateXplain (Mathew et al., 2020) is a largescale benchmark dataset for explainable hate speech detection that covers multiple aspects of hate speech detection. It consists of „20k samples across 3 labels “hateful”, “offensive”, and “normal”. Additionally, each sample is accompanied by a corresponding target group and explainable rationales. In our experiments, we combine the output classes of hateful and offensive into one resulting in„8k/1k/1k hateful samples and „6k/781/782 nonhateful samples for train/validation/test respectively. Additionally, we utilize the accompanying rationales for ruleset construction.367Jigsaw5is a largescale dataset of Wikipedia comments labeled by human raters for toxic behavior. The defined types of toxicity are “toxic”, “severe toxic”, “obscene”, “threat”, “insult”, and “identity hate”. Each comment can have any one or more of these labels. In total, it contains „230k samples. In our experiments, we define examples of the “identity hate” class as hateful and the rest as nonhateful resulting in a dataset of 1405/100/712 hateful samples and „158k/1k/63k nonhateful examples for train/validation/test respectively. Contextual Abuse Dataset (CAD) (Vidgen et al., 2021) is annotated dataset of „25k Reddit entries labeled across six conceptually distinct primary categories of “Identitydirected”, “Persondirected”, “Affiliation directed”, “Counter Speech”, “Nonhateful Slurs”, and “Neutral”. In our experiment, we define examples of the “identitydirected” class as hateful and treat the remaining examples as nonhateful resulting in a dataset of 1353/513/428 hateful samples and „12k/4k/4k nonhateful samples for train/validation/test. 3.2 Ruleset Construction Hate+Abuse List We utilize a ruleset targeting identity hate which we’ll refer to as Hate+Abuse List. It consists of a list of ngrams representing harmful language such as slurs or hate verbs. Hate+Abuse List is similar to the publically available bad word lists commonly found online. We treat each ngram entry in Hate+Abuse List as its own rule that proposes a positive label if the ngram is in the input text. In total, Hate+Abuse List consists of 2957 distinct identity hate rules. HateXplain Rationale Ruleset Using the labeled annotator rationales included in the HateXplain dataset, we programmatically generate a Ruleset for HateXplain. To do so, we extract 1, 2, and 3gram substrings from the annotator rationales and cluster them by annotatoridentified target demographic groups. We then take the top N ngrams per each demographic group and automatically create rules for each of them. This results in rules similar in nature to our Hate+Abuse List. Using a default cluster size of 100 across the 25 target categories defined in HateXplain, we generated a total of 670 distinct rules for HateXplain. 5https://www.kaggle.com/competitions/ jigsawtoxic-commentclassification% 2DchallengeContextual Abuse Rationale Ruleset Similar to our derived HateXplain ruleset we programmatically generate a Ruleset for the Contextual Abuse Dataset using annotatorlabeled rationales. Following the identical process outlined before, this results in a total of 2712 distinct rules for CAD. Exemplar Selection For each dataset we complete our Ruleset construction by pairing each rule with accompanying exemplars. To achieve this, we first run our Ruleset on the dataset trainset and extract instances for which a rule correctly fires. For each rule that correctly fires, we then randomly select N instances to act as the exemplars. Additionally, to restrict potentially overgeneralized rules we enforce the condition that no two rules can be mapped to the same exemplar. Unless stated otherwise, we report results using just one exemplar per rule in our experiments. 3.3 Unsupervised Setting In addition to evaluating RBE in supervised settings, we investigate the applicability of RBE in unsupervised settings where no labeled data is present. In this setting, we are presented with a large unlabeled corpus Tand a given ruleset R. This setting is particularly challenging due to the inherent generalization problem of rules. Loosely applying rules as is in this setting results in the model overfitting to the distribution of the ruleset as seen in Table 3. To combat this issue, we design three different semantic clusteringbased strategies for determining rule quality in an unsupervised setting: Mean ,Concat , and Distance clustering. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first encode the entire corpus Tusing a pretrained sentence embedding model EΘ. In our case, we use a finetuned version of MPNet (Song et al., 2020) from the Sentence Transformers library. After receiving our encoded corpus EΘpTq, for the Mean andConcat , we construct a rule embedding ri Θfor each rule riin the ruleset. In the Mean strategy, this is obtained by taking the mean of all rule exemplars µpri Θq“p1 mřm iei mq. For Concat , this is calculated by concatenating all rule exemplars µpriq“EΘpei 1}...}ei mqand encoding the concatenated representation. Once ri Θis constructed, we then label each text in the corpus whose cosine similarity is within the threshold k:368Content Moderation Using Rules (Fully Supervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`0.808 0.841 0.824 0.787 0.459 0.729 0.563 0.987 0.445 0.421 0.433 0.893 MPNet^0.795 0.854 0.823 0.783 0.510 0.674 0.581 0.989 0.519 0.417 0.463 0.906 Rule By Example`△0.758 0.903 0.824 0.771 0.581 0.625 0.602 0.991 0.416 0.478 0.445 0.885 Rule By Example^△0.790 0.891 0.837 0.795 0.508 0.746 0.604 0.989 0.484 0.468 0.476 0.900 Rule By Example`˚0.738 0.912 0.816 0.756 - - - - - - - - Rule By Example^˚0.779 0.893 0.832 0.786 - - - - - - - - Rule By Example`;- - - - - - - - 0.512 0.378 0.435 0.905 Rule By Example^;- - - - - - - - 0.508 0.448 0.476 0.905 Table 1: Experiment Results in Fully Supervised Setting on hate speech classification datasets.`Uses BERT (Devlin et al., 2018) as the base model.^Uses MPNet (Song et al., 2020) as the base model.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset.;Uses CAD Ruleset. Note: The HateXplain Ruleset and Contextual Abuse Dataset (CAD) Ruleset are only applicable to their respective datasets. fptiq“# 1,ifsimpri Θ, EΘptiqqěk 0,otherwise(3) In contrast to the Mean and Concat strategies, the Distance strategy takes a rule elimination approach. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first noisily label the entire corpus using the ruleset Ri:xtÑ t1,Hu such that each rule is paired with a cover set R“tpr1, e1, c1q, ...,prn, en, cnquwhere ciis the set of texts in covered by ri. Next, for each rule, we encode text in its cover set EΘpciqand calculate the average cosine distance between each embedding and its neighboring examples in ci. avgDistpEΘpciqq“1 nnÿ idistpci j, ci j´1q(4) Lastly, once the average distance for each rule is calculated, using the defined threshold k, we flip any weakly labeled examples in the cover set if the average distance for that rule is above the threshold k: fptiq“# 1,ifavgDistpriqěk 0,otherwise(5) 4 Results and Discussion We analyze the results of our experiments, detail our insights, and discuss the implications of applying RBE for explainable hate speech detection.Evaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1. Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric. 4.1 Supervised Performance Table 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformerbased models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1)Ruleset Size and Alignment - As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown369Rule By Example Rule Grounding Dataset Text Fired Rules Exemplar HateXplain fully agree every personal interaction with these queers reinforces what i already knew severe mental illness and obnoxious to bootif contains(\"queers\") Ñ 1,0yes but queers are too self righteous to let them be included originally the gay and pedophile communities were working together nambla was started by gay men who liked boys now all subsets of pedophiles are members assuming nambla still exists Jigsaw Why do they put so many gay people on the damn show since when it was okay to be gay.if contains(\"so\" ^”gay”qÑ1,0stop reverting my edit your so f**** gay get a f**** life your f**** or go get laid or something CAD What a little b**** if contains(\"b****\") Ñ1,0 Nope, today is tuna b**** Table 2: Example of Rulegrounding explanations. This table shows examples of traced model predictions produced by RBE. By displaying the rules and exemplars responsible, rule authors and users are better able to understand model predictions and can automatically adjust their ruleset to further improve model performance. by the high recall score of the HateXplain Rationale Ruleset in Table 1. Additionally, when applied to the HateXplain dataset, the HateXplain Rationale Ruleset produces a total of 577 rules compared to the 377 rules derived from the Hate+Abuse Ruleset, allowing for more rule representations for the model to contrast against. 2)Embedding Initialization - Out of the box, pretrained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful information after downstream finetuning. This is shown by the BERT performance in Table 1. However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any finetuning, BERT embeddings obtain a precision, recall, and F1score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pretrained sentence embedder, such as MPNet, with a pretrain task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned. On Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward nonhateful examples and thus more representative of the realworld case of content moderation where most content is considered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1, on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE’s reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting overgeneralized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines. Outof-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an outof-domain ruleset. This provides an example of outof-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data. 4.2 Interpretability In addition to its improved performance, another advantage of RBE lies in its ability to perform Rulegrounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rulegrounding examples extracted from each of our tested datasets. By nature, Rulegrounding enables two main features in RBE: 1)Customizability/Ruleset Adaptation : Given the vast reach of online applications, content mod370Content Moderation Using Rules (Unsupervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`˚0.606 0.990 0.752 0.613 - - - - - - - - BERT`△0.747 0.717 0.732 0.688 0.234 0.461 0.310 0.977 0.587 0.205 0.303 0.909 BERT`;- - - - - - - - 0.107 0.865 0.191 0.290 MPNet`˚0.611 0.991 0.756 0.621 - - - - - - - - MPNet`△0.652 0.850 0.738 0.641 0.247 0.501 0.331 0.977 0.642 0.199 0.304 0.912 MPNet`;- - - - - - - - 0.111 0.840 0.196 0.335 Rule By Example (Distance)˚0.614 0.983 0.756 0.623 - - - - - - - - Rule By Example (Distance)△0.629 0.955 0.758 0.639 0.358 0.284 0.317 0.986 0.280 0.322 0.299 0.854 Rule By Example (Distance);- - - - - - - - 0.166 0.522 0.252 0.701 Rule By Example (Concat)˚0.621 0.950 0.751 0.626 - - - - - - - - Rule By Example (Concat)△0.612 0.985 0.755 0.621 0.189 0.052 0.081 0.987 0.175 0.437 0.250 0.747 Rule By Example (Concat);- - - - - - - - 0.178 0.437 0.253 0.750 Rule By Example (Mean)˚0.612 0.983 0.754 0.620 - - - - - - - - Rule By Example (Mean)△0.636 0.944 0.760 0.646 0.188 0.124 0.149 0.984 0.294 0.273 0.283 0.866 Rule By Example (Mean);- - - - - - - - 0.189 0.411 0.259 0.772 Unsupervised PreTraining Rule By Example (Mean)△0.641 0.954 0.767 0.656 0.166 .626 0.262 0.961 0.260 0.320 0.287 0.846 Rule By Example (Distance)△0.617 0.968 0.753 0.624 0.203 0.465 0.283 0.974 0.484 0.236 0.317 0.902 Table 3: Unsupervised Performance across all clustering strategies.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset. ;Uses CAD ruleset. Note: The HateXplain Ruleset is not applicable to Jigsaw and Contextual Abuse Dataset (CAD). eration systems need to be easily adaptable to everemerging trends of hateful content. Particularly in online social settings, expert users of these platforms continually find new and interesting ways to bypass moderation systems. Additionally, new terminologies and slang are being introduced every day. RBE is seamlessly capable of addressing these concerns by facilitating ruleguided learning. By defining a new rule and adding at least one exemplar, RBE is able to capture emerging content without the need for retraining. Additionally, users of RBE can easily modify existing rules that may be too broad and add additional exemplars to further refine predictions in a controllable manner. 2)Prediction Transparency : By facilitating model interpretations via rulegrounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate. 4.3 Unsupervised Performance Table 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE’s Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain andCAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy’s rule elimination objective. As opposed to the Mean andConcat strategies which focus on deriving rule representations in a selfsupervised manner, the Distance strategy instead focuses on eliminating overgeneralized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives. For Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE’s overreliance on the ruleset in this setting, particularly for the Mean andConcat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is overgeneralized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing selfsupervised model pretraining (Gao et al., 2021) on the target corpus circumvents this trend for the Mean andConcat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD. 5 Related Work There has been active work on detecting hate speech in language (Poletto et al., 2021; AlMakhadmeh and Tolba, 2020; Schmidt and Wie371gand, 2017). Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023). However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021), thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this Evaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1. Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric. 4.1 Supervised Performance Table 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformerbased models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1)Ruleset Size and Alignment - As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown369Rule By Example Rule Grounding Dataset Text Fired Rules Exemplar HateXplain fully agree every personal interaction with these queers reinforces what i already knew severe mental illness and obnoxious to bootif contains(\"queers\") Ñ 1,0yes but queers are too self righteous to let them be included originally the gay and pedophile communities were working together nambla was started by gay men who liked boys now all subsets of pedophiles are members assuming nambla still exists Jigsaw Why do they put so many gay people on the damn show since when it was okay to be gay.if contains(\"so\" ^”gay”qÑ1,0stop reverting my edit your so f**** gay get a f**** life your f**** or go get laid or something CAD What a little b**** if contains(\"b****\") Ñ1,0 Nope, today is tuna b**** Table 2: Example of Rulegrounding explanations. This table shows examples of traced model predictions produced by RBE. By displaying the rules and exemplars responsible, rule authors and users are better able to understand model predictions and can automatically adjust their ruleset to further improve model performance. by the high recall score of the HateXplain Rationale Ruleset in Table 1. Additionally, when applied to the HateXplain dataset, the HateXplain Rationale Ruleset produces a total of 577 rules compared to the 377 rules derived from the Hate+Abuse Ruleset, allowing for more rule representations for the model to contrast against. 2)Embedding Initialization - Out of the box, pretrained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful information after downstream finetuning. This is shown by the BERT performance in Table 1. However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any finetuning, BERT embeddings obtain a precision, recall, and F1score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pretrained sentence embedder, such as MPNet, with a pretrain task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned. On Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward nonhateful examples and thus more representative of the realworld case of content moderation where most content is considered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1, on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE’s reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting overgeneralized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines. Outof-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an outof-domain ruleset. This provides an example of outof-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data. 4.2 Interpretability In addition to its improved performance, another advantage of RBE lies in its ability to perform Rulegrounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rulegrounding examples extracted from each of our tested datasets. By nature, Rulegrounding enables two main features in RBE: 1)Customizability/Ruleset Adaptation : Given the vast reach of online applications, content mod370Content Moderation Using Rules (Unsupervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`˚0.606 0.990 0.752 0.613 - - - - - - - - BERT`△0.747 0.717 0.732 0.688 0.234 0.461 0.310 0.977 0.587 0.205 0.303 0.909 BERT`;- - - - - - - - 0.107 0.865 0.191 0.290 MPNet`˚0.611 0.991 0.756 0.621 - - - - - - - - MPNet`△0.652 0.850 0.738 0.641 0.247 0.501 0.331 0.977 0.642 0.199 0.304 0.912 MPNet`;- - - - - - - - 0.111 0.840 0.196 0.335 Rule By Example (Distance)˚0.614 0.983 0.756 0.623 - - - - - - - - Rule By Example (Distance)△0.629 0.955 0.758 0.639 0.358 0.284 0.317 0.986 0.280 0.322 0.299 0.854 Rule By Example (Distance);- - - - - - - - 0.166 0.522 0.252 0.701 Rule By Example (Concat)˚0.621 0.950 0.751 0.626 - - - - - - - - Rule By Example (Concat)△0.612 0.985 0.755 0.621 0.189 0.052 0.081 0.987 0.175 0.437 0.250 0.747 Rule By Example (Concat);- - - - - - - - 0.178 0.437 0.253 0.750 Rule By Example (Mean)˚0.612 0.983 0.754 0.620 - - - - - - - - Rule By Example (Mean)△0.636 0.944 0.760 0.646 0.188 0.124 0.149 0.984 0.294 0.273 0.283 0.866 Rule By Example (Mean);- - - - - - - - 0.189 0.411 0.259 0.772 Unsupervised PreTraining Rule By Example (Mean)△0.641 0.954 0.767 0.656 0.166 .626 0.262 0.961 0.260 0.320 0.287 0.846 Rule By Example (Distance)△0.617 0.968 0.753 0.624 0.203 0.465 0.283 0.974 0.484 0.236 0.317 0.902 Table 3: Unsupervised Performance across all clustering strategies.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset. ;Uses CAD ruleset. Note: The HateXplain Ruleset is not applicable to Jigsaw and Contextual Abuse Dataset (CAD). eration systems need to be easily adaptable to everemerging trends of hateful content. Particularly in online social settings, expert users of these platforms continually find new and interesting ways to bypass moderation systems. Additionally, new terminologies and slang are being introduced every day. RBE is seamlessly capable of addressing these concerns by facilitating ruleguided learning. By defining a new rule and adding at least one exemplar, RBE is able to capture emerging content without the need for retraining. Additionally, users of RBE can easily modify existing rules that may be too broad and add additional exemplars to further refine predictions in a controllable manner. 2)Prediction Transparency : By facilitating model interpretations via rulegrounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate. 4.3 Unsupervised Performance Table 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE’s Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain andCAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy’s rule elimination objective. As opposed to the Mean andConcat strategies which focus on deriving rule representations in a selfsupervised manner, the Distance strategy instead focuses on eliminating overgeneralized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives. For Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE’s overreliance on the ruleset in this setting, particularly for the Mean andConcat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is overgeneralized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing selfsupervised model pretraining (Gao et al., 2021) on the target corpus circumvents this trend for the Mean andConcat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD. 5 Related Work There has been active work on detecting hate speech in language (Poletto et al., 2021; AlMakhadmeh and Tolba, 2020; Schmidt and Wie371gand, 2017). Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023). However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021), thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this analysis and insights into the customizability and interpretability features of RBE to address the problem of emerging hateful content and model transparency. 2 Rule By Example Framework In this section, we outline the Rule By Example framework, define its operational terms, and describe its endto-end architecture. We first formally describe the two main operational terms used in our framework: 1) Ruleset - a ruleset is comprised of a series of executable functions that when given text as input “fire” if and only if all conditions defined in the rule are met by the input. Figure 1 shows an example of a simple rule that is triggered if a given text contains the keywords “hate” or 4https://github.com/ChrisIsKing/ RuleBy-Example365Figure 2: Rule By Example Framework : RBE is comprised of two neural networks, a rule encoder and a text encoder, which jointly learn rich embedding representations for hateful content and the logical rules that govern them. Through Contrastive learning, RBE utilizes a semantic similarity objective that pairs hateful examples with clusters of rule exemplars that govern it. “loathe” and contains “women” . Rules can be any programmable function that acts on text such as regular expressions, blocklists, keywords, etc. In the scope of this work, we only consider simple rules that humans can easily interpret. As such an ML model cannot be considered a rule, given their blackbox nature. 2) Exemplar - an exemplar is a given textual example that welldefines the type of content governed by a rule. For example, X1 andX2in Figure 1 can be considered exemplars of ruleR1since they correctly match the conditions ofR1. Consider a ruleset of ruleexemplar pairs R“tpr1, e1q,pr2, e2q, ...,prn, enquwhere ridenotes a defined rule and eidenotes an exemplar for which ricorrectly fires. For a given corpus Xcomprising labeled examples X“tpx1, y1q,px2, y2q, ...,pxm, ymqu, each rule ri can be used as a blackbox function Ri:xÑ tyi,Huto noisily label each instance xsuch that it assigns a label yor no label at all. An instance may be covered by more than one rule or no rule at all. Additionally, the cover set Cdenotes the set of instances in Xwhere a rule rifires. The generalization problem that arises when rules are applied noisily is twofold. When rules are too broad the cover set Cis large and incorrectly labels a large amount of nonhateful content. Likewise, when rules are too strict and fragile, the cover set Cis too small, and lexically and semantically similar content that is hateful ends up being ignored. Our goal is to leverage these rules and their exemplars to facilitate explainable model learning.Algorithm 1 Supervised Dual Encoder Training Require: Rule Encoder ΘrText Encoder Θt Input: Training Data X“px1, y1q...pxn, ynq, Ruleset R“pr1, e1q, ...,prn, enq Output: Updated parameters Θr,Θt 1: Initialize ΘrandΘt 2:while not converged do 3: Get minibatch Xb 4: foreach instance xiinXbdo 5: Get exemplars ei“doRulesetpR, x iq 6: Concatenate exemplars ei 7: end for 8: Get ΘrpEbqandΘtpXbq 9: Compute L“1 2pYbD2`p1´Ybqmaxpmargin´ D,0q2q 10: Update parameters of ΘrandΘt 11:end while 2.1 Dual Encoder Architecture The DualEncoder architecture, as illustrated in Figure 2, is commonly used in dense retrieval systems and multimodal applications (Clarke et al., 2022; Reimers and Gurevych, 2019; Xu et al., 2022). Our architecture consists of a Rule Encoder Θrand a Text Encoder Θt. These are two Bertlike bidirectional transformer models (Devlin et al., 2018) each responsible for learning embedding representations of their respective inputs. This Dual Encoder architecture enables preindexing of exemplars allowing for faster inference at runtime after training. Encoding Pipeline Given an input text xt, we first extract the set of applicable rules and their respective exemplars from the ruleset R. We then concatenate each extracted exemplar to form xe. In the event that no rules are applicable to xt, we randomly sample exemplars from the en366tire ruleset to form xe. Using the form xe“␣ rCLSs, e1 1, ..., e1 m,rSEPs, en 1, ...., en k( , we then use rule encoder Θrto encode xeinto hidden states he“␣ vrCLSs, v1, ..., vrSEPs( where en kis the kth token of the nth exemplar and rSEPsandrCLSs are special tokens. Similarly, using the text encoder Θt, we encode xt. In order to obtain a dense representation, we apply a mean pooling operation to the hidden states and derive a fixedsized sentence embedding. After obtaining the representation for both the exemplars xeand the text xt, we use the cosine function to measure the similarity between them: simpxe, xtq“Θrpxeq¨Θtpxtq }Θrpxeq}}Θtpxtq}(1) We employ a contrastive loss (Hadsell et al., 2006) to learn the embedding representations for our rule and text encoder. Contrastive learning encourages the model to maximize the representation similarity between samelabel examples and to minimize it for differentlabel examples. This enables the embedding representations of our encoded ruleset to match the representation of the text correctly covered by cover set C. Likewise, for benign examples that rules incorrectly cover, our contrastive learning objective increases the distance between those representations, thus restricting the overgeneralization of certain rules in the ruleset. LetYtbe the correct label of the texts Xt,Dbe the cosine distance of pxe, xtqandmbe the margin, our contrastive learning loss function is defined as follows: L“1 2pYtD2`p1´Ytqmaxpm´D,0q2q (2) The training loop, with the encoding pipeline and constrastive loss step, are detailed in Algorithm 1. 2.2 RuleGrounding By taking an embeddingsbased approach to learning representations, RBE enables what we define asrulegrounding . Rulegrounding enables us to trace our model predictions back to the explainable ruleset accompanied by the exemplars that define each rule. For any input xtthat has been marked as positive by our dual encoder, we perform a rules search to find which rules fire on that input as well as an embedding similarity search to find the nearest exemplars and the rules those exemplars belong to. Table 2 shows an example of this.3 Experimental Setup Training We train all models with AdamW optimizer and weight decay of 0.01 on all data. We employ early stopping with a ceiling of 10 epochs, a learning rate of 2e5, batch size of 8, and linear learning rate warmup over the first 10% steps with a cosine schedule. Our models are trained with NVIDIA Tesla V100 32GB GPUs using Azure Machine Learning Studio. We preprocess data and train all models with different random seeds over multiple runs. Our implementation of RBE is based on Huggingface Transformers (Wolf et al., 2020) and Sentence Transformers (Reimers and Gurevych, 2019). RBE utilizes two Bertbased networks consisting of 110 million parameters each. Approximately 2,000 GPU hours were required to train all hyperparameter variations of RBE plus the Bert baseline across all 3 test sets. Baselines We evaluate our training algorithms in both supervised and unsupervised settings. We compare against the baselines of applying logical rules as is and the current SOTA approach of training transformerbased sequence classifiers (Mathew et al., 2020). 3.1 Datasets We evaluate RBE across three datasets on the task of hatespeech classification. Across each dataset, we frame the problem as a binary classification task of detecting whether a given text is hateful or nonhateful. We augment each dataset with rulesets that we manually curate. More information on each dataset and ruleset is provided below. HateXplain (Mathew et al., 2020) is a largescale benchmark dataset for explainable hate speech detection that covers multiple aspects of hate speech detection. It consists of „20k samples across 3 labels “hateful”, “offensive”, and “normal”. Additionally, each sample is accompanied by a corresponding target group and explainable rationales. In our experiments, we combine the output classes of hateful and offensive into one resulting in„8k/1k/1k hateful samples and „6k/781/782 nonhateful samples for train/validation/test respectively. Additionally, we utilize the accompanying rationales for ruleset construction.367Jigsaw5is a largescale dataset of Wikipedia comments labeled by human raters for toxic behavior. The defined types of toxicity are “toxic”, “severe toxic”, “obscene”, “threat”, “insult”, and “identity hate”. Each comment can have any one or more of these labels. In total, it contains „230k samples. In our experiments, we define examples of the “identity hate” class as hateful and the rest as nonhateful resulting in a dataset of 1405/100/712 hateful samples and „158k/1k/63k nonhateful examples for train/validation/test respectively. Contextual Abuse Dataset (CAD) (Vidgen et al., 2021) is annotated dataset of „25k Reddit entries labeled across six conceptually distinct primary categories of “Identitydirected”, “Persondirected”, “Affiliation directed”, “Counter Speech”, “Nonhateful Slurs”, and “Neutral”. In our experiment, we define examples of the “identitydirected” class as hateful and treat the remaining examples as nonhateful resulting in a dataset of 1353/513/428 hateful samples and „12k/4k/4k nonhateful samples for train/validation/test. 3.2 Ruleset Construction Hate+Abuse List We utilize a ruleset targeting identity hate which we’ll refer to as Hate+Abuse List. It consists of a list of ngrams representing harmful language such as slurs or hate verbs. Hate+Abuse List is similar to the publically available bad word lists commonly found online. We treat each ngram entry in Hate+Abuse List as its own rule that proposes a positive label if the ngram is in the input text. In total, Hate+Abuse List consists of 2957 distinct identity hate rules. HateXplain Rationale Ruleset Using the labeled annotator rationales included in the HateXplain dataset, we programmatically generate a Ruleset for HateXplain. To do so, we extract 1, 2, and 3gram substrings from the annotator rationales and cluster them by annotatoridentified target demographic groups. We then take the top N ngrams per each demographic group and automatically create rules for each of them. This results in rules similar in nature to our Hate+Abuse List. Using a default cluster size of 100 across the 25 target categories defined in HateXplain, we generated a total of 670 distinct rules for HateXplain. 5https://www.kaggle.com/competitions/ jigsawtoxic-commentclassification% 2DchallengeContextual Abuse Rationale Ruleset Similar to our derived HateXplain ruleset we programmatically generate a Ruleset for the Contextual Abuse Dataset using annotatorlabeled rationales. Following the identical process outlined before, this results in a total of 2712 distinct rules for CAD. Exemplar Selection For each dataset we complete our Ruleset construction by pairing each rule with accompanying exemplars. To achieve this, we first run our Ruleset on the dataset trainset and extract instances for which a rule correctly fires. For each rule that correctly fires, we then randomly select N instances to act as the exemplars. Additionally, to restrict potentially overgeneralized rules we enforce the condition that no two rules can be mapped to the same exemplar. Unless stated otherwise, we report results using just one exemplar per rule in our experiments. 3.3 Unsupervised Setting In addition to evaluating RBE in supervised settings, we investigate the applicability of RBE in unsupervised settings where no labeled data is present. In this setting, we are presented with a large unlabeled corpus Tand a given ruleset R. This setting is particularly challenging due to the inherent generalization problem of rules. Loosely applying rules as is in this setting results in the model overfitting to the distribution of the ruleset as seen in Table 3. To combat this issue, we design three different semantic clusteringbased strategies for determining rule quality in an unsupervised setting: Mean ,Concat , and Distance clustering. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first encode the entire corpus Tusing a pretrained sentence embedding model EΘ. In our case, we use a finetuned version of MPNet (Song et al., 2020) from the Sentence Transformers library. After receiving our encoded corpus EΘpTq, for the Mean andConcat , we construct a rule embedding ri Θfor each rule riin the ruleset. In the Mean strategy, this is obtained by taking the mean of all rule exemplars µpri Θq“p1 mřm iei mq. For Concat , this is calculated by concatenating all rule exemplars µpriq“EΘpei 1}...}ei mqand encoding the concatenated representation. Once ri Θis constructed, we then label each text in the corpus whose cosine similarity is within the threshold k:368Content Moderation Using Rules (Fully Supervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`0.808 0.841 0.824 0.787 0.459 0.729 0.563 0.987 0.445 0.421 0.433 0.893 MPNet^0.795 0.854 0.823 0.783 0.510 0.674 0.581 0.989 0.519 0.417 0.463 0.906 Rule By Example`△0.758 0.903 0.824 0.771 0.581 0.625 0.602 0.991 0.416 0.478 0.445 0.885 Rule By Example^△0.790 0.891 0.837 0.795 0.508 0.746 0.604 0.989 0.484 0.468 0.476 0.900 Rule By Example`˚0.738 0.912 0.816 0.756 - - - - - - - - Rule By Example^˚0.779 0.893 0.832 0.786 - - - - - - - - Rule By Example`;- - - - - - - - 0.512 0.378 0.435 0.905 Rule By Example^;- - - - - - - - 0.508 0.448 0.476 0.905 Table 1: Experiment Results in Fully Supervised Setting on hate speech classification datasets.`Uses BERT (Devlin et al., 2018) as the base model.^Uses MPNet (Song et al., 2020) as the base model.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset.;Uses CAD Ruleset. Note: The HateXplain Ruleset and Contextual Abuse Dataset (CAD) Ruleset are only applicable to their respective datasets. fptiq“# 1,ifsimpri Θ, EΘptiqqěk 0,otherwise(3) In contrast to the Mean and Concat strategies, the Distance strategy takes a rule elimination approach. Given an unlabeled corpus T“ tt1, t2, ..., t nu, ruleset R“tpr1, e1q, ...,prn, enqu, and a threshold k, we first noisily label the entire corpus using the ruleset Ri:xtÑ t1,Hu such that each rule is paired with a cover set R“tpr1, e1, c1q, ...,prn, en, cnquwhere ciis the set of texts in covered by ri. Next, for each rule, we encode text in its cover set EΘpciqand calculate the average cosine distance between each embedding and its neighboring examples in ci. avgDistpEΘpciqq“1 nnÿ idistpci j, ci j´1q(4) Lastly, once the average distance for each rule is calculated, using the defined threshold k, we flip any weakly labeled examples in the cover set if the average distance for that rule is above the threshold k: fptiq“# 1,ifavgDistpriqěk 0,otherwise(5) 4 Results and Discussion We analyze the results of our experiments, detail our insights, and discuss the implications of applying RBE for explainable hate speech detection.Evaluation Metrics: The precision, recall, and F1 score for each dataset in a supervised setting are reported in Table 1. Due to the highly skewed class distribution, we favor macro F1 scores as our main evaluation metric. We also report accuracy scores (the fraction of entries for which the full set of labels matches) as another metric. 4.1 Supervised Performance Table 1 reports our results on three hate speech classification datasets in the supervised setting. We observe that RBE is able to outperform SOTA transformerbased models BERT and MPNet by 1.3/1.4%, 4.1/2.3%, and 4.3/1.3% in F1score on HateXplain, Jigsaw, and CAD respectively. This improvement highlights the impact of leveraging rules in the training process of our framework. Additionally, it is important to note that this increase was achieved using only 1 exemplar per rule in the ruleset. These exemplars were also used to train the comparative baseline models, ensuring that all approaches were trained on the same number of samples. This further showcases how lightweight and flexible RBE is to integrate into a content moderation workflow. For HateXplain, our experiments show that the combination of MPNet as the initialized encoder with both the HateXplain Rationale and Hate+Abuse Ruleset delivers the best performance. Upon deeper analysis, we find that this is due to two main factors: 1)Ruleset Size and Alignment - As explained in Section 3.2 the HateXplain Rationale Ruleset was automatically crafted using rationale labels from expert annotators. This results in a powerful ruleset capable of identifying a large amount of hateful content in the HateXplain dataset as shown369Rule By Example Rule Grounding Dataset Text Fired Rules Exemplar HateXplain fully agree every personal interaction with these queers reinforces what i already knew severe mental illness and obnoxious to bootif contains(\"queers\") Ñ 1,0yes but queers are too self righteous to let them be included originally the gay and pedophile communities were working together nambla was started by gay men who liked boys now all subsets of pedophiles are members assuming nambla still exists Jigsaw Why do they put so many gay people on the damn show since when it was okay to be gay.if contains(\"so\" ^”gay”qÑ1,0stop reverting my edit your so f**** gay get a f**** life your f**** or go get laid or something CAD What a little b**** if contains(\"b****\") Ñ1,0 Nope, today is tuna b**** Table 2: Example of Rulegrounding explanations. This table shows examples of traced model predictions produced by RBE. By displaying the rules and exemplars responsible, rule authors and users are better able to understand model predictions and can automatically adjust their ruleset to further improve model performance. by the high recall score of the HateXplain Rationale Ruleset in Table 1. Additionally, when applied to the HateXplain dataset, the HateXplain Rationale Ruleset produces a total of 577 rules compared to the 377 rules derived from the Hate+Abuse Ruleset, allowing for more rule representations for the model to contrast against. 2)Embedding Initialization - Out of the box, pretrained BERT does not produce meaningfully distinct sentence representations. In practice, the BERT [CLS] token as well as averaged BERT outputs can contain useful information after downstream finetuning. This is shown by the BERT performance in Table 1. However, when the pretrained model output is pooled across all dimensions and used for calculating semantic similarity, this results in similar representations even for completely different input text. As a result, if applied to the HateXplain dataset without any finetuning, BERT embeddings obtain a precision, recall, and F1score of 59%, 100%, and 75% respectively, where every example is labeled as hateful. This lack of varied sentence representation coupled with a verbose ruleset such as the HateXplain Rationale Ruleset results in an initial biasing towards hateful examples as shown by the high recall scores. As such, utilizing a pretrained sentence embedder, such as MPNet, with a pretrain task more optimized for semantic embeddings results in better performance. We observe a similar trend when utilizing our derived ruleset for CAD. Note: When trained longer, the bias of the BERT model decreases as more varied sentence representations are learned. On Jigsaw and Contextual Abuse datasets using the Hate+Abuse List and derived CAD Ruleset, RBE outperforms SOTA by an increased margin of 4.1/2.3%, and 4.3/1.3% respectively. Contrary to HateXplain, these two datasets are more heavily imbalanced toward nonhateful examples and thus more representative of the realworld case of content moderation where most content is considered benign. This increased performance highlights the power of incorporating logical rules to assist model learning and also the ability of RBE to better generalize rules. As seen in Table 1, on its own the Hate+Abuse ruleset performs poorly on each dataset in both precision and recall. Despite RBE’s reliance on this ruleset to guide model learning, when combined with labeled training data, RBE is capable of both restricting overgeneralized rules and leveraging its understanding of semantic similarity to extend fragile rules regardless of the base model. Additionally, when using the CAD ruleset which is heavily overfitted to the CAD dataset, as shown by the skewed recall score, RBE is still capable of outperforming the baselines. Outof-domain Rulesets Our Hate+Abuse ruleset is a generic ruleset unrelated to any of the datasets evaluated, and thereby an outof-domain ruleset. This provides an example of outof-domain performance using rules not derived from the target dataset. We observe that even when applying RBE with the Hate+Abuse ruleset we are able to outperform the baselines on each dataset. When applying RBE to new domain settings, all that is required is the authoring of additional rules for this new domain. This can be done manually, or more scalably by automatically deriving rules from the new domain data. 4.2 Interpretability In addition to its improved performance, another advantage of RBE lies in its ability to perform Rulegrounding. As explained in section 2.2, Rulegrounding enables us to trace our model predictions back to their respective rule accompanied by the exemplars that define that rule. Table 2 shows Rulegrounding examples extracted from each of our tested datasets. By nature, Rulegrounding enables two main features in RBE: 1)Customizability/Ruleset Adaptation : Given the vast reach of online applications, content mod370Content Moderation Using Rules (Unsupervised) HateXplain Jigsaw CAD Model Precision Recall F1 Acc Precision Recall F1 Acc Precision Recall F1 Acc HateXplain Rules 0.609 0.983 0.752 0.615 - - - - - - Hate+Abuse Rules 0.755 0.687 0.719 0.682 0.164 0.361 0.226 0.972 0.586 0.193 0.290 0.909 CAD Rules - - - - - - - - 0.110 0.842 0.194 0.325 BERT`˚0.606 0.990 0.752 0.613 - - - - - - - - BERT`△0.747 0.717 0.732 0.688 0.234 0.461 0.310 0.977 0.587 0.205 0.303 0.909 BERT`;- - - - - - - - 0.107 0.865 0.191 0.290 MPNet`˚0.611 0.991 0.756 0.621 - - - - - - - - MPNet`△0.652 0.850 0.738 0.641 0.247 0.501 0.331 0.977 0.642 0.199 0.304 0.912 MPNet`;- - - - - - - - 0.111 0.840 0.196 0.335 Rule By Example (Distance)˚0.614 0.983 0.756 0.623 - - - - - - - - Rule By Example (Distance)△0.629 0.955 0.758 0.639 0.358 0.284 0.317 0.986 0.280 0.322 0.299 0.854 Rule By Example (Distance);- - - - - - - - 0.166 0.522 0.252 0.701 Rule By Example (Concat)˚0.621 0.950 0.751 0.626 - - - - - - - - Rule By Example (Concat)△0.612 0.985 0.755 0.621 0.189 0.052 0.081 0.987 0.175 0.437 0.250 0.747 Rule By Example (Concat);- - - - - - - - 0.178 0.437 0.253 0.750 Rule By Example (Mean)˚0.612 0.983 0.754 0.620 - - - - - - - - Rule By Example (Mean)△0.636 0.944 0.760 0.646 0.188 0.124 0.149 0.984 0.294 0.273 0.283 0.866 Rule By Example (Mean);- - - - - - - - 0.189 0.411 0.259 0.772 Unsupervised PreTraining Rule By Example (Mean)△0.641 0.954 0.767 0.656 0.166 .626 0.262 0.961 0.260 0.320 0.287 0.846 Rule By Example (Distance)△0.617 0.968 0.753 0.624 0.203 0.465 0.283 0.974 0.484 0.236 0.317 0.902 Table 3: Unsupervised Performance across all clustering strategies.˚Uses HateXplain ruleset.△Uses Hate+Abuse ruleset. ;Uses CAD ruleset. Note: The HateXplain Ruleset is not applicable to Jigsaw and Contextual Abuse Dataset (CAD). eration systems need to be easily adaptable to everemerging trends of hateful content. Particularly in online social settings, expert users of these platforms continually find new and interesting ways to bypass moderation systems. Additionally, new terminologies and slang are being introduced every day. RBE is seamlessly capable of addressing these concerns by facilitating ruleguided learning. By defining a new rule and adding at least one exemplar, RBE is able to capture emerging content without the need for retraining. Additionally, users of RBE can easily modify existing rules that may be too broad and add additional exemplars to further refine predictions in a controllable manner. 2)Prediction Transparency : By facilitating model interpretations via rulegrounding, users of online systems are offered tangible guidance should their content be flagged, potentially increasing user trust in the system. Additionally, this acts as a direct indicator of the type of content the rule authors want to moderate. 4.3 Unsupervised Performance Table 3 reports our results in the unsupervised setting. We observe that RBE is able to outperform SOTA trained on noisy rules labeled samples for the HateXplain and Jigsaw dataset while also outperforming the ruleset as is on all three datasets. Across each dataset, we find that RBE’s Distance based strategy produces the most consistent performance, outperforming SOTA on HateXplain andCAD while performing on par with SOTA on Jigsaw. We observe that this stability in performance is due to this strategy’s rule elimination objective. As opposed to the Mean andConcat strategies which focus on deriving rule representations in a selfsupervised manner, the Distance strategy instead focuses on eliminating overgeneralized rules whose cover set of examples are semantically dissimilar. This is particularly useful in cases where precision scores are low due to a large number of false positives. For Jigsaw, we observe a slight decrease in performance compared to SOTA. Upon further analysis, we posit that this is a result of RBE’s overreliance on the ruleset in this setting, particularly for the Mean andConcat strategies. This is because the ruleset directly influences the derived rule embedding due to its labeling of the cover set C. As such when the ruleset is overgeneralized, as is the case of Hate+Abuse rules on Jigsaw, RBE is likely to match the distribution of the ruleset. We find that performing selfsupervised model pretraining (Gao et al., 2021) on the target corpus circumvents this trend for the Mean andConcat strategy. As such, with a more refined ruleset, a performance increase is expected as seen in HateXplain and CAD. 5 Related Work There has been active work on detecting hate speech in language (Poletto et al., 2021; AlMakhadmeh and Tolba, 2020; Schmidt and Wie371gand, 2017). Hate Speech detection has proven to be a nuanced and difficult task, leading to the development of approaches and datasets targeted at various aspects of the problem (Vidgen et al., 2021; Mathew et al., 2020; Mody et al., 2023). However, few attempts have been made to focus on the explainability of these models, which is an increasing area of concern surrounding their use online (Tarasov, 2021; Haimson et al., 2021), thus leading to the continued utilization of less powerful but more explainable methods such as rules. Prior works have explored incorporating logical rules into model learning. Awasthi et al. (2020) proposed to weakly learn from rules by pairing them with exemplars and training a denoising model. However, this requires defining rules for all output classes, making it inapplicable to the task of hate speech detection. Additionally, this  Experimental Setup Our overall setup consists of 3 steps: (1) we create ENsource sentences, each of which contains 3rd person pronouns representing different “pronoun categories” (e.g., gendered pronoun , etc.) in different grammatical cases. (2) Next, we employ an MT system to translate the ENsentences to five target languages. (3) Last, we let native speakers manually analyze the translations with respect to diverse criteria, e.g., grammaticality of the output . Creation of ENSource Data. We start with the WinoMT data set (Stanovsky et al., 2019), designed to assess gender bias in MT and consisting of sentences that contain occupations stereotypically associated with women (e.g., secretary ) or men (e.g., developer ). We conduct an automatic morphological analysis on each pronoun in the data set.2Based on the output, we randomly sample for each grammatical case (e.g., nominative, etc.), in which a 3rd person pronoun referring to an occupation appears in, two sentences: one in which the target occupation is stereotypically associated with men and one in which it is stereotypically associated with women. We then replace those pronouns with placeholders, indicating the case (e.g., <n> for nominative) of each. Since WinoMT does not contain pronouns in the possessive independent case, we create these by sampling additional sentences with possessive dependent pronouns and remove the target noun. Accordingly, we end up with 10 templates from WinoMT (2 for each of the 5 grammatical cases). Additionally, given that WinoMT 2For this, we use partof-speech tags and morphological output generated by spaCy ( https://spacy.io ).sentences are designed to be more complex and ambiguous, we manually create two additional, simpler sentences for each grammatical case (10 in total). In these sentences, the pronoun placeholders refer to given names. In accordance with the WinoMT pattern, we choose the top name stereotypically associated with women and the top name stereotypically associated with men according to 2020 U.S. Social Security name statistics.3We show example templates in Table 1. We fill the placeholders with pronouns of the correct grammatical case taken from 8 sets of pronouns that reflect diverse pronounrelated phenomena as described by Lauscher et al. (2022). For example, we use she/ her /her/ hers/ herself as an instance of gendered pronouns, and vam/ vamp / vamps/ vamps/ vampself as an instance of nounself pronouns (Miltersen, 2016). The latter are prototypically derived from a noun, and possibly match distinct aspects of an individual’s identity. We list our test pronouns in Table 2. Our setup allows us to test the translation of sentences containing different types of pronouns, in all of their grammatical forms, in more and less complex sentences and in contexts that are prone to different stereotypical associations. Our procedure results in 164 ENsentences (4 sentences per 5 cases for each of the 8 pronoun sets plus 4 additional sentences for the variant themself instead of themselves ). Automatic Translation. Next, we automatically translate the ENsource sentences to five languages: Danish ( DA), Farsi ( FA), French ( FR), German ( DE), and Italian ( IT). We choose these languages based on (a) typological diversity, (b) our access to native speakers, and (c) their coverage by commercial MT. We ensure diversity with respect to family branches, scripts, and the handling of gender and pronouns in the languages: DEand DArepresent the Germanic branch, FRand ITthe Romanic branch, and FAthe Iranian branch of IndoEuropean languages. DA, DE,FR, and ITemploy the Latin script, and FA the Arabic one. Most importantly, the handling of grammatical gender and pronouns differs among languages. Concretely, DA,DE,FR, and ITare gendered languages but differ in their number of genders (e.g., DEhas three grammatical genders while FRhas two). While for DEand IT, there is currently no genderneutral pronoun recognized by an institutional body, for FR, the dictionary Le Robert recently included the genderneutral pronoun “iel” . 3https://www.ssa.gov/oact/babynames/379Case Templates NominativeThe attendant recognized the developer because <n> reads a lot of technical news. The analyst employed the housekeeper because <n> could not stand housework. Olivia lost the game, so <n> was sad. Liam received a good grade, so <n> was happy. AccusativeThe developer wanted free bread from the baker and made up a story for <a> about not having a kitchen. The attendant did not want to fight with the guard and gave <a> flowers. I like Olivia, so I met <a> today. I do not like Liam, so I do not want to meet <a> today. Poss. Depen.The mechanic visited the writer and helped on fixing <pd> car engine. The baker sold bread to the CEO and enjoyed <pd> visits. Liam lost <pd> phone. Olivia found <pd> ring. Poss. Indep.During lunch, the janitor looked for the attendant to steal <pi>. Last Saturday, the physician called the tailor to fix <pi>. I had no phone, so Olivia gave me <pi>. I lost my notes, so Liam gave me <pi>. ReflexiveThe farmer did not want to talk to the writer because <n> was burying <r> in writing a new novel. The chief employed the receptionist because <n> was too busy to answer those phone calls by <r> every day. Olivia wanted to impress, so <n> baked a cake <r>. Liam wanted a new haircut, so <n> cut the hair <r>. Table 1: The templates we use for each grammatical case. Placeholders are indicated with brackets and the grammatical case of the pronoun to fill, e.g., <pd> (possessive dependent pronoun). The first two templates for each case are extracted from WinoMT (Stanovsky et al., 2019), while the second two templates are added by us. Phenomon N A PD PI R Genderedhe him his his himself she her her hers herself Genderneutral they them their theirsthemselves themself Neoxe xem xyr xyrs xemself ey em eir eirs emself Nounself vam vamp vamps vamps vampself Emojiself s s self Numberself 0 0 0 s 0s 0self Table 2: Phenomena and 3rd person pronoun sets by which they are represented in our analysis when translating from English ( EN→DA,DE,FA,FR,IT). We list the pronouns for each grammatical case: nominative (N), accusative (A), possesive dependent (PD), possessive independent (PI), and reflexive (R). In contrast, FAis a genderneutral language. Thus, there should also be no potential for misgendering in the resulting translations. Another interesting aspect is that two of the languages fall under the class of prodrop languages ( IT,FA)4, while the others do not allow for dropping the pronoun. We focus on assessing the state of commercial MT, and accordingly rely on 3 established MT en4Pro-drop refers to a linguistic phenomenon where the subject pronoun can be omitted from a sentence without affecting its grammaticality or clarity. It is often clear from the verb inflection, as in Italian “Vado”: “(I) go. ”gines: Google Translate,5Microsoft Bing,6and DeepL Translator.7Currently, DeepL does not cover Farsi (all other languages are covered by all three commercial MT engines). Annotation Criteria. While initially, we wanted to focus solely on identity aspects conveyed by the pronouns, we noticed in an early prestudy that some of the translations exhibited more fundamental issues. This is why we resort to the following three categories, which allow us to answer research questions RQ1–RQ3, to guide our analysis of a translation B based on an ENsentence A: grammatical correctness ,semantic consistency , and pronoun translation behavior . (1) Grammatical Correctness. We ask our annotators to assess whether translation B is grammatically correct. Annotators are instructed to not let their judgment be affected by the occurrence of neopronouns that are potentially uncommon in the target language, e.g., emojiself pronouns. (2) Semantic Consistency. We let our annotators judge whether B conveys the same message as A in 5https://translate.google.com ; we accessed Google Translate through the interface provided in Google Sheets. Note that we observed differences in translation when using the graphical user interface. 6https://www.bing.com/translator 7https://www.deepl.com/translator380two variants: First, we seek to understand whether independent of how the pronoun was translated the semantics of A are preserved. Second, we ask whether when also considering the pronoun translation, semantics are preserved. (3) Pronoun Translation Behavior. The third category specifically focuses on assessing the translation of the pronoun. We investigate whether the pronoun was omitted (i.e., it is not present in B), copied (pronoun in B is exactly the same as in A), ortranslated (the system output some other string in B as correspondence to the pronoun in A). Note that none of these cases necessarily corresponds to a translation error (or translation success) – for instance, it might be a valid option to directly copy the pronoun from the input in the source language to fully preserve its individual semantics. If the pronoun was “translated”, we ask annotators to highlight its translation, and to further indicate if the translation corresponds to a common pronoun in the target language (and also, whether it still functions as a pronoun). If a common pronoun is chosen, we also collect its number and its commonly associated gender. Annotation Process. As the evaluation task requires annotators to be familiar with the target language, the concept of neopronouns, and linguistic properties such as partof-speech tags, we hired five native speakers of target languages who all hold a university degree, are proficient speakers of English, and have diverse gender identities (man, woman, nonbinary). We payed our annotators 15 C per hour, which is substantially above the minimum wage in Italy and in line with the main authors’ university recommendations for academic assistants. All annotators demonstrated great interest in helping to make MT more inclusive and were familiar with the overall topic. We took a descriptive annotation evaluation, and Dev et al. (2021) surveyed the378harms arising from nonbinary exclusion in NLP, indicating MT as one particularly harmful application. Following up, Lauscher et al. (2022) explored the various phenomena related to 3rdperson pronoun usage in English, e.g., neopronouns. We are the first to study the translation of these novel pronounrelated phenomena in MT. 3 The Status Quo To shed light on the state of identity inclusion through 3rd person pronouns in commercial MT, we conduct a thorough error analysis when translating from English ( EN) to five diverse languages. We further describe an experiment opposite to this, translating from Danish ( DA) to EN, in §3.3. 3.1 Experimental Setup Our overall setup consists of 3 steps: (1) we create ENsource sentences, each of which contains 3rd person pronouns representing different “pronoun categories” (e.g., gendered pronoun , etc.) in different grammatical cases. (2) Next, we employ an MT system to translate the ENsentences to five target languages. (3) Last, we let native speakers manually analyze the translations with respect to diverse criteria, e.g., grammaticality of the output . Creation of ENSource Data. We start with the WinoMT data set (Stanovsky et al., 2019), designed to assess gender bias in MT and consisting of sentences that contain occupations stereotypically associated with women (e.g., secretary ) or men (e.g., developer ). We conduct an automatic morphological analysis on each pronoun in the data set.2Based on the output, we randomly sample for each grammatical case (e.g., nominative, etc.), in which a 3rd person pronoun referring to an occupation appears in, two sentences: one in which the target occupation is stereotypically associated with men and one in which it is stereotypically associated with women. We then replace those pronouns with placeholders, indicating the case (e.g., <n> for nominative) of each. Since WinoMT does not contain pronouns in the possessive independent case, we create these by sampling additional sentences with possessive dependent pronouns and remove the target noun. Accordingly, we end up with 10 templates from WinoMT (2 for each of the 5 grammatical cases). Additionally, given that WinoMT 2For this, we use partof-speech tags and morphological output generated by spaCy ( https://spacy.io ).sentences are designed to be more complex and ambiguous, we manually create two additional, simpler sentences for each grammatical case (10 in total). In these sentences, the pronoun placeholders refer to given names. In accordance with the WinoMT pattern, we choose the top name stereotypically associated with women and the top name stereotypically associated with men according to 2020 U.S. Social Security name statistics.3We show example templates in Table 1. We fill the placeholders with pronouns of the correct grammatical case taken from 8 sets of pronouns that reflect diverse pronounrelated phenomena as described by Lauscher et al. (2022). For example, we use she/ her /her/ hers/ herself as an instance of gendered pronouns, and vam/ vamp / vamps/ vamps/ vampself as an instance of nounself pronouns (Miltersen, 2016). The latter are prototypically derived from a noun, and possibly match distinct aspects of an individual’s identity. We list our test pronouns in Table 2. Our setup allows us to test the translation of sentences containing different types of pronouns, in all of their grammatical forms, in more and less complex sentences and in contexts that are prone to different stereotypical associations. Our procedure results in 164 ENsentences (4 sentences per 5 cases for each of the 8 pronoun sets plus 4 additional sentences for the variant themself instead of themselves ). Automatic Translation. Next, we automatically translate the ENsource sentences to five languages: Danish ( DA), Farsi ( FA), French ( FR), German ( DE), and Italian ( IT). We choose these languages based on (a) typological diversity, (b) our access to native speakers, and (c) their coverage by commercial MT. We ensure diversity with respect to family branches, scripts, and the handling of gender and pronouns in the languages: DEand DArepresent the Germanic branch, FRand ITthe Romanic branch, and FAthe Iranian branch of IndoEuropean languages. DA, DE,FR, and ITemploy the Latin script, and FA the Arabic one. Most importantly, the handling of grammatical gender and pronouns differs among languages. Concretely, DA,DE,FR, and ITare gendered languages but differ in their number of genders (e.g., DEhas three grammatical genders while FRhas two). While for DEand IT, there is currently no genderneutral pronoun recognized by an institutional body, for FR, the dictionary Le Robert recently included the genderneutral pronoun “iel” . 3https://www.ssa.gov/oact/babynames/379Case Templates NominativeThe attendant recognized the developer because <n> reads a lot of technical news. The analyst employed the housekeeper because <n> could not stand housework. Olivia lost the game, so <n> was sad. Liam received a good grade, so <n> was happy. AccusativeThe developer wanted free bread from the baker and made up a story for <a> about not having a kitchen. The attendant did not want to fight with the guard and gave <a> flowers. I like Olivia, so I met <a> today. I do not like Liam, so I do not want to meet <a> today. Poss. Depen.The mechanic visited the writer and helped on fixing <pd> car engine. The baker sold bread to the CEO and enjoyed <pd> visits. Liam lost <pd> phone. Olivia found <pd> ring. Poss. Indep.During lunch, the janitor looked for the attendant to steal <pi>. Last Saturday, the physician called the tailor to fix <pi>. I had no phone, so Olivia gave me <pi>. I lost my notes, so Liam gave me <pi>. ReflexiveThe farmer did not want to talk to the writer because <n> was burying <r> in writing a new novel. The chief employed the receptionist because <n> was too busy to answer those phone calls by <r> every day. Olivia wanted to impress, so <n> baked a cake <r>. Liam wanted a new haircut, so <n> cut the hair <r>. Table 1: The templates we use for each grammatical case. Placeholders are indicated with brackets and the grammatical case of the pronoun to fill, e.g., <pd> (possessive dependent pronoun). The first two templates for each case are extracted from WinoMT (Stanovsky et al., 2019), while the second two templates are added by us. Phenomon N A PD PI R Genderedhe him his his himself she her her hers herself Genderneutral they them their theirsthemselves themself Neoxe xem xyr xyrs xemself ey em eir eirs emself Nounself vam vamp vamps vamps vampself Emojiself s s self Numberself 0 0 0 s 0s 0self Table 2: Phenomena and 3rd person pronoun sets by which they are represented in our analysis when translating from English ( EN→DA,DE,FA,FR,IT). We list the pronouns for each grammatical case: nominative (N), accusative (A), possesive dependent (PD), possessive independent (PI), and reflexive (R). In contrast, FAis a genderneutral language. Thus, there should also be no potential for misgendering in the resulting translations. Another interesting aspect is that two of the languages fall under the class of prodrop languages ( IT,FA)4, while the others do not allow for dropping the pronoun. We focus on assessing the state of commercial MT, and accordingly rely on 3 established MT en4Pro-drop refers to a linguistic phenomenon where the subject pronoun can be omitted from a sentence without affecting its grammaticality or clarity. It is often clear from the verb inflection, as in Italian “Vado”: “(I) go. ”gines: Google Translate,5Microsoft Bing,6and DeepL Translator.7Currently, DeepL does not cover Farsi (all other languages are covered by all three commercial MT engines). Annotation Criteria. While initially, we wanted to focus solely on identity aspects conveyed by the pronouns, we noticed in an early prestudy that some of the translations exhibited more fundamental issues. This is why we resort to the following three categories, which allow us to answer research questions RQ1–RQ3, to guide our analysis of a translation B based on an ENsentence A: grammatical correctness ,semantic consistency , and pronoun translation behavior . (1) Grammatical Correctness. We ask our annotators to assess whether translation B is grammatically correct. Annotators are instructed to not let their judgment be affected by the occurrence of neopronouns that are potentially uncommon in the target language, e.g., emojiself pronouns. (2) Semantic Consistency. We let our annotators judge whether B conveys the same message as A in 5https://translate.google.com ; we accessed Google Translate through the interface provided in Google Sheets. Note that we observed differences in translation when using the graphical user interface. 6https://www.bing.com/translator 7https://www.deepl.com/translator380two variants: First, we seek to understand whether independent of how the pronoun was translated the semantics of A are preserved. Second, we ask whether when also considering the pronoun translation, semantics are preserved. (3) Pronoun Translation Behavior. The third category specifically focuses on assessing the translation of the pronoun. We investigate whether the pronoun was omitted (i.e., it is not present in B), copied (pronoun in B is exactly the same as in A), ortranslated (the system output some other string in B as correspondence to the pronoun in A). Note that none of these cases necessarily corresponds to a translation error (or translation success) – for instance, it might be a valid option to directly copy the pronoun from the input in the source language to fully preserve its individual semantics. If the pronoun was “translated”, we ask annotators to highlight its translation, and to further indicate if the translation corresponds to a common pronoun in the target language (and also, whether it still functions as a pronoun). If a common pronoun is chosen, we also collect its number and its commonly associated gender. Annotation Process. As the evaluation task requires annotators to be familiar with the target language, the concept of neopronouns, and linguistic properties such as partof-speech tags, we hired five native speakers of target languages who all hold a university degree, are proficient speakers of English, and have diverse gender identities (man, woman, nonbinary). We payed our annotators 15 C per hour, which is substantially above the minimum wage in Italy and in line with the main authors’ university recommendations for academic assistants. All annotators demonstrated great interest in helping to make MT more inclusive and were familiar with the overall topic. We took a descriptive annotation analysis shows that the presence of a genderneutral pronoun often leads to grammatical and semantic translation errors. Similarly, gender neutrality is often not preserved. By surveying the opinions of affected native speakers from diverse languages, we provide recommendations to address the issue in future MT research. 1 experiments show APE achieves new stateof-theart with a large margin in the EAE task. When only ten records are available in the target dataset, our model dramatically outperforms the baseline model with average 27.27% F1 gain.1 1  Evaluation Metric Following baseline models, we adopt two metrics: ArgI and ArgC. Following Li et al. (2021), we add HeadC for WikiEvents datasets. Please refer to Appendix A for the detail of evaluation metric. 3.4 Implementation Details We initialize the weight of the Transformer with BART model (Lewis et al., 2020). The length |P| of Prefix is set to 70, and the interdim dadapter of the Adapter is set to 512 for BARTbase model and 768 for BARTlarge model. For simplicity, we initialize the Prefix and the Adapter randomly. We optimized our models on NVIDIA A40 GPU by AdamW (Loshchilov and Hutter, 2019) with β1= 0.9, β 2= 0.999, ϵ= 1e−8, and 10% warmup steps. We set the learning rate to 1e3 for Prefix and 1e4 for Adapter. To ensure the confidence of the result, we repeated the model training five times with five fixed seeds [14, 21, 28, 35, 42]. The reported experimental results are the average score. We exhibit some examples of M(r) (Table 10) and prompts (Table 11) in the Appendix. The complete M(r)and prompts of each dataset are available in our codebase. 4 Results and Analyses To investigate the efficacy of our APE model, we compare our model with several stateof-theart baseline models (4.1). Then, we verify the significance of transfer overlap knowledge (4.2) in the fewshot setting. We also perform ablation studies and further analysis to examine the effectiveness of the main components in our model (4.3). 4.1 Overall Performance Table 1 present the main result of all baseline models and APE on three datasets. APE refers to our397Table 1: The Overall performance of our model and baselines. We bold the best result and underline the second best. b in column PLM denotes base model and l is large model. Model PLMACE05 RAMS WikiEvents ArgI ArgC ArgI ArgC ArgI ArgC HeadC OneIEBERTb 65.9 59.2 - - - - - BERTl 73.2 69.2 - - - - - EEQABERTb 68.2 65.4 46.4 44.0 54.3 53.2 56.9 BERTl 70.5 68.9 48.7 46.7 56.9 54.5 59.3 BARTGenBART-b 59.6 55.0 50.9 44.9 47.5 41.7 44.2 BARTl 69.9 66.7 51.2 47.1 66.8 62.4 65.4 PAIEBARTb 73.6 69.8 54.7 49.5 68.9 63.4 66.5 BARTl 75.7 72.7 56.8 52.2 70.5 65.3 68.4 PAIEJointBART-b 73.8 69.5 53.3 48.3 69.3 63.7 65.9 BARTl 75.1 72.4 55.9 51.8 70.1 65.2 67.9 UnifiedEAE BARTb 76.1 71.9 55.5 49.9 69.8 64.0 66.3 APE(Single)BARTb 74.1 70.1 54.8 49.6 66.2 62.1 64.9 BARTl 75.3 72.9 56.3 51.7 70.6 65.8 68.4 APEBARTb 75.5 72.9 56.1 51.6 70.7 66.0 68.7 BARTl 78.2 75.4 58.1 54.3 73.7 68.7 70.8 full model, which optimizes the Prefix in multidatasets. APE(Single) refers to the APE model trained in the transferdisable setting, which optimizes the Prefix only in the target dataset. In the APE(Single), the overlap knowledge degrades into shared knowledge between different event types within the same target dataset. From Table 1, we have the following observations. First, APE achieves the highest F1 score on every evaluation metric compared with all the baselines model. Our base model obtained +1%, +1.7%, and +2% gain of ArgC F1 scores on ACE05, RAMS, and WikiEvents, respectively. The large model expands the margin to +2.7%, +2.1%, and +3.4%. The results show that there is abundant overlap knowledge in multidatasets, and our model can fully utilize it in the target dataset. Second, despite not relying on transfer learning, APE (Single) also achieves stateof-theart performance on ACE05 and WikiEvents, and a competitive score on RAMS, which suggests that knowledge shared between different event types in a single dataset can also boost performance. Third, the PAIEJoint even slightly worse than the PAIE. It donate that it is difficult for the model to find overlap knowledge by itself from datasets with various event structures, event types, and even different annotation guidelines. The APE can exploit the overlap knowledge from the transparent training objective of the PER task, and achieve better performance.Table 2: ArgC F1 score on fewshot setting Dataset ACE05 RAMS Wiki. PAIE10 3.3±2.14.3±1.45.7±3.6 50 35.2±5.325.2±6.131.4±4.6 100 39.6±2.530.4±2.142.1±3.2 200 51.2±1.335.8±1.953.2±1.7 APE10 32.1±7.126.3±4.236.7±8.3 50 42.5±3.933.4±4.147.6±5.4 100 53.2±1.738.5±1.655.6±2.6 200 59.3±0.941.1±1.259.5±1.5 4.2 Fewshot Setting APE is exceptionally suited for lacking indomain labeled data because APE can learn from outdomain event records. Therefore, we conduct a fewshot experiment to verify the ability of APE to reduce the dependence on target dataset samples. Specifically, we optimize Prefix on the other two intact datasets and train Adapter on the target dataset with few samples. Table 2 reports the ArgC F1 score in the target dataset with 10, 50, 100, and 200 random sampled event records. From the results, we obtain the following observations. 1). APE significantly outperforms the stateof-theart baseline PAIE model in three benchmarks. 2). Especially in the case of only ten samples, APE achieves 27.27% F1 score gains average in three datasets. 3). APE with 200 samples achieves competitive scores with some398Table 3: The performance of different variants on ACE05 VariantParamACE05overlap specific APE Prefix Adapter 72.9 APE reversed Adapter Perfix 72.1 w/o Prefix BART Adapter 71.5 w/o Adapter Prefix BART 71.7 BART BART BART 69.4 baseline model trained on the whole WikiEvents or ACE05 dataset. The results indicate that APE significantly reduces the need for the scale of the target dataset. 4.3 Detailed Analysis In this section, we study the effectiveness of the main components in our model and take a deeper look at what contributes to APE’s final performance. All experiments will be based on the baseversion model and report the average ArgC F1 scores on five seeds. The experimental conclusions are also proper for the large version model. 4.3.1 Model Architecture Design We first explore the effectiveness of APE model architecture in preventing catastrophic forgetting. We tried variants of APE as follows: 1) APE reversed : it has the same model architecture as APE but saves overlap knowledge in the Adapter and specific knowledge in the Prefix. 2) w/o Prefix: it is an APE without Prefix, which updates all pretrained parameters to save overlap knowledge. 3) w/o Adapter: pretrained parameters will be updated to save specific knowledge. 4) BART: it is a standard BART model without additional parameters. We optimize the model in the overlap knowledge learning phase and finetune it in the specific knowledge learning phase. The result of ACE05 is summarized in Table 3, and the result of other datasets is in Appendix Table 8. All variants that save overlap and specific knowledge into different parameters outperform the plain BART model significantly. Since the plain BART model saves overlap and specific knowledge in the same parameters, serial learning phases will lead to catastrophic forgetting of previous knowledge. Suppose we save both knowledges into new parameter regions (APE, APE reversed ). In this case, we can also obtain a considerable performance gainTable 4: The performance of different learning tasks Task ACE05 RAMS Wiki. Joint EAE Task 69.9 49.4 64.1 PER Task 72.9 51.6 66.0 Table 5: The performance of different prompt styles prompt styleACE05 RAMS Wiki.overlap specific ST ST 72.9 51.6 66.0 NL NL 72.1 51.1 65.3 NL ST 69.5 49.3 63.5 because our task formulation is similar to the pretrain task of BART, where the entitytype special tokens can be seen as [MASK] tokens. Retaining the pretraining parameter is helpful to take the best advantage of PLM’s knowledge. Finally, there is a slightly negative effect when we reverse the parameter regions to save overlap and specific knowledge. We conjecture that APE reversed cannot model the order of knowledge utilization in the SCRD framework. 4.3.2 Overlap Knowledge Learning Task To investigate the effect of the PER task and its transparent training objective (Equation 8) in learning the overlap knowledge, we throw out the SCRD framework and replace the PER task with Joint EAE Task like the previous work. The Joint EAE Task ignores the difference of datasets and merges multidatasets to force the model directly learn overlap knowledge from the EAE training objective. The input and the target output string of the Joint EAE Task are as same as the specific knowledge learning phase. Two versions of Prefix will be respectively learned from the Joint EAE Task and the PER task and used in target datasets. It can be observed in Table 4 that there is a 3.0%, 2.2%, and 1.9% decrease for the ArgC F1 score on three datasets when changing the task. It is difficult for the model to discern the overlap knowledge from the imprecise EAE training objective. The PER task provides a transparent training objective to indicate the overlap knowledge explicitly. 4.3.3 Stressing EntityType Prompt As aforementioned, prompts that keeping the same style in two learning phases can ignite the utilization of overlap knowledge in the specific knowledge learning phase and EAE inference scene. In399Figure 3: The performance of different multidatasets order to verify it, we propose another prompt style named Natural Language Pronouns (NL), which replaces the entitytype Special Token (ST) with pronouns. The conversion between the two styles is shown in Appendix Table 9. We observe in Table 5 that there is a huge F1 score decrease of about 3.4% on the ACE05 dataset when we build prompts with different styles in two learning phases. The result indicates that narrowing the gap between the two phases is crucial to ignite the overlap knowledge. Meanwhile, the special token is a more powerful way to alert the model to the entity type than natural language. 4.3.4 Number of Datasets in Multidatasets In order to deeply observe the impact of the amount of the training data used in the overlap knowledge learning phase, we trained four versions of Prefix on varying numbers of training sets and transferred them to the target dataset. When the number of datasets was set to 0, the Prefix was randomly initialized and used directly without training. When the number of datasets was set to 1, we trained Prefix on {ACE05}. When the number of datasets was 2, we trained Prefix on {ACE05, RAMS}. Figure 3 shows the ArgC F1 score increase as the number of datasets used to learn the overlap knowledge. The experiment result shows that with more available outdomain event records, the APE model can learn more abundant overlap knowledge and achieve better performance in the target dataset. 5 Related Works 5.1 Transfer Learning in EAE Event argument extraction (EAE) aims to extract event arguments by the given event trigger and argument roles (Chen et al., 2015). Most existing approaches (Lin et al., 2020; Du and Cardie, 2020; Luet al., 2021; Nguyen et al., 2022; Ma et al., 2022) suffer from insufficient training data and cannot perform better. Therefore, some studies (Liu et al., 2020b; Chen et al., 2020; Feng et al., 2020) focus on transferring knowledge from machine reading comprehension (MRC) datasets. Huang et al. (2022) leverages multilingual pretrained models (Liu et al., 2020a; Xue et al., 2021) to achieve crosslingual knowledge transfer. About transferring overlap knowledge from other available event datasets to the target dataset, only Zhou et al. (2022) attempt to introduce variational information bottleneck (Li and Eisner, 2019) to explore the overlap knowledge from two event datasets. Unlike their work, we clearly define the crossdataset overlap knowledge in the EAE task. Our model does not limit the number of datasets and can explore overlap knowledge from all available datasets to achieve better performance. 5.2 ParameterEfficient Tuning analysis in Section 4.3 confirms the efficacy of the main components in our model. 2 Experiments demonstrate that, only requiring 0.08% extra training parameters of the GPT2, Tailor can achieve effective and general improvements on eleven attributespecific generation tasks. 1 Experimental Setup Datasets We conduct experiments on the widelyused benchmark dataset YELP (Lample et al., 2019). It contains multiple singleattribute data that can verify Tailor’s performance on both singleattribute and multiattribute CTG, while ensuring that the combination of these attributes is reasonable. Following previous works that conduct experiments on attributes of emotions and topics for multiattribute CTG, we choose Yelp restaurants reviews of sentiment attributes (positive (PO) and negative (NE)) and topics of food type (Mexican (ME), American (AM) and Asian (AS) foods) to evaluate models. Specifically, each attribute contains 30,000 / 3,000 sentences for training / validation. For evaluation, to keep in line with previous works (Yang and Klein, 2021; Dathathri et al., 2020), we use 15 attributeunrelated prefixes6and ask the model to continue writing with them (for each of the 15 prefixes, 100 completions are generated, total: 1500 for each attribute) while satisfying prespecified attribute as the final results.7 Automatic Evaluation Following Yang and Klein (2021); Dathathri et al. (2020), we automatically evaluate generation results from three aspects: (1) Correctness . We used RoBERTa Large (Liu et al., 2019) based attribute classifiers to compute the fraction of final sentences that contain a prespecified attribute, details in Appendix C. (2) Text Quality . Grammar (GRAM) (Warstadt et al., 2019) indicates the averaged grammaticality probabilities of all final sentences, evaluated by a RoBERTabased CoLA grammaticality model (Yang and Klein, 2021). Perplexity (PPL), we average the scores from GPT2 Base, GPT2 Medium and GPT2 Large version of GPT2 (Radford et al., 2019) as the final result. (3) Diversity . Following Li et al. (2015), we report the distinctness of the final results. Specifically, we count the number of unigrams, bigrams 6https://github.com/uberresearch/PPLM 7More details can be found in Appendix Band trigrams and then normalize them by the total number of words ( i.e., Dist1 / Dist2 / Dist3). Human Evaluation Following Qian et al. (2022), we also conduct the human evaluation. For each model, three crowdsource evaluators are shown 15 randomly selected samples (one per each attributeunrelated prefixes) for each generation task (Total: 75 samples for singleattribute CTG and 90 samples for multiattribute CTG), respectively. Then, they are asked to rate model results in two categories: the text quality of generation sentences and whether they contain the target attribute . Scores are ranged from 1 to 5, the higher the better.8 Tailor Settings Tailor Single denotes the singleattribute prompts. For multiattribute, Concat Simple means simply concatenating two singleattribute prompts and Tailor Concat is our nontraining evaluation, to keep in line with previous works (Yang and Klein, 2021; Dathathri et al., 2020), we use 15 attributeunrelated prefixes6and ask the model to continue writing with them (for each of the 15 prefixes, 100 completions are generated, total: 1500 for each attribute) while satisfying prespecified attribute as the final results.7 Automatic Evaluation Following Yang and Klein (2021); Dathathri et al. (2020), we automatically evaluate generation results from three aspects: (1) Correctness . We used RoBERTa Large (Liu et al., 2019) based attribute classifiers to compute the fraction of final sentences that contain a prespecified attribute, details in Appendix C. (2) Text Quality . Grammar (GRAM) (Warstadt et al., 2019) indicates the averaged grammaticality probabilities of all final sentences, evaluated by a RoBERTabased CoLA grammaticality model (Yang and Klein, 2021). Perplexity (PPL), we average the scores from GPT2 Base, GPT2 Medium and GPT2 Large version of GPT2 (Radford et al., 2019) as the final result. (3) Diversity . Following Li et al. (2015), we report the distinctness of the final results. Specifically, we count the number of unigrams, bigrams 6https://github.com/uberresearch/PPLM 7More details can be found in Appendix Band trigrams and then normalize them by the total number of words ( i.e., Dist1 / Dist2 / Dist3). Human Evaluation Following Qian et al. (2022), we also conduct the human evaluation. For each model, three crowdsource evaluators are shown 15 randomly selected samples (one per each attributeunrelated prefixes) for each generation task (Total: 75 samples for singleattribute CTG and 90 samples for multiattribute CTG), respectively. Then, they are asked to rate model results in two categories: the text quality of generation sentences and whether they contain the target attribute . Scores are ranged from 1 to 5, the higher the better.8 Tailor Settings Tailor Single denotes the singleattribute prompts. For multiattribute, Concat Simple means simply concatenating two singleattribute prompts and Tailor Concat is our nontraining  experiments in the previous section for different country groups. The results are shown in Figure 4. We also try sampling the same number of countries in each group. The results remain robust and are illustrated in AppendixF. Our ﬁndings indicate that EPLMs contain more knowledge about moral norms of the Rich West countries as opposed to nonwestern and nonrich countries. Similarly, EPLMs have captured a more accurate estimation of the moral norms in countries located in Oceania, North America, and Europe, as opposed to African, Asian, and South American countries. The empirical moral norm ratings from European countries in WVS are highly aligned with North American countries ( r= 0.938), which explains why their moral norms are inferred more accurately than nonEnglish speaking countries. Next, for each topic, we compare the zscores of the empirical moral ratings with the zscores of the GPT3PROBS inferred moral scores, using MannWhitney U rank test. The results reveal that “abortion”, “suicide”, “euthanasia”, “for a man to beat his wife”, “parents beating children”, “having casual sex”, “political violence”, and “death penalty” in nonwestern and nonrich countries are all en7https://worldpopulationreview.com/ countryrankings/westerncountriescoded as more morally appropriate than the actual data. Such misrepresentations of moral norms in these countries could lead to stereotypical content generation. We also ﬁnd that For Rich West countries, “homosexuality”, “divorce”, and “sex before marriage” are encoded as more morally inappropriate than the ground truth, ( p < 0.001for all, Bonferroni corrected). Such underlying moral biases, speciﬁcally toward “homosexuality” might stimulate the generation of harmful content and stigmatization of members of LGBTQ+, which has been reported in BERTbased EPLMs (Nozza et al., 2022). The results for the rest of the models are similar and are shown in Table 6 in the Appendix. Our method of clustering countries is simplistic and may overlook things such as the signiﬁcant diversity in religious beliefs within the NonRich-West category, and thus it does not reﬂect the nuanced biases that models may possess when it comes to moral norms inﬂuenced by different religious traditions. Nonetheless, our approach still serves as a valuable starting point for studying EPLM’s moral biases towards more ﬁnegrained religious and ethnic communities. 5.4 Cultural diversities and shared tendencies over the morality of different topics We next investigate whether EPLMs have captured the cultural diversities and shared tendencies over the morality of different topics (level 2 analysis). For example, people across cultures tend to disagree more about “divorce” than about “violence against other people” as depicted in Figure 1. Such cultural diversities for each topic can be measured by taking the standard deviation of the empirical moral ratings across different countries. The EPLMs’ inferred cultural diversities can similarly be measured by taking the standard deviation of the estimated ﬁnegrained moral scores for different countries. We then quantify the alignment between the two using Pearson correlation. Figure 5 shows the results for SBERT, GPT2LARGE, GPT3PROBS , and the rest are shown in Figure 8 in the Appendix. None of the correlations with the PEW survey were signiﬁcant. For WVS, SBERT, GPT2 andGPT2MEDIUM exhibited a signiﬁcant correlation ( p<0.001) with r= 0.618,r= 0.579, andr= 0.734respectively. The results for GPT3 are insigniﬁcant, suggesting that it is more challenging to correctly estimate433PEW surveyWorld V alue Survey Figure 3: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. The legends display the moral topics in the surveys. Similar topics in the World Value Surveys are shown with the same color. Figure 4: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey, analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). cultural controversies of topics for GPT3 . For example, stealing property is incorrectly estimated to be more controversial than abortion . 6 Finetuning language models on global surveys Finally, we explore the utilitybias tradeoff in encoding cultural moral knowledge into EPLMs by ﬁnetuning them on crosscultural surveys. The utility comes from increasing the cultural moral knowledge in these models, and the bias denotes their decreased ability to infer English moral norms, in addition to the cultural moral biases introduced to the model. We run our experiments on GPT2 , which our results suggest having captured minimum information about cultural moral norms compared to other autoregressive models. To ﬁnetune the model, for each participant from [Country] with [Moral rating] toward [Topic] , we designed a prompt with the structure“A person in [Country] believes [Topic] is [Moral rating] .”. We used the surveys’ wordings for [Moral rating] . Table 8 in the Appendix shows our prompts for WVS and PEW. These prompts constructed our data for ﬁnetuning, during which we maximize the probability of the next token. The ﬁnetuned models were evaluated on the same correlation tests introduced in the previous Sections 5.2, 5.3, and 5.4. The ﬁnetuning data was partitioned into training and evaluation sets using different strategies (i.e., Random, Countrybased, and Topicbased). For the Random strategy, we randomly selected 80% of the ﬁnetuning data for training the model. The topiccountry pairs not seen in the training data composed the evaluation set. For our Countrybased and Topicbased strategies, we randomly removed 20% of the countries ( n= 11 for WVS,n= 8 for PEW) and topics ( n= 4for WVS,n= 2for PEW) from the training data to compose the evalu434PEW surveyWorld V alues SurveyEstimated degree of cultural diversity Empirical degree of cultural diversity Figure 5: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The numerical indices are consistent with the legend indices in Table 5. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Train data Data partition strategy EvaluationPerformance on the Homogeneous norms WVSRandom 0.832∗∗∗↑ (0.271∗∗∗) 0.71∗∗∗↓ (0.80∗∗∗)Countrybased 0.759∗∗∗↑ (0.225∗∗) 0.72∗∗∗↓ Topicbased 0.508∗∗∗↑ (0.286∗∗∗) 0.70∗∗∗↓ PEWRandom 0.818∗∗∗↑ (0.204, n.s.) 0.64∗∗∗↓ Countrybased 0.764∗∗∗↑ (0.055, n.s.) 0.67∗∗∗↓ Topicbased 0.733∗∗∗↑ (−0.146, n.s.) 0.61∗∗∗↓ Table 1: Summary of ﬁnetuned GPT2 language model performance on inferring moral norms across cultures and the degradation of its performance on inferring Homogeneous moral norms. Values in parentheses show the performance before ﬁnetuning. The arrows and colors show performance increase (blue, ↑) and decrease (red, ↓) after ﬁnetuning. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001). ation set. See Appendix G for the total number of samples. Table 1 shows the gained utilities, that is the correlation test results between the ﬁnegrained moral scores inferred by the ﬁnetuned models and the empirical ﬁnegrained moral ratings. All ﬁnetuned models align better with the ground truth than the pretrained-only models (i.e., the values in parentheses). For both WVS and PEW, the Random strategy is indeed the best as each country and topic are seen in the training data at least once (but may not appear together as a pair). The ﬁnetuned models can also generalize their moral scores to unseen countries and topics. Repeating the experiment in Section 5.4 also shows substantial improvement in identifying cultural diversities of different topics by all ﬁnetuned models. For example, the WVS and PEWtrained models with Random strategy gain Pearson’s r values of 0.893, and 0.944respectively. The results for the rest of the models are shown in Table 7 in the Appendix.Nevertheless, the bias introduced during the ﬁnetuning decreases the performance on the Homogeneous norms dataset. This observation displays a tradeoff between cultural and homogeneous moral representations in language models. Moreover, injecting the crosscultural surveys into EPLMs might introduce additional social biases to the model that are captured through these surveys (Joseph and Morgan, 2020). In addition, we probe the best ﬁnetuned model (i.e., WVS with Random strategy) on its ability to capture the moral norms of nonwestern cultures by repeating the experiment in Section 5.3. The results in Figure 4 show that the ﬁnetuned GPT2 performs the best for all country groups. There is still a gap between western and nonwestern countries. However, basic ﬁnetuning proves to be effective in adapting EPLMs to the ground truth.4357 Discussion and conclusion We investigated whether English pretrained language models contain knowledge about moral norms across many different cultures. Our analyses show that large EPLMs capture moral norm variation to a certain degree, with the inferred norms being predominantly more accurate in western cultures than nonwestern cultures. Our ﬁnetuning analysis further suggests that EPLMs’ cultural moral knowledge can be improved using global surveys of moral norms, although this strategy reduces the capacity to estimate the English moral norms and potentially introduces new biases into the model. Given the increasing use of EPLMs in multicultural environments, our work highlights the importance of cultural diversity in automated inference of moral norms. Even when an action such as “political violence” is assessed by an EPLM as morally inappropriate in a homogeneous setting, the same issue may be inferred as morally appropriate for underrepresented cultures in these large language models. Future work can explore alternative and richer representations of cultural moral norms that go beyond the point estimation we presented here and investigate how those representations might better capture culturally diverse moral views. Limitations Although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future (Bloom, 2010; Bicchieri, 2005). Additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries. Moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point (Talat et al., 2021). Implementing a framework that incorporates both withincountry variation and temporal moral variation (Xie et al., 2019) is a potential future research direction. Currently, it is not clear whether the difference between EPLMs’ estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pretraining data, orthat the cultural moral norms mentioned in the pretraining data represent the perspective of an Englishspeaking person of another country. For example, a person from the United States could write about the moral norms in another country from a western perspective. A person from a nonwestern country could also write about their own moral views using English. These two cases have different implications and introduce different moral biases into the system. Potential risks We believe that the language models should not be used to prescribe ethics, and here we approach the moral norm inference problem from a descriptive perspective. However, we acknowledge modifying prompts could lead language models to generate ethical prescriptions for different cultures. Additionally, our ﬁnetuning approach could be exploited to implant cultural stereotypical biases into these models. Many topics shown in this work might be sensitive to some people yet more tolerable to some other people. Throughout the paper, we tried to emphasize that none of the moral norms, coming from either the models’ estimation or the empirical data, should be regarded as deﬁnitive values of right and wrong, and the moral judgments analyzed in this work do not reﬂect the opinions of the authors. Acknowledgements This work was supported by a SSHRC Insight Grant 435190272.436References Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent antimuslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’21, page 298–306, New York, NY , USA. Association for Computing Machinery. Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. 2022. Aligning to social norms and values in interactive narratives. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5994–6017, Seattle, United States. Association for Computational Linguistics. Arnav Arora, LucieAimée Kaffee, and Isabelle Augenstein. 2022. Probing PreTrained Language Models for CrossCultural Differences in Values. arXiv preprint arXiv:2203.13722 . Mohammad Atari, Jonathan Haidt, Jesse Graham, Sena Koleva, Sean Stevens, and Morteza Dehghani. 2022. Morality Beyond the WEIRD: How the Nomological Network of Morality Varies Across Cultures. Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, JeanFrançois Bonnefon, and Iyad Rahwan. 2018. The Moral Machine experiment. Nature , 563(7729):59– 64. Edmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rahwan, and JeanFrançois Bonnefon. 2020. Universals and variations in moral decisions made in 42 countries by 70,000 participants. Proceedings of the National Academy of Sciences , 117(5):2332–2337. Cristina Bicchieri. 2005. The grammar of society: The nature and dynamics of social norms . Cambridge University Press. Paul Bloom. 2010. How do morals change? Nature , 464(7288):490–490. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are fewshot learners. Advances in neural information processing systems , 33:1877–1901. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. Can ai language models replace human participants? Trends in Cognitive Sciences . Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings ofthe 2021 Conference on Empirical Methods in Natural Language Processing , pages 698–718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social Chemistry 101: Learning to Reason about Social and Moral Norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 653–670, Online. Association for Computational Linguistics. Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism. In Advances in Experimental Social Psychology , volume 47, pages 55–130. Elsevier. Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime DiezMedrano, Marta Lagos, Pippa Norris, E Ponarin, and B Puranen. 2021. World Values Survey: Round Seven – CountryPooled Dataﬁle. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. Data File Version , 2(0). Jonathan Haidt, Silvia Helena Koller, and Maria G Dias. 1993. Affect, culture, and morality, or is it wrong to eat your dog? Journal of personality and social psychology , 65(4):613. Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jind ˇrich Libovick `y, Alexander Fraser, and Kristian Kersting. 2022. Do Multilingual Language Models Capture Differing Moral Norms? arXiv preprint arXiv:2203.09904 . Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values. In International Conference on Learning Representations . Kathryn Iurino and Gerard Saucier. 2020. Testing measurement invariance of the Moral Foundations Questionnaire across 27 countries. Assessment , 27(2):365–372. Liwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards Machine Ethics and Norms. ArXiv , abs/2110.07574. Kenneth Joseph and Jonathan Morgan. 2020. When do word embeddings accurately reﬂect surveys on our beliefs about people? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4392–4415, Online. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.4372020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 . Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush V osoughi. 2022. Aligning Generative Language Models with Human Values. In Findings of the Association for Computational Linguistics: NAACL 2022 , pages 241–252. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 . Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 reallife anecdotes. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 35, pages 13470–13479. Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 5356–5371, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2398–2406, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, Anne Lauscher, and Dirk Hovy. 2022. Measuring harmful sentence completion in language models for LGBTQIA+ individuals. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 26–34, Dublin, Ireland. Association for Computational Linguistics. Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and DitYan Yeung. 2021. Probing toxic content in large pretrained language models. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 4262–4274, Online. Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. PEW Research Center. 2014. Global Attitudes survey . Washington, D.C. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5477–5490, Online. Association for Computational Linguistics. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian Kersting. 2022. Large pretrained language models contain humanlike biases of what is right and wrong to do. Nature Machine Intelligence , 4(3):258–268. Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2021. A Word on Machine Ethics: A Response to Jiang et al.(2021). arXiv preprint arXiv:2111.04158 . Samia Touileb, Lilja Øvrelid, and Erik Velldal. 2022. Occupational biases in Norwegian and multilingual language models. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP) , pages 200–211, Seattle, Washington. Association for Computational Linguistics. Jackson Trager, Alireza S Ziabari, Aida Mostafazadeh Davani, Preni Golazazian, Farzan KarimiMalekabadi, Ali Omrani, Zhihe Li, Brendan Kennedy, Nils Karl Reimer, Melissa Reyes, et al. 2022. The Moral Foundations Reddit Corpus. arXiv preprint arXiv:2208.05545 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NIPS’17, page 6000–6010, Red Hook, NY , USA. Curran Associates Inc. Jing Yi Xie, Renato Ferreira Pinto Junior, Graeme Hirst, and Yang Xu. 2019. Textbased inference of moral sentiment change. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 4654–4663, Hong Kong, China. Association for Computational Linguistics. Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and KaiWei Chang. 2022. Geomlama: Geodiverse commonsense probing on multilingual pretrained language models. arXiv preprint arXiv:2205.12247 .438Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Storylike Visual Explanations by Watching Movies and Reading Books. CoRR , abs/1506.06724. A Data license Both World Values Survey and PEW survey are publicly available to use for research purposes. We accept and follow the terms and conditions for using these datasets, which can be found in https://www.worldvaluessurvey. org/WVSContents.jsp?CMSID=Documentation , and https://www.pewresearch.org/about/ termsand-conditions/ . B Comparison of humanrated and machinescored moral norms Figure 6 shows the comparison between humanrated moral norms in PEW, and the moral scores inferred by SBERT (Reimers and Gurevych, 2019). C Probing experiments Table 2 shows our prompt design for probing ﬁnegrained moral norms in EPLMs. As mentioned in the main text, we repeat our probing experiment for GPT2 models and GPT3PROBS with another template “People in [Country] believe [Topic] is [Moral Judgment]”. The results are substantially worse than our initial template, suggesting that extracting the moral knowledge in language models is sensitive to the wording used in the input. The results for the ﬁnegrained analysis (level 1 analysis) and the cultural diversities and shared tendencies (level 2 analysis) with this template are shown in Table 4. In all experiments, we used a single NVIDIA TITAN V GPU. Each probing experiment took approximately 1 hour to complete. D Homogeneous moral norm inference Table 3 shows the detailed values of the correlation tests in our homogeneous moral norm inference experiment. E Finegrained cultural variation of moral norm Figure 7 and Figure 8 show the result of our ﬁnegrained cultural moral inference, and inference ofcultural diversities and shared tendencies respectively for GPT2, GPT2MEDIUM , and GPT3QA. The numerical indices in Figure 8 are consistent with the indices in Table 5. F Sampling for cultural clusters Since in section 5.3 there are a different number of countries in each group, we redo the experiment by randomly sampling the same number of countries (n= 11 for Rich West grouping, n= 5for continent grouping) and repeating the sampling process for50times. The results and the general pattern remain the same and are depicted in Figure 9. G Details of ﬁnetuning on global surveys Table 8 shows the Moral rating in our prompt design for constructing our ﬁnetuning dataset. For example, The World Value Survey represents the two ends of the ratings scale where 1 is “Never justiﬁable” and 10 is “Always justiﬁable”. The options in between are presented to the participants in a 10point scale. Therefore, we mapped these options to different prompts that are semantically similar and in between the two ends. For example, if a participant from the United States rated stealing property as 2, which is slightly more positive than the ﬁrst option (“Never justiﬁable”), we mapped this rating to “not justiﬁable”, creating the prompt “A person in the United States believes stealing property is not justifiable. ” for our ﬁnetuning data. Since there are a different number of participants from each country, in order to balance this dataset, we randomly select 100samples for each topiccountry pair and removed the rest of the utterances from the training data. We ﬁnetuned GPT2 on one epoch, with a batch size of 8, learning rate of 5e−5, and weight decay of 0.01. The number of training and evaluation samples for all data partition strategies are shown in Table 9. In all experiments, we used a single NVIDIA TITAN V GPU. Finetuning and evaluation took approximately 2 hours to complete for each model.439Figure 6: Comparison of humanrated and machinescored moral norms across cultures. Left: Boxplots of human ratings of moral norms across cultures in the PEW survey. Each dot represents the empirical average of participants’ ratings for a morally relevant topic (e.g., “having an abortion”) within a country. Right: Corresponding moral scores estimated by a language model (SentenceBERT) (Reimers and Gurevych, 2019). Each dot represents the moral score obtained by probing the language model in a given country. Model Prompt SentenceBERT [Topic] in [Country]. GPT2 models and GPT3PROBSIn [Country] [Topic] is [Moral judgement]. People in [Country] believe [Topic] is [Moral judgement]. GPT3QA (for PEW)Do people in [Country] believe that [Topic] is: 1) Morally acceptable 2) Not a moral issue 3) Morally unacceptable. GPT3QA (for WVS)Do people in [Country] believe that [Topic] is: 1) Always Justifiable 2) Something in between 3) Never justifiable. Table 2: Prompting design used for estimating the ﬁnegrained moral norms in different language models. In our homogeneous norm inference, we remove “ In [country] ” from the prompts. Model World Values Survey (n= 1,028)PEW survey ( n= 312 )Homogeneous norms ( n= 100 ) SBERT 0.210∗∗∗−0.038(n.s.) 0.79∗∗∗ GPT2 0.176∗∗∗−0.069(n.s.) 0.80∗∗∗ GPT2MEDIUM 0.181∗∗∗0.033(n.s.) 0.79∗∗∗ GPT2LARGE 0.226∗∗∗0.157(n.s.) 0.76∗∗∗ GPT3QA 0.330∗∗∗0.391∗∗∗0.79∗∗∗ GPT3PROBS 0.346∗∗∗0.340∗∗∗0.85∗∗∗ Table 3: Performance of pretrained language models (without cultural prompts) on inferring 1) homogeneous westernized moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey data.440PEW surveyWorld V alue SurveyFigure 7: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. Similar topics in the World Value Surveys are shown with the same color. PEW surveyWorld V alues Survey Estimated degree of cultural diversity Empirical degree of cultural diversity Figure 8: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Figure 9: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The results are generated by sampling and the error bars show the conﬁdence intervals with α= 0.05.441Data modelFinegrained evaluation of moral normsEvaluation on cultural diversity and shared tendencies WVSGPT3PROBS 0.078∗−0.176 GPT2 −0.114∗∗∗0.231 GPT2MEDIUM −0.261∗∗∗−0.357 GPT2LARGE −0.07∗−0.356 PEWGPT3PROBS 0.539∗∗∗0.041 GPT2 0.168∗∗0.566 GPT2MEDIUM 0.165∗∗0.184 GPT2LARGE 0.19∗∗∗0.542 Table 4: Performance of pretrained autoregressive language models on identifying ﬁnegrained moral norms and cultural diversities and shared tendencies, using the prompt template “People in [Country] believe [Topic] is [Moral judgment]\". The values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” forp<0.05,0.01,0.001respectively). World Values Survey Index Topic 1 stealing property 2 euthanasia 3 sex before marriage 4 violence against other people 5 cheating on taxes 6 avoiding a fare on public transport 7 abortion 8 suicide 9 someone accepting a bribe on a course of their duties 10 terrorism as a political, ideological, or religious mean 11 homosexuality 12 parents beating children 13 prostitution 14 divorce 15 political violence 16 death penalty 17 claiming governments beneﬁts to which you are not entitled 18 for a man to beat his wife 19 having casual sex PEW survey 1 using contraceptives 2 getting a divorce 3 having an abortion 4 homosexuality 5 drinking alcohol 6 married people having an affair 7 gambling 8 sex between unmarried adults Table 5: Numerical indexing for topics in moral surveys.442ModelPositively evaluated topics for nonrich and nonwestern countriesNegatively evaluated topics for RichWest countries SBERTsex before marriage∗∗, homosexuality∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, claiming government beneﬁts to which you are not entitled∗∗∗, someone accepting a bribe in the course of their duties∗∗∗sex before marriage∗∗∗, euthanasia∗∗∗, divorce∗∗∗, death penalty∗∗∗, parents beating children∗∗∗ GPT2abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2MEDIUMeuthanasia∗∗∗, abortion∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗, violence against other people∗∗∗, stealing property∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2LARGEeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT3QAhaving casual sex∗∗, abortion∗∗, avoiding a fare on public transport∗∗∗, cheating on taxes∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗sex before marriage∗∗∗, divorce∗∗, death penalty∗∗, prostitution∗∗, parents beating children∗∗, suicide∗∗, for a man to beat his wife∗∗∗, stealing property∗∗ GPT3PROBSeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, death penalty∗∗∗, suicide∗∗∗, political violence∗∗∗, for a man to beat his wife∗∗∗sex before marriage∗∗∗, homosexuality∗∗∗, divorce∗∗ Table 6: Topics evaluated as morally positive for nonrich and nonwestern countries and morally negative for RichWest countries, in comparison to the ground truth in these countries. In each entry, the topics are sorted from the most controversial (i.e., having the highest degree of cultural diversity) to the least controversial. The asterisks indicate the signiﬁcance levels of MannWhitney U rank test after Bonferroni pvalue correction (“*”, “**”, “***” forp<0.05,0.01,0.001respectively).443Train data Data partition strategy Evaluation WVSRandom 0.893∗∗∗↑ (0.579∗∗∗) Countrybased 0.894∗∗∗↑ Topicbased 0.835∗∗∗↑ PEWRandom 0.944∗∗↑ (n.s.) Countrybased 0.839∗↑ Topicbased 0.953∗∗∗↑ Table 7: Summary of ﬁnetuned GPT2 language model performance in inferring the cultural diversities and shared tendencies over the morality of different topics. The arrows and colors show performance increase (blue, ↑) and decrease (red,↓) after ﬁnetuning. All values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). Nonsigniﬁcant results are shown by “n.s.”. Dataset Rating [Moral rating] in ﬁnetuning prompts WVS1 never justifiable [2, 3, 4] not justifiable [5, 6] somewhat justifiable [7, 8, 9] justifiable 10 always justifiable PEW1 morally unacceptable 2 not a moral issue 3 morally acceptable Table 8: Different prompting designs for ﬁnetuning language models on the global survey datasets. Data Data partition strategy Training samples Evaluation sample pairs WVSRandom 82200 206 Countrybased 82600 202 Topicbased 81200 216 PEWRandom 24900 63 Countrybased 24800 64 Topicbased 23400 78 Table 9: Number of samples in training and evaluation datasets for ﬁnetuning GPT2 on global surveys of morality.444ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? 8 /square\u0013A2. Did you discuss any potential risks of your work? 8 /square\u0013A3. Do the experimental setup, including hyperparameter search and bestfound hyperparameter values? We did not do any hyperparameter search. /square\u0013C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of Evaluation and results We evaluate EPLMs’ moral knowledge with respect to 1) homogeneous moral norms, 2) ﬁnegrained moral norms across cultures, and 3) cultural diversities and shared tendencies on moral judgment of different topics. 5.1 Homogeneous moral norm inference For homogeneous moral norm inference, we compute Pearson correlation between 1) the empirical homogeneous moral ratings, obtained by aggregating the human moral ratings toward a topic from all countries, and 2) language model inferred moral scores, estimated from our homogeneous probing method (i.e., without specifying country in prompts). Figure 2 shows the results on World Values Survey (n= 1,028), PEW survey ( n= 312 ), and the Homogeneous norms datasets ( n= 100 ). The high correlation of GPT2 andGPT3 moral scores with the Homogeneous norms dataset indicate that our methodology does indeed capture the embedded moral biases in these models, with similar performance to the method proposed by Schramowski et al. (2022) for SBERT (r= 0.79), and higher forGPT3PROBS (r= 0.85). The moral norms in this dataset are typically more globally agreeable (e.g., You should not kill people ) than topics in WVS and PEW. As expected, EPLMs are less correlated with WVS and PEW, since their moral biases are derived from pretraining on English and westernized data. Aggregated ratings in WVS and PEW, however, capture a more global view toward moral issues, which are also morally contentious (e.g., “getting a divorce”). Table 3 in Appendix includes the values for this experiment. Figure 2: Performance of EPLMs (without cultural prompts) on inferring 1) English moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey data. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). 5.2 Finegrained cultural variation of moral norms toward different topics Going beyond probing EPLMs for their general knowledge of moral norms, we assess whether they can accurately identify the moral norms of different cultures (level 1 analysis). Using our ﬁnegrained probing approach described in Section 3, we compute Pearson correlation between EPLMs’ moral scores and the ﬁnegrained moral ratings from the ground truth. Each sample pair in the correlation test corresponds to 1) the moral norms estimated by EPLMs for a country cand a topict, and 2) the empirical average of moral ratings toward topic t from all the participants in the country c. Figure 3 summarizes the results for SBERT, GPT2LARGE , and GPT3PROBS models, and the rest of the models are shown in Figure 7 in the Appendix. To facilitate direct comparison, the estimated moral scores are normalized to a range of −1to1, where−1, 0, and 1 indicate morally negative, morally neutral, and morally positive norms, respectively. GPT3QA andGPT3PROBS both show a relatively high correlation with the cultural variations of moral norms ( r= 0.352,r= 0.411, p<0.001, for both), and GPT2LARGE achieves a correlation of r= 0.207(p <0.001) in WVS wheren= 1,028. The correlations are relatively better for PEW ( n= 312 ) withr= 0.657, r= 0.503, andr= 0.468forGPT3QA, GPT3PROBS andGPT2LARGE respectively. These results show that EPLMs have captured some432knowledge about the moral norms of different cultures, but with much less accuracy (especially for GPT2 andSBERT ) compared to their inference of English moral norms shown in the previous analysis. In addition, we check whether GPT3 ’s high correlation with PEW is because it has seen and memorized the empirical data. Our investigation shows thatGPT3 has seen the data during pretraining, as it can generate the sentences used on the survey website. However, the scores suggested by GPT3 text generation and the countries’ rankings based on their ratings are different from the ground truth data. 5.3 Culture clustering through ﬁnegrained moral inference EPLMs’ ﬁnegrained knowledge of moral norms, inspected in the previous experiment, might be more accuracte for western cultures than other cultures. We investigate this claim by clustering countries based on 1) their WesternEastern economic status (i.e., Rich West grouping)7, and 2) their continent (i.e., geographical grouping). We repeat the experiments in the previous section for different country groups. The results are shown in Figure 4. We also try sampling the same number of countries in each group. The results remain robust and are illustrated in AppendixF. Our ﬁndings indicate that EPLMs contain more knowledge about moral norms of the Rich West countries as opposed to nonwestern and nonrich countries. Similarly, EPLMs have captured a more accurate estimation of the moral norms in countries located in Oceania, North America, and Europe, as opposed to African, Asian, and South American countries. The empirical moral norm ratings from European countries in WVS are highly aligned with North American countries ( r= 0.938), which explains why their moral norms are inferred more accurately than nonEnglish speaking countries. Next, for each topic, we compare the zscores of the empirical moral ratings with the zscores of the GPT3PROBS inferred moral scores, using MannWhitney U rank test. The results reveal that “abortion”, “suicide”, “euthanasia”, “for a man to beat his wife”, “parents beating children”, “having casual sex”, “political violence”, and “death penalty” in nonwestern and nonrich countries are all en7https://worldpopulationreview.com/ countryrankings/westerncountriescoded as more morally appropriate than the actual data. Such misrepresentations of moral norms in these countries could lead to stereotypical content generation. We also ﬁnd that For Rich West countries, “homosexuality”, “divorce”, and “sex before marriage” are encoded as more morally inappropriate than the ground truth, ( p < 0.001for all, Bonferroni corrected). Such underlying moral biases, speciﬁcally toward “homosexuality” might stimulate the generation of harmful content and stigmatization of members of LGBTQ+, which has been reported in BERTbased EPLMs (Nozza et al., 2022). The results for the rest of the models are similar and are shown in Table 6 in the Appendix. Our method of clustering countries is simplistic and may overlook things such as the signiﬁcant diversity in religious beliefs within the NonRich-West category, and thus it does not reﬂect the nuanced biases that models may possess when it comes to moral norms inﬂuenced by different religious traditions. Nonetheless, our approach still serves as a valuable starting point for studying EPLM’s moral biases towards more ﬁnegrained religious and ethnic communities. 5.4 Cultural diversities and shared tendencies over the morality of different topics We next investigate whether EPLMs have captured the cultural diversities and shared tendencies over the morality of different topics (level 2 analysis). For example, people across cultures tend to disagree more about “divorce” than about “violence against other people” as depicted in Figure 1. Such cultural diversities for each topic can be measured by taking the standard deviation of the empirical moral ratings across different countries. The EPLMs’ inferred cultural diversities can similarly be measured by taking the standard deviation of the estimated ﬁnegrained moral scores for different countries. We then quantify the alignment between the two using Pearson correlation. Figure 5 shows the results for SBERT, GPT2LARGE, GPT3PROBS , and the rest are shown in Figure 8 in the Appendix. None of the correlations with the PEW survey were signiﬁcant. For WVS, SBERT, GPT2 andGPT2MEDIUM exhibited a signiﬁcant correlation ( p<0.001) with r= 0.618,r= 0.579, andr= 0.734respectively. The results for GPT3 are insigniﬁcant, suggesting that it is more challenging to correctly estimate433PEW surveyWorld V alue Survey Figure 3: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. The legends display the moral topics in the surveys. Similar topics in the World Value Surveys are shown with the same color. Figure 4: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey, analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). cultural controversies of topics for GPT3 . For example, stealing property is incorrectly estimated to be more controversial than abortion . 6 Finetuning language models on global surveys Finally, we explore the utilitybias tradeoff in encoding cultural moral knowledge into EPLMs by ﬁnetuning them on crosscultural surveys. The utility comes from increasing the cultural moral knowledge in these models, and the bias denotes their decreased ability to infer English moral norms, in addition to the cultural moral biases introduced to the model. We run our experiments on GPT2 , which our results suggest having captured minimum information about cultural moral norms compared to other autoregressive models. To ﬁnetune the model, for each participant from [Country] with [Moral rating] toward [Topic] , we designed a prompt with the structure“A person in [Country] believes [Topic] is [Moral rating] .”. We used the surveys’ wordings for [Moral rating] . Table 8 in the Appendix shows our prompts for WVS and PEW. These prompts constructed our data for ﬁnetuning, during which we maximize the probability of the next token. The ﬁnetuned models were evaluated on the same correlation tests introduced in the previous Sections 5.2, 5.3, and 5.4. The ﬁnetuning data was partitioned into training and evaluation sets using different strategies (i.e., Random, Countrybased, and Topicbased). For the Random strategy, we randomly selected 80% of the ﬁnetuning data for training the model. The topiccountry pairs not seen in the training data composed the evaluation set. For our Countrybased and Topicbased strategies, we randomly removed 20% of the countries ( n= 11 for WVS,n= 8 for PEW) and topics ( n= 4for WVS,n= 2for PEW) from the training data to compose the evalu434PEW surveyWorld V alues SurveyEstimated degree of cultural diversity Empirical degree of cultural diversity Figure 5: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The numerical indices are consistent with the legend indices in Table 5. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Train data Data partition strategy EvaluationPerformance on the Homogeneous norms WVSRandom 0.832∗∗∗↑ (0.271∗∗∗) 0.71∗∗∗↓ (0.80∗∗∗)Countrybased 0.759∗∗∗↑ (0.225∗∗) 0.72∗∗∗↓ Topicbased 0.508∗∗∗↑ (0.286∗∗∗) 0.70∗∗∗↓ PEWRandom 0.818∗∗∗↑ (0.204, n.s.) 0.64∗∗∗↓ Countrybased 0.764∗∗∗↑ (0.055, n.s.) 0.67∗∗∗↓ Topicbased 0.733∗∗∗↑ (−0.146, n.s.) 0.61∗∗∗↓ Table 1: Summary of ﬁnetuned GPT2 language model performance on inferring moral norms across cultures and the degradation of its performance on inferring Homogeneous moral norms. Values in parentheses show the performance before ﬁnetuning. The arrows and colors show performance increase (blue, ↑) and decrease (red, ↓) after ﬁnetuning. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001). ation set. See Appendix G for the total number of samples. Table 1 shows the gained utilities, that is the correlation test results between the ﬁnegrained moral scores inferred by the ﬁnetuned models and the empirical ﬁnegrained moral ratings. All ﬁnetuned models align better with the ground truth than the pretrained-only models (i.e., the values in parentheses). For both WVS and PEW, the Random strategy is indeed the best as each country and topic are seen in the training data at least once (but may not appear together as a pair). The ﬁnetuned models can also generalize their moral scores to unseen countries and topics. Repeating the experiment in Section 5.4 also shows substantial improvement in identifying cultural diversities of different topics by all ﬁnetuned models. For example, the WVS and PEWtrained models with Random strategy gain Pearson’s r values of 0.893, and 0.944respectively. The results for the rest of the models are shown in Table 7 in the Appendix.Nevertheless, the bias introduced during the ﬁnetuning decreases the performance on the Homogeneous norms dataset. This observation displays a tradeoff between cultural and homogeneous moral representations in language models. Moreover, injecting the crosscultural surveys into EPLMs might introduce additional social biases to the model that are captured through these surveys (Joseph and Morgan, 2020). In addition, we probe the best ﬁnetuned model (i.e., WVS with Random strategy) on its ability to capture the moral norms of nonwestern cultures by repeating the experiment in Section 5.3. The results in Figure 4 show that the ﬁnetuned GPT2 performs the best for all country groups. There is still a gap between western and nonwestern countries. However, basic ﬁnetuning proves to be effective in adapting EPLMs to the ground truth.4357 Discussion and conclusion We investigated whether English pretrained language models contain knowledge about moral norms across many different cultures. Our analyses show that large EPLMs capture moral norm variation to a certain degree, with the inferred norms being predominantly more accurate in western cultures than nonwestern cultures. Our ﬁnetuning analysis further suggests that EPLMs’ cultural moral knowledge can be improved using global surveys of moral norms, although this strategy reduces the capacity to estimate the English moral norms and potentially introduces new biases into the model. Given the increasing use of EPLMs in multicultural environments, our work highlights the importance of cultural diversity in automated inference of moral norms. Even when an action such as “political violence” is assessed by an EPLM as morally inappropriate in a homogeneous setting, the same issue may be inferred as morally appropriate for underrepresented cultures in these large language models. Future work can explore alternative and richer representations of cultural moral norms that go beyond the point estimation we presented here and investigate how those representations might better capture culturally diverse moral views. Limitations Although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future (Bloom, 2010; Bicchieri, 2005). Additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries. Moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point (Talat et al., 2021). Implementing a framework that incorporates both withincountry variation and temporal moral variation (Xie et al., 2019) is a potential future research direction. Currently, it is not clear whether the difference between EPLMs’ estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pretraining data, orthat the cultural moral norms mentioned in the pretraining data represent the perspective of an Englishspeaking person of another country. For example, a person from the United States could write about the moral norms in another country from a western perspective. A person from a nonwestern country could also write about their own moral views using English. These two cases have different implications and introduce different moral biases into the system. Potential risks We believe that the language models should not be used to prescribe ethics, and here we approach the moral norm inference problem from a descriptive perspective. However, we acknowledge modifying prompts could lead language models to generate ethical prescriptions for different cultures. Additionally, our ﬁnetuning approach could be exploited to implant cultural stereotypical biases into these models. Many topics shown in this work might be sensitive to some people yet more tolerable to some other people. Throughout the paper, we tried to emphasize that none of the moral norms, coming from either the models’ estimation or the empirical data, should be regarded as deﬁnitive values of right and wrong, and the moral judgments analyzed in this work do not reﬂect the opinions of the authors. Acknowledgements This work was supported by a SSHRC Insight Grant 435190272.436References Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent antimuslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’21, page 298–306, New York, NY , USA. Association for Computing Machinery. Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. 2022. Aligning to social norms and values in interactive narratives. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5994–6017, Seattle, United States. Association for Computational Linguistics. Arnav Arora, LucieAimée Kaffee, and Isabelle Augenstein. 2022. Probing PreTrained Language Models for CrossCultural Differences in Values. arXiv preprint arXiv:2203.13722 . Mohammad Atari, Jonathan Haidt, Jesse Graham, Sena Koleva, Sean Stevens, and Morteza Dehghani. 2022. Morality Beyond the WEIRD: How the Nomological Network of Morality Varies Across Cultures. Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, JeanFrançois Bonnefon, and Iyad Rahwan. 2018. The Moral Machine experiment. Nature , 563(7729):59– 64. Edmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rahwan, and JeanFrançois Bonnefon. 2020. Universals and variations in moral decisions made in 42 countries by 70,000 participants. Proceedings of the National Academy of Sciences , 117(5):2332–2337. Cristina Bicchieri. 2005. The grammar of society: The nature and dynamics of social norms . Cambridge University Press. Paul Bloom. 2010. How do morals change? Nature , 464(7288):490–490. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are fewshot learners. Advances in neural information processing systems , 33:1877–1901. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. Can ai language models replace human participants? Trends in Cognitive Sciences . Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings ofthe 2021 Conference on Empirical Methods in Natural Language Processing , pages 698–718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social Chemistry 101: Learning to Reason about Social and Moral Norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 653–670, Online. Association for Computational Linguistics. Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism. In Advances in Experimental Social Psychology , volume 47, pages 55–130. Elsevier. Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime DiezMedrano, Marta Lagos, Pippa Norris, E Ponarin, and B Puranen. 2021. World Values Survey: Round Seven – CountryPooled Dataﬁle. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. Data File Version , 2(0). Jonathan Haidt, Silvia Helena Koller, and Maria G Dias. 1993. Affect, culture, and morality, or is it wrong to eat your dog? Journal of personality and social psychology , 65(4):613. Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jind ˇrich Libovick `y, Alexander Fraser, and Kristian Kersting. 2022. Do Multilingual Language Models Capture Differing Moral Norms? arXiv preprint arXiv:2203.09904 . Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values. In International Conference on Learning Representations . Kathryn Iurino and Gerard Saucier. 2020. Testing measurement invariance of the Moral Foundations Questionnaire across 27 countries. Assessment , 27(2):365–372. Liwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards Machine Ethics and Norms. ArXiv , abs/2110.07574. Kenneth Joseph and Jonathan Morgan. 2020. When do word embeddings accurately reﬂect surveys on our beliefs about people? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4392–4415, Online. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.4372020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 . Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush V osoughi. 2022. Aligning Generative Language Models with Human Values. In Findings of the Association for Computational Linguistics: NAACL 2022 , pages 241–252. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 . Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 reallife anecdotes. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 35, pages 13470–13479. Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 5356–5371, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2398–2406, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, Anne Lauscher, and Dirk Hovy. 2022. Measuring harmful sentence completion in language models for LGBTQIA+ individuals. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 26–34, Dublin, Ireland. Association for Computational Linguistics. Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and DitYan Yeung. 2021. Probing toxic content in large pretrained language models. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 4262–4274, Online. Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. PEW Research Center. 2014. Global Attitudes survey . Washington, D.C. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5477–5490, Online. Association for Computational Linguistics. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian Kersting. 2022. Large pretrained language models contain humanlike biases of what is right and wrong to do. Nature Machine Intelligence , 4(3):258–268. Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2021. A Word on Machine Ethics: A Response to Jiang et al.(2021). arXiv preprint arXiv:2111.04158 . Samia Touileb, Lilja Øvrelid, and Erik Velldal. 2022. Occupational biases in Norwegian and multilingual language models. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP) , pages 200–211, Seattle, Washington. Association for Computational Linguistics. Jackson Trager, Alireza S Ziabari, Aida Mostafazadeh Davani, Preni Golazazian, Farzan KarimiMalekabadi, Ali Omrani, Zhihe Li, Brendan Kennedy, Nils Karl Reimer, Melissa Reyes, et al. 2022. The Moral Foundations Reddit Corpus. arXiv preprint arXiv:2208.05545 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NIPS’17, page 6000–6010, Red Hook, NY , USA. Curran Associates Inc. Jing Yi Xie, Renato Ferreira Pinto Junior, Graeme Hirst, and Yang Xu. 2019. Textbased inference of moral sentiment change. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 4654–4663, Hong Kong, China. Association for Computational Linguistics. Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and KaiWei Chang. 2022. Geomlama: Geodiverse commonsense probing on multilingual pretrained language models. arXiv preprint arXiv:2205.12247 .438Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Storylike Visual Explanations by Watching Movies and Reading Books. CoRR , abs/1506.06724. A Data license Both World Values Survey and PEW survey are publicly available to use for research purposes. We accept and follow the terms and conditions for using these datasets, which can be found in https://www.worldvaluessurvey. org/WVSContents.jsp?CMSID=Documentation , and https://www.pewresearch.org/about/ termsand-conditions/ . B Comparison of humanrated and machinescored moral norms Figure 6 shows the comparison between humanrated moral norms in PEW, and the moral scores inferred by SBERT (Reimers and Gurevych, 2019). C Probing experiments Table 2 shows our prompt design for probing ﬁnegrained moral norms in EPLMs. As mentioned in the main text, we repeat our probing experiment for GPT2 models and GPT3PROBS with another template “People in [Country] believe [Topic] is [Moral Judgment]”. The results are substantially worse than our initial template, suggesting that extracting the moral knowledge in language models is sensitive to the wording used in the input. The results for the ﬁnegrained analysis (level 1 analysis) and the cultural diversities and shared tendencies (level 2 analysis) with this template are shown in Table 4. In all experiments, we used a single NVIDIA TITAN V GPU. Each probing experiment took approximately 1 hour to complete. D Homogeneous moral norm inference Table 3 shows the detailed values of the correlation tests in our homogeneous moral norm inference experiment. E Finegrained cultural variation of moral norm Figure 7 and Figure 8 show the result of our ﬁnegrained cultural moral inference, and inference ofcultural diversities and shared tendencies respectively for GPT2, GPT2MEDIUM , and GPT3QA. The numerical indices in Figure 8 are consistent with the indices in Table 5. F Sampling for cultural clusters Since in section 5.3 there are a different number of countries in each group, we redo the experiment by randomly sampling the same number of countries (n= 11 for Rich West grouping, n= 5for continent grouping) and repeating the sampling process for50times. The results and the general pattern remain the same and are depicted in Figure 9. G Details of ﬁnetuning on global surveys Table 8 shows the Moral rating in our prompt design for constructing our ﬁnetuning dataset. For example, The World Value Survey represents the two ends of the ratings scale where 1 is “Never justiﬁable” and 10 is “Always justiﬁable”. The options in between are presented to the participants in a 10point scale. Therefore, we mapped these options to different prompts that are semantically similar and in between the two ends. For example, if a participant from the United States rated stealing property as 2, which is slightly more positive than the ﬁrst option (“Never justiﬁable”), we mapped this rating to “not justiﬁable”, creating the prompt “A person in the United States believes stealing property is not justifiable. ” for our ﬁnetuning data. Since there are a different number of participants from each country, in order to balance this dataset, we randomly select 100samples for each topiccountry pair and removed the rest of the utterances from the training data. We ﬁnetuned GPT2 on one epoch, with a batch size of 8, learning rate of 5e−5, and weight decay of 0.01. The number of training and evaluation samples for all data partition strategies are shown in Table 9. In all experiments, we used a single NVIDIA TITAN V GPU. Finetuning and evaluation took approximately 2 hours to complete for each model.439Figure 6: Comparison of humanrated and machinescored moral norms across cultures. Left: Boxplots of human ratings of moral norms across cultures in the PEW survey. Each dot represents the empirical average of participants’ ratings for a morally relevant topic (e.g., “having an abortion”) within a country. Right: Corresponding moral scores estimated by a language model (SentenceBERT) (Reimers and Gurevych, 2019). Each dot represents the moral score obtained by probing the language model in a given country. Model Prompt SentenceBERT [Topic] in [Country]. GPT2 models and GPT3PROBSIn [Country] [Topic] is [Moral judgement]. People in [Country] believe [Topic] is [Moral judgement]. GPT3QA (for PEW)Do people in [Country] believe that [Topic] is: 1) Morally acceptable 2) Not a moral issue 3) Morally unacceptable. GPT3QA (for WVS)Do people in [Country] believe that [Topic] is: 1) Always Justifiable 2) Something in between 3) Never justifiable. Table 2: Prompting design used for estimating the ﬁnegrained moral norms in different language models. In our homogeneous norm inference, we remove “ In [country] ” from the prompts. Model World Values Survey (n= 1,028)PEW survey ( n= 312 )Homogeneous norms ( n= 100 ) SBERT 0.210∗∗∗−0.038(n.s.) 0.79∗∗∗ GPT2 0.176∗∗∗−0.069(n.s.) 0.80∗∗∗ GPT2MEDIUM 0.181∗∗∗0.033(n.s.) 0.79∗∗∗ GPT2LARGE 0.226∗∗∗0.157(n.s.) 0.76∗∗∗ GPT3QA 0.330∗∗∗0.391∗∗∗0.79∗∗∗ GPT3PROBS 0.346∗∗∗0.340∗∗∗0.85∗∗∗ Table 3: Performance of pretrained language models (without cultural prompts) on inferring 1) homogeneous westernized moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey data.440PEW surveyWorld V alue SurveyFigure 7: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. Similar topics in the World Value Surveys are shown with the same color. PEW surveyWorld V alues Survey Estimated degree of cultural diversity Empirical degree of cultural diversity Figure 8: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Figure 9: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The results are generated by sampling and the error bars show the conﬁdence intervals with α= 0.05.441Data modelFinegrained evaluation of moral normsEvaluation on cultural diversity and shared tendencies WVSGPT3PROBS 0.078∗−0.176 GPT2 −0.114∗∗∗0.231 GPT2MEDIUM −0.261∗∗∗−0.357 GPT2LARGE −0.07∗−0.356 PEWGPT3PROBS 0.539∗∗∗0.041 GPT2 0.168∗∗0.566 GPT2MEDIUM 0.165∗∗0.184 GPT2LARGE 0.19∗∗∗0.542 Table 4: Performance of pretrained autoregressive language models on identifying ﬁnegrained moral norms and cultural diversities and shared tendencies, using the prompt template “People in [Country] believe [Topic] is [Moral judgment]\". The values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” forp<0.05,0.01,0.001respectively). World Values Survey Index Topic 1 stealing property 2 euthanasia 3 sex before marriage 4 violence against other people 5 cheating on taxes 6 avoiding a fare on public transport 7 abortion 8 suicide 9 someone accepting a bribe on a course of their duties 10 terrorism as a political, ideological, or religious mean 11 homosexuality 12 parents beating children 13 prostitution 14 divorce 15 political violence 16 death penalty 17 claiming governments beneﬁts to which you are not entitled 18 for a man to beat his wife 19 having casual sex PEW survey 1 using contraceptives 2 getting a divorce 3 having an abortion 4 homosexuality 5 drinking alcohol 6 married people having an affair 7 gambling 8 sex between unmarried adults Table 5: Numerical indexing for topics in moral surveys.442ModelPositively evaluated topics for nonrich and nonwestern countriesNegatively evaluated topics for RichWest countries SBERTsex before marriage∗∗, homosexuality∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, claiming government beneﬁts to which you are not entitled∗∗∗, someone accepting a bribe in the course of their duties∗∗∗sex before marriage∗∗∗, euthanasia∗∗∗, divorce∗∗∗, death penalty∗∗∗, parents beating children∗∗∗ GPT2abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2MEDIUMeuthanasia∗∗∗, abortion∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗, violence against other people∗∗∗, stealing property∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2LARGEeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT3QAhaving casual sex∗∗, abortion∗∗, avoiding a fare on public transport∗∗∗, cheating on taxes∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗sex before marriage∗∗∗, divorce∗∗, death penalty∗∗, prostitution∗∗, parents beating children∗∗, suicide∗∗, for a man to beat his wife∗∗∗, stealing property∗∗ GPT3PROBSeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, death penalty∗∗∗, suicide∗∗∗, political violence∗∗∗, for a man to beat his wife∗∗∗sex before marriage∗∗∗, homosexuality∗∗∗, divorce∗∗ Table 6: Topics evaluated as morally positive for nonrich and nonwestern countries and morally negative for RichWest countries, in comparison to the ground truth in these countries. In each entry, the topics are sorted from the most controversial (i.e., having the highest degree of cultural diversity) to the least controversial. The asterisks indicate the signiﬁcance levels of MannWhitney U rank test after Bonferroni pvalue correction (“*”, “**”, “***” forp<0.05,0.01,0.001respectively).443Train data Data partition strategy Evaluation WVSRandom 0.893∗∗∗↑ (0.579∗∗∗) Countrybased 0.894∗∗∗↑ Topicbased 0.835∗∗∗↑ PEWRandom 0.944∗∗↑ (n.s.) Countrybased 0.839∗↑ Topicbased 0.953∗∗∗↑ Table 7: Summary of ﬁnetuned GPT2 language model performance in inferring the cultural diversities and shared tendencies over the morality of different topics. The arrows and colors show performance increase (blue, ↑) and decrease (red,↓) after ﬁnetuning. All values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). Nonsigniﬁcant results are shown by “n.s.”. Dataset Rating [Moral rating] in ﬁnetuning prompts WVS1 never justifiable [2, 3, 4] not justifiable [5, 6] somewhat justifiable [7, 8, 9] justifiable 10 always justifiable PEW1 morally unacceptable 2 not a moral issue 3 morally acceptable Table 8: Different prompting designs for ﬁnetuning language models on the global survey datasets. Data Data partition strategy Training samples Evaluation sample pairs WVSRandom 82200 206 Countrybased 82600 202 Topicbased 81200 216 PEWRandom 24900 63 Countrybased 24800 64 Topicbased 23400 78 Table 9: Number of samples in training and evaluation datasets for ﬁnetuning GPT2 on global surveys of morality.444ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? 8 /square\u0013A2. Did you discuss any potential risks of your work? 8 /square\u0013A3. Do the analysis: 1) whether language models capture ﬁnegrained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We ﬁnd that pretrained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, ﬁnetuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms. 1 Introduction Moral norms vary from culture to culture (Haidt et al., 1993; Bicchieri, 2005; Atari et al., 2022; Iurino and Saucier, 2020). Understanding the cultural variation in moral norms has become critically relevant to the development of machine intelligence. For instance, recent work has shown that cultures vary substantially in their judgment toward moral dilemmas regarding autonomous driving (Awad et al., 2018, 2020). Work in Natural Language Processing (NLP) also shows that language models capture some knowledge of socialor moral norms and values. For example, with no supervision, English pretrained language models (EPLMs) have been shown to capture people’s moral biases and distinguish between morally right and wrong actions (Schramowski et al., 2022). Here we investigate whether EPLMs encode knowledge about moral norms across cultures, an open issue that has not been examined comprehensively. Multilingual pretrained language models (mPLMs) have been probed for their ability to identify cultural norms and biases in a restricted setting (Yin et al., 2022; Arora et al., 2022; Hämmerl et al., 2022; Touileb et al., 2022). For instance, Hämmerl et al. (2022) show that mPLMs capture moral norms in a handful of cultures that speak different languages. However, it remains unclear whether monolingual EPLMs encode cultural knowledge about moral norms. Prior studies have only used EPLMs to assess how they encode undesirable biases toward different communities (Ousidhoum et al., 2021; Abid et al., 2021; Sap et al., 2020; Nozza et al., 2021, 2022). For instance, Abid et al. (2021) show that GPT3 can generate toxic comments against Muslims, and Nozza et al. (2022) explore harmful text generation toward LGBTQIA+ groups in BERT models (Devlin et al., 2018; Liu et al., 2019). Extending these lines of work, we assess whether monolingual EPLMs can accurately infer moral norms across many cultures. Our focus on EPLMs is due partly to the fact that English as a lingua franca has widespread uses for communication inperson and through online media. Given that EPLMs may be applied to multicultural settings, it is important to understand whether these models encode basic knowledge about cultural diversity. Such knowledge has both relevance and applications for NLP such as automated toxicity reduction and content moderation (Schramowski et al., 2022). Another motivation for our focus is that while it is expected that EPLMs should encode western and428Figure 1: Comparison of humanrated and machinescored moral norms across cultures. Left: Boxplots of human ratings of moral norms across countries in the World Values Survey (WVS) (Haerpfer et al., 2021). Each dot represents the empirical average of participants’ ratings for a morally relevant topic (e.g., “abortion”) within a country. Right: Corresponding moral scores estimated by a language model (SentenceBERT) (Reimers and Gurevych, 2019). Each dot represents the moral score obtained by probing the language model in a given country. Englishbased moral knowledge, such knowledge might entail potential (implicit) biases toward nonEnglish speaking cultures. For example, an EPLM might infer a situation to be morally justiﬁable (e.g., “political violence”) in a nonEnglish speaking culture (because these events tend to associate with nonEnglish speaking cultures in corpora) and thus generate misleading representations of that community. Here we probe stateof-theart EPLMs trained on large Englishbased datasets. Using EPLMs also supports a scalable analysis of 55 countries, which goes beyond existing work focusing on a small set of highresource languages from mPLMs and monolingual PLMs. We take the moral norms reported in different countries to be a proxy of cultural moral norms and consider two main levels of analysis to address the following questions: •Level 1: Do EPLMs encode moral knowledge that mirrors the moral norms in different countries? For example, “getting a divorce” can be a morally frownedupon topic in country i, but morally acceptable in country j. •Level 2: Can EPLMs infer the cultural diversity and shared tendencies in moral judgment of different topics? For example, people across nations might agree that doing X is morally wrong while disagreeing in theirmoral judgment toward Y. We probe EPLMs using two publicly available global surveys of morality, World Values Survey wave 7 (Haerpfer et al., 2021)1(WVS) and PEW Global Attitudes survey (PEW) (Research Center, 2014)2. For example, according to WVS survey (illustrated in Figure 1), people in different cultures hold disparate views on whether “having casual sex” is morally acceptable. In contrast, they tend to agree more about the immorality of “violence against other people”. Our level 1 analysis allows us to probe the ﬁnegrained cultural moral knowledge in EPLMs, and our level 2 analysis investigates the EPLMs’ knowledge about shared “universals” and variability across cultures in moral judgment. Following previous work (Arora et al., 2022) and considering the current scale of global moral surveys, we use country as a proxy to culture, although this approach is not fully representative of all the different cultures within a country. We also explore the utilitybias tradeoff in encoding the knowledge of cultural moral norms in EPLMs through a ﬁnetuning approach. With this approach it may be possible to enhance the moral knowledge of EPLMs in a multicultural setting. We 1https://www.worldvaluessurvey.org/ WVSContents.jsp 2https://www.pewresearch.org/global/ interactives/globalmorality/429examine how this approach might reduce the ability of EPLMs to infer Englishbased moral norms and discuss how it might induce cultural biases. 2 Related work 2.1 Automated moral inference in NLP Large language models have been utilized to make automated moral inference from text. Trager et al. (2022) used an annotated dataset to ﬁnetune language models to predict the moral foundations (Graham et al., 2013) expressed in Reddit comments. Many other textual datasets and methods have been proposed for ﬁnetuning LMs for moral norm generation, reasoning, and adaptation (Forbes et al., 2020; Emelin et al., 2021; Hendrycks et al., 2021; Ammanabrolu et al., 2022; Liu et al., 2022; Lourie et al., 2021; Jiang et al., 2021). Schramowski et al. (2022) proposed a method to estimate moral values and found EPLMs to capture humanlike moral judgment even without ﬁnetuning. They identiﬁed aMORAL DIRECTION using the semantic space of SentenceBERT (Reimers and Gurevych, 2019) (SBERT ) that corresponds to values of right and wrong. The semantic representations of different actions (e.g., killing people ) would then be projected in this direction for moral judgment estimation. However, this method assumed a homogeneous set of moral norms, so it did not examine cultural diversity in moral norms. 2.2 Language model probing Probing has been used to study knowledge captured in language models. Petroni et al. (2019) proposed a methodology to explore the factual information that language models store in their weights. Similar probing techniques have been proposed to identify harmful biases captured by PLMs. Ousidhoum et al. (2021) probed PLMs to identify toxic contents that they generate toward people of different communities. Nadeem et al. (2021) took a similar approach and introduced Context Association Tests to measure the stereotypical biases in PLMs, Yin et al. (2022) used probing to evaluate mPLMs on geodiverse commonsense knowledge, and Touileb et al. (2022) developed probing templates to investigate the occupational gender biases in multilingual and Norwegian language models. Related to our work, Arora et al. (2022) used crosscultural surveys to generate prompts for evaluating mPLMs in 13 languages. For each country and category (e.g.,Ethical Values) in the surveys, they take an average of participants’ responses to different questions in the category and show that mPLMs do not correlate with the cultural values of the countries speaking these languages. Differing from that study, we assess ﬁnergrained prediction of EPLMs on people’s responses to individual survey questions. More recently, Dillion et al. (2023) prompted GPT3.5 (Brown et al., 2020) with human judgments in different moral scenarios and found striking correlation between the model outputs and the human judgments. Similar to Schramowski et al. (2022), this work also used a homogeneous set of moral ratings which represented Englishbased and Western cultures. 3 Methodology for inferring cultural moral norms We develop a method for ﬁnegrained moral norm inference across cultures. This method allows us to probe EPLMs with topiccountry pairs, such as “getting a divorce in [Country]”.3 We build this method from the baseline method proposed by Schramowski et al. (2022) for homogeneous moral inference, where we probe EPLM’s moral knowledge about a topic without incorporating the cultural factor (i.e., the country names). Similar to that work, we use SBERT through bertlarge-nlimean-tokens sentence transformer model and use topic and topiccountry pairs as our prompts.4This model is built on top of the BERT model, which is pretrained on BOOKS CORPUS (Zhu et al., 2015) and Wikipedia. 3.1 Autoregressive EPLMs Since the MORAL DIRECTION is constructed from the semantic space of the BERT -based EPLMs (Schramowski et al., 2022), we develop a novel approach to probe autoregressive stateof- theart EPLMs, GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020). For each topic or topiccountry pair, we construct the input sas “In [Country] [Topic]”. We then append a pair of opposing moral judgments to sand represent them formally as(s+,s−). For example, for s=“In [Country] getting a divorce”, and ( always justiﬁable, never justiﬁable ) as the moral judgment pair, s+ands− would be “In [Country] getting a divorce is always 3We replace [Country] with a country’s name. 4We make our code and data available on https:// github.com/AidaRamezani/cultural_inference .430justiﬁable” and “In [Country] getting a divorce is never justiﬁable” respectively.5To make our probing robust to the choice of moral judgments, we use a set ofK= 5prompt pairs (i.e.,{ (always justiﬁable, never justiﬁable), (morally good, morally bad), (right, wrong), (ethically right, ethically wrong), (ethical, unethical) }), and refer to appended input pairs as (s+ i,s− i)wherei∈[K]. Since GPT2 andGPT3 are composed of decoder blocks in the transformer architecture (Vaswani et al., 2017), we use the probabilities of the last token ins+ i, ands− ias a moral score for each. The moral score of the pair (s+ i,s− i)is the difference between the log probabilities of its positive and negative statements. MS(s+ i,s− i) = logP(s+ iT|s+ i<T) P(s− iT|s− i<T)(1) Heres+ iTands− iTare the last tokens in s+ iands− irespectively, and their probabilities can be estimated by the softmax layer in autoregressive EPLMs. We take an average of the estimated moral scores for allKpair statements to compute the moral score of the input. MS(s) =1 KK/summationdisplay i=1MS(s+ i,s− i) (2) To construct the baseline, we compute the homogeneous moral score of a topic without specifying the country in the prompts. Using prompt pairs allows us to operationalize moral polarity: a positive moral score indicates that on average the EPLM is more likely to generate positive moral judgment for inputs, compared to negative moral judgment. We use GPT2 (117M parameters), GPT2MEDIUM (345M parameters), GPT2LARGE (774M parameters), and GPT3 (denoted as GPT3PROBS , 175B parameters)6.GPT2 is trained on WEBTEXT, which is a dataset of webpages and contains very few nonEnglish samples. Around 82% of the pretraining data for GPT3 comes from Common Crawl data and WEBTEXT2(Kaplan et al., 2020), an extended version of WEBTEXT (Radford et al., 2019). Around 7%of the training corpus 5We also try probing with the template s=“People in [Country] believe [Topic]”, but the results do not improve, so we report the most optimal prompts in the main text, and the rest are shown in Appendix C. 6We access GPT2 through transformer package provided by Huggingface . We access GPT3 through OpenAI API of textdavinci-002 engine with a temperature of 0.6 for text generation.ofGPT3 is nonEnglish text. Considering such data shift from books and articles in BERT to webpages in GPT2 andGPT3 in astronomical sizes, it is interesting to observe how cultural moral norms would be captured by EPLMs trained on webpages, which cover a more heterogeneous set of contents and authors. We also design multiplechoice question prompts to leverage the questionanswering capabilities of GPT3 (denoted as GPT3QA). Similar to the wording used in our groundtruth survey datasets, questions are followed by three options each describing a degree of moral acceptability. We repeat this questionanswering process 5times for each topiccountry pair and take the average of the model responses. Table 2 in the Appendix shows our prompts for all models. 4 Datasets We describe two open survey data that record moral norms across cultures over a variety of topics. 4.1 World Values Survey The Ethical Values section in World Values Survey Wave 7 (WVS for short) is our primary dataset. This wave covers the span of 20172021 and is publicly available (Haerpfer et al., 2021). In the Ethical Values section, participants from 55 countries were surveyed regarding their opinions on 19 morallyrelated topics. The questionnaire was translated into the ﬁrst languages spoken in each country and had multiple options. We normalized the options to range from −1 to 1, with−1 representing “never justiﬁable” and 1 “always justiﬁable”. The moral rating of each country on each topic (i.e., topiccountry pair) would then be the average of the participant’s responses. 4.2 PEW 2013 global attitude survey We use a secondary dataset from PEW Research Center (Research Center, 2014) based on a public survey in 2013 that studied global moral attitudes in 40 countries toward eight morallyrelated topics (PEW for short). 100 people from each country participated in the survey. The questions were asked in English and had three options representing “morally acceptable”, “not a moral issue”, and “morally unacceptable”. We normalized these ratings to be in the range of −1 to 1 and represented each topiccountry pair by taking an expected value of all the responses.4314.3 Homogeneous moral norms We also use the data from the global user study in Schramowski et al. (2022) which were collected via Amazon MTurk from English speakers. This dataset contains 234participants’ aggregated ratings of moral norms used for identifying the MORAL DIRECTION . Around half of the participants are from North America and Europe. We refer to this dataset as “Homogeneous norms” since it does not contain information about moral norms across cultures. 5 Evaluation and results We evaluate EPLMs’ moral knowledge with respect to 1) homogeneous moral norms, 2) ﬁnegrained moral norms across cultures, and 3) cultural diversities and shared tendencies on moral judgment of different topics. 5.1 Homogeneous moral norm inference For homogeneous moral norm inference, we compute Pearson correlation between 1) the empirical homogeneous moral ratings, obtained by aggregating the human moral ratings toward a topic from all countries, and 2) language model inferred moral scores, estimated from our homogeneous probing method (i.e., without specifying country in prompts). Figure 2 shows the results on World Values Survey (n= 1,028), PEW survey ( n= 312 ), and the Homogeneous norms datasets ( n= 100 ). The high correlation of GPT2 andGPT3 moral scores with the Homogeneous norms dataset indicate that our methodology does indeed capture the embedded moral biases in these models, with similar performance to the method proposed by Schramowski et al. (2022) for SBERT (r= 0.79), and higher forGPT3PROBS (r= 0.85). The moral norms in this dataset are typically more globally agreeable (e.g., You should not kill people ) than topics in WVS and PEW. As expected, EPLMs are less correlated with WVS and PEW, since their moral biases are derived from pretraining on English and westernized data. Aggregated ratings in WVS and PEW, however, capture a more global view toward moral issues, which are also morally contentious (e.g., “getting a divorce”). Table 3 in Appendix includes the values for this experiment. Figure 2: Performance of EPLMs (without cultural prompts) on inferring 1) English moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey data. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). 5.2 Finegrained cultural variation of moral norms toward different topics Going beyond probing EPLMs for their general knowledge of moral norms, we assess whether they can accurately identify the moral norms of different cultures (level 1 analysis). Using our ﬁnegrained probing approach described in Section 3, we compute Pearson correlation between EPLMs’ moral scores and the ﬁnegrained moral ratings from the ground truth. Each sample pair in the correlation test corresponds to 1) the moral norms estimated by EPLMs for a country cand a topict, and 2) the empirical average of moral ratings toward topic t from all the participants in the country c. Figure 3 summarizes the results for SBERT, GPT2LARGE , and GPT3PROBS models, and the rest of the models are shown in Figure 7 in the Appendix. To facilitate direct comparison, the estimated moral scores are normalized to a range of −1to1, where−1, 0, and 1 indicate morally negative, morally neutral, and morally positive norms, respectively. GPT3QA andGPT3PROBS both show a relatively high correlation with the cultural variations of moral norms ( r= 0.352,r= 0.411, p<0.001, for both), and GPT2LARGE achieves a correlation of r= 0.207(p <0.001) in WVS wheren= 1,028. The correlations are relatively better for PEW ( n= 312 ) withr= 0.657, r= 0.503, andr= 0.468forGPT3QA, GPT3PROBS andGPT2LARGE respectively. These results show that EPLMs have captured some432knowledge about the moral norms of different cultures, but with much less accuracy (especially for GPT2 andSBERT ) compared to their inference of English moral norms shown in the previous analysis. In addition, we check whether GPT3 ’s high correlation with PEW is because it has seen and memorized the empirical data. Our investigation shows thatGPT3 has seen the data during pretraining, as it can generate the sentences used on the survey website. However, the scores suggested by GPT3 text generation and the countries’ rankings based on their ratings are different from the ground truth data. 5.3 Culture clustering through ﬁnegrained moral inference EPLMs’ ﬁnegrained knowledge of moral norms, inspected in the previous experiment, might be more accuracte for western cultures than other cultures. We investigate this claim by clustering countries based on 1) their WesternEastern economic status (i.e., Rich West grouping)7, and 2) their continent (i.e., geographical grouping). We repeat the experiments in the previous section for different country groups. The results are shown in Figure 4. We also try sampling the same number of countries in each group. The results remain robust and are illustrated in AppendixF. Our ﬁndings indicate that EPLMs contain more knowledge about moral norms of the Rich West countries as opposed to nonwestern and nonrich countries. Similarly, EPLMs have captured a more accurate estimation of the moral norms in countries located in Oceania, North America, and Europe, as opposed to African, Asian, and South American countries. The empirical moral norm ratings from European countries in WVS are highly aligned with North American countries ( r= 0.938), which explains why their moral norms are inferred more accurately than nonEnglish speaking countries. Next, for each topic, we compare the zscores of the empirical moral ratings with the zscores of the GPT3PROBS inferred moral scores, using MannWhitney U rank test. The results reveal that “abortion”, “suicide”, “euthanasia”, “for a man to beat his wife”, “parents beating children”, “having casual sex”, “political violence”, and “death penalty” in nonwestern and nonrich countries are all en7https://worldpopulationreview.com/ countryrankings/westerncountriescoded as more morally appropriate than the actual data. Such misrepresentations of moral norms in these countries could lead to stereotypical content generation. We also ﬁnd that For Rich West countries, “homosexuality”, “divorce”, and “sex before marriage” are encoded as more morally inappropriate than the ground truth, ( p < 0.001for all, Bonferroni corrected). Such underlying moral biases, speciﬁcally toward “homosexuality” might stimulate the generation of harmful content and stigmatization of members of LGBTQ+, which has been reported in BERTbased EPLMs (Nozza et al., 2022). The results for the rest of the models are similar and are shown in Table 6 in the Appendix. Our method of clustering countries is simplistic and may overlook things such as the signiﬁcant diversity in religious beliefs within the NonRich-West category, and thus it does not reﬂect the nuanced biases that models may possess when it comes to moral norms inﬂuenced by different religious traditions. Nonetheless, our approach still serves as a valuable starting point for studying EPLM’s moral biases towards more ﬁnegrained religious and ethnic communities. 5.4 Cultural diversities and shared tendencies over the morality of different topics We next investigate whether EPLMs have captured the cultural diversities and shared tendencies over the morality of different topics (level 2 analysis). For example, people across cultures tend to disagree more about “divorce” than about “violence against other people” as depicted in Figure 1. Such cultural diversities for each topic can be measured by taking the standard deviation of the empirical moral ratings across different countries. The EPLMs’ inferred cultural diversities can similarly be measured by taking the standard deviation of the estimated ﬁnegrained moral scores for different countries. We then quantify the alignment between the two using Pearson correlation. Figure 5 shows the results for SBERT, GPT2LARGE, GPT3PROBS , and the rest are shown in Figure 8 in the Appendix. None of the correlations with the PEW survey were signiﬁcant. For WVS, SBERT, GPT2 andGPT2MEDIUM exhibited a signiﬁcant correlation ( p<0.001) with r= 0.618,r= 0.579, andr= 0.734respectively. The results for GPT3 are insigniﬁcant, suggesting that it is more challenging to correctly estimate433PEW surveyWorld V alue Survey Figure 3: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. The legends display the moral topics in the surveys. Similar topics in the World Value Surveys are shown with the same color. Figure 4: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey, analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). cultural controversies of topics for GPT3 . For example, stealing property is incorrectly estimated to be more controversial than abortion . 6 Finetuning language models on global surveys Finally, we explore the utilitybias tradeoff in encoding cultural moral knowledge into EPLMs by ﬁnetuning them on crosscultural surveys. The utility comes from increasing the cultural moral knowledge in these models, and the bias denotes their decreased ability to infer English moral norms, in addition to the cultural moral biases introduced to the model. We run our experiments on GPT2 , which our results suggest having captured minimum information about cultural moral norms compared to other autoregressive models. To ﬁnetune the model, for each participant from [Country] with [Moral rating] toward [Topic] , we designed a prompt with the structure“A person in [Country] believes [Topic] is [Moral rating] .”. We used the surveys’ wordings for [Moral rating] . Table 8 in the Appendix shows our prompts for WVS and PEW. These prompts constructed our data for ﬁnetuning, during which we maximize the probability of the next token. The ﬁnetuned models were evaluated on the same correlation tests introduced in the previous Sections 5.2, 5.3, and 5.4. The ﬁnetuning data was partitioned into training and evaluation sets using different strategies (i.e., Random, Countrybased, and Topicbased). For the Random strategy, we randomly selected 80% of the ﬁnetuning data for training the model. The topiccountry pairs not seen in the training data composed the evaluation set. For our Countrybased and Topicbased strategies, we randomly removed 20% of the countries ( n= 11 for WVS,n= 8 for PEW) and topics ( n= 4for WVS,n= 2for PEW) from the training data to compose the evalu434PEW surveyWorld V alues SurveyEstimated degree of cultural diversity Empirical degree of cultural diversity Figure 5: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The numerical indices are consistent with the legend indices in Table 5. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Train data Data partition strategy EvaluationPerformance on the Homogeneous norms WVSRandom 0.832∗∗∗↑ (0.271∗∗∗) 0.71∗∗∗↓ (0.80∗∗∗)Countrybased 0.759∗∗∗↑ (0.225∗∗) 0.72∗∗∗↓ Topicbased 0.508∗∗∗↑ (0.286∗∗∗) 0.70∗∗∗↓ PEWRandom 0.818∗∗∗↑ (0.204, n.s.) 0.64∗∗∗↓ Countrybased 0.764∗∗∗↑ (0.055, n.s.) 0.67∗∗∗↓ Topicbased 0.733∗∗∗↑ (−0.146, n.s.) 0.61∗∗∗↓ Table 1: Summary of ﬁnetuned GPT2 language model performance on inferring moral norms across cultures and the degradation of its performance on inferring Homogeneous moral norms. Values in parentheses show the performance before ﬁnetuning. The arrows and colors show performance increase (blue, ↑) and decrease (red, ↓) after ﬁnetuning. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001). ation set. See Appendix G for the total number of samples. Table 1 shows the gained utilities, that is the correlation test results between the ﬁnegrained moral scores inferred by the ﬁnetuned models and the empirical ﬁnegrained moral ratings. All ﬁnetuned models align better with the ground truth than the pretrained-only models (i.e., the values in parentheses). For both WVS and PEW, the Random strategy is indeed the best as each country and topic are seen in the training data at least once (but may not appear together as a pair). The ﬁnetuned models can also generalize their moral scores to unseen countries and topics. Repeating the experiment in Section 5.4 also shows substantial improvement in identifying cultural diversities of different topics by all ﬁnetuned models. For example, the WVS and PEWtrained models with Random strategy gain Pearson’s r values of 0.893, and 0.944respectively. The results for the rest of the models are shown in Table 7 in the Appendix.Nevertheless, the bias introduced during the ﬁnetuning decreases the performance on the Homogeneous norms dataset. This observation displays a tradeoff between cultural and homogeneous moral representations in language models. Moreover, injecting the crosscultural surveys into EPLMs might introduce additional social biases to the model that are captured through these surveys (Joseph and Morgan, 2020). In addition, we probe the best ﬁnetuned model (i.e., WVS with Random strategy) on its ability to capture the moral norms of nonwestern cultures by repeating the experiment in Section 5.3. The results in Figure 4 show that the ﬁnetuned GPT2 performs the best for all country groups. There is still a gap between western and nonwestern countries. However, basic ﬁnetuning proves to be effective in adapting EPLMs to the ground truth.4357 Discussion and conclusion We investigated whether English pretrained language models contain knowledge about moral norms across many different cultures. Our analyses show that large EPLMs capture moral norm variation to a certain degree, with the inferred norms being predominantly more accurate in western cultures than nonwestern cultures. Our ﬁnetuning analysis further suggests that EPLMs’ cultural moral knowledge can be improved using global surveys of moral norms, although this strategy reduces the capacity to estimate the English moral norms and potentially introduces new biases into the model. Given the increasing use of EPLMs in multicultural environments, our work highlights the importance of cultural diversity in automated inference of moral norms. Even when an action such as “political violence” is assessed by an EPLM as morally inappropriate in a homogeneous setting, the same issue may be inferred as morally appropriate for underrepresented cultures in these large language models. Future work can explore alternative and richer representations of cultural moral norms that go beyond the point estimation we presented here and investigate how those representations might better capture culturally diverse moral views. Limitations Although our datasets are publicly available and gathered from participants in different countries, they cannot entirely represent the moral norms from all the individuals in different cultures over the world or predict how moral norms might change into the future (Bloom, 2010; Bicchieri, 2005). Additionally, we examine a limited set of moral issues for each country, therefore the current experiments should not be regarded as comprehensive of the space of moral issues that people might encounter in different countries. Moreover, taking the average of moral ratings for each culture is a limitation of our work and reduces the natural distribution of moral values in a culture to a single point (Talat et al., 2021). Implementing a framework that incorporates both withincountry variation and temporal moral variation (Xie et al., 2019) is a potential future research direction. Currently, it is not clear whether the difference between EPLMs’ estimated moral norms and the empirical moral ratings is due to the lack of cultural moral norms in the pretraining data, orthat the cultural moral norms mentioned in the pretraining data represent the perspective of an Englishspeaking person of another country. For example, a person from the United States could write about the moral norms in another country from a western perspective. A person from a nonwestern country could also write about their own moral views using English. These two cases have different implications and introduce different moral biases into the system. Potential risks We believe that the language models should not be used to prescribe ethics, and here we approach the moral norm inference problem from a descriptive perspective. However, we acknowledge modifying prompts could lead language models to generate ethical prescriptions for different cultures. Additionally, our ﬁnetuning approach could be exploited to implant cultural stereotypical biases into these models. Many topics shown in this work might be sensitive to some people yet more tolerable to some other people. Throughout the paper, we tried to emphasize that none of the moral norms, coming from either the models’ estimation or the empirical data, should be regarded as deﬁnitive values of right and wrong, and the moral judgments analyzed in this work do not reﬂect the opinions of the authors. Acknowledgements This work was supported by a SSHRC Insight Grant 435190272.436References Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent antimuslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’21, page 298–306, New York, NY , USA. Association for Computing Machinery. Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. 2022. Aligning to social norms and values in interactive narratives. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5994–6017, Seattle, United States. Association for Computational Linguistics. Arnav Arora, LucieAimée Kaffee, and Isabelle Augenstein. 2022. Probing PreTrained Language Models for CrossCultural Differences in Values. arXiv preprint arXiv:2203.13722 . Mohammad Atari, Jonathan Haidt, Jesse Graham, Sena Koleva, Sean Stevens, and Morteza Dehghani. 2022. Morality Beyond the WEIRD: How the Nomological Network of Morality Varies Across Cultures. Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, JeanFrançois Bonnefon, and Iyad Rahwan. 2018. The Moral Machine experiment. Nature , 563(7729):59– 64. Edmond Awad, Sohan Dsouza, Azim Shariff, Iyad Rahwan, and JeanFrançois Bonnefon. 2020. Universals and variations in moral decisions made in 42 countries by 70,000 participants. Proceedings of the National Academy of Sciences , 117(5):2332–2337. Cristina Bicchieri. 2005. The grammar of society: The nature and dynamics of social norms . Cambridge University Press. Paul Bloom. 2010. How do morals change? Nature , 464(7288):490–490. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are fewshot learners. Advances in neural information processing systems , 33:1877–1901. Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. Can ai language models replace human participants? Trends in Cognitive Sciences . Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings ofthe 2021 Conference on Empirical Methods in Natural Language Processing , pages 698–718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social Chemistry 101: Learning to Reason about Social and Moral Norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 653–670, Online. Association for Computational Linguistics. Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P Wojcik, and Peter H Ditto. 2013. Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism. In Advances in Experimental Social Psychology , volume 47, pages 55–130. Elsevier. Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime DiezMedrano, Marta Lagos, Pippa Norris, E Ponarin, and B Puranen. 2021. World Values Survey: Round Seven – CountryPooled Dataﬁle. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. Data File Version , 2(0). Jonathan Haidt, Silvia Helena Koller, and Maria G Dias. 1993. Affect, culture, and morality, or is it wrong to eat your dog? Journal of personality and social psychology , 65(4):613. Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jind ˇrich Libovick `y, Alexander Fraser, and Kristian Kersting. 2022. Do Multilingual Language Models Capture Differing Moral Norms? arXiv preprint arXiv:2203.09904 . Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2021. Aligning AI With Shared Human Values. In International Conference on Learning Representations . Kathryn Iurino and Gerard Saucier. 2020. Testing measurement invariance of the Moral Foundations Questionnaire across 27 countries. Assessment , 27(2):365–372. Liwei Jiang, Jena D. Hwang, Chandrasekhar Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards Machine Ethics and Norms. ArXiv , abs/2110.07574. Kenneth Joseph and Jonathan Morgan. 2020. When do word embeddings accurately reﬂect surveys on our beliefs about people? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 4392–4415, Online. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.4372020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 . Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush V osoughi. 2022. Aligning Generative Language Models with Human Values. In Findings of the Association for Computational Linguistics: NAACL 2022 , pages 241–252. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 . Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 reallife anecdotes. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence , volume 35, pages 13470–13479. Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 5356–5371, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2398–2406, Online. Association for Computational Linguistics. Debora Nozza, Federico Bianchi, Anne Lauscher, and Dirk Hovy. 2022. Measuring harmful sentence completion in language models for LGBTQIA+ individuals. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 26–34, Dublin, Ireland. Association for Computational Linguistics. Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and DitYan Yeung. 2021. Probing toxic content in large pretrained language models. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 4262–4274, Online. Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 2463–2473, Hong Kong, China. Association for Computational Linguistics.Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. PEW Research Center. 2014. Global Attitudes survey . Washington, D.C. Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5477–5490, Online. Association for Computational Linguistics. Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin A Rothkopf, and Kristian Kersting. 2022. Large pretrained language models contain humanlike biases of what is right and wrong to do. Nature Machine Intelligence , 4(3):258–268. Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, and Adina Williams. 2021. A Word on Machine Ethics: A Response to Jiang et al.(2021). arXiv preprint arXiv:2111.04158 . Samia Touileb, Lilja Øvrelid, and Erik Velldal. 2022. Occupational biases in Norwegian and multilingual language models. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP) , pages 200–211, Seattle, Washington. Association for Computational Linguistics. Jackson Trager, Alireza S Ziabari, Aida Mostafazadeh Davani, Preni Golazazian, Farzan KarimiMalekabadi, Ali Omrani, Zhihe Li, Brendan Kennedy, Nils Karl Reimer, Melissa Reyes, et al. 2022. The Moral Foundations Reddit Corpus. arXiv preprint arXiv:2208.05545 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NIPS’17, page 6000–6010, Red Hook, NY , USA. Curran Associates Inc. Jing Yi Xie, Renato Ferreira Pinto Junior, Graeme Hirst, and Yang Xu. 2019. Textbased inference of moral sentiment change. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP) , pages 4654–4663, Hong Kong, China. Association for Computational Linguistics. Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and KaiWei Chang. 2022. Geomlama: Geodiverse commonsense probing on multilingual pretrained language models. arXiv preprint arXiv:2205.12247 .438Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Storylike Visual Explanations by Watching Movies and Reading Books. CoRR , abs/1506.06724. A Data license Both World Values Survey and PEW survey are publicly available to use for research purposes. We accept and follow the terms and conditions for using these datasets, which can be found in https://www.worldvaluessurvey. org/WVSContents.jsp?CMSID=Documentation , and https://www.pewresearch.org/about/ termsand-conditions/ . B Comparison of humanrated and machinescored moral norms Figure 6 shows the comparison between humanrated moral norms in PEW, and the moral scores inferred by SBERT (Reimers and Gurevych, 2019). C Probing experiments Table 2 shows our prompt design for probing ﬁnegrained moral norms in EPLMs. As mentioned in the main text, we repeat our probing experiment for GPT2 models and GPT3PROBS with another template “People in [Country] believe [Topic] is [Moral Judgment]”. The results are substantially worse than our initial template, suggesting that extracting the moral knowledge in language models is sensitive to the wording used in the input. The results for the ﬁnegrained analysis (level 1 analysis) and the cultural diversities and shared tendencies (level 2 analysis) with this template are shown in Table 4. In all experiments, we used a single NVIDIA TITAN V GPU. Each probing experiment took approximately 1 hour to complete. D Homogeneous moral norm inference Table 3 shows the detailed values of the correlation tests in our homogeneous moral norm inference experiment. E Finegrained cultural variation of moral norm Figure 7 and Figure 8 show the result of our ﬁnegrained cultural moral inference, and inference ofcultural diversities and shared tendencies respectively for GPT2, GPT2MEDIUM , and GPT3QA. The numerical indices in Figure 8 are consistent with the indices in Table 5. F Sampling for cultural clusters Since in section 5.3 there are a different number of countries in each group, we redo the experiment by randomly sampling the same number of countries (n= 11 for Rich West grouping, n= 5for continent grouping) and repeating the sampling process for50times. The results and the general pattern remain the same and are depicted in Figure 9. G Details of ﬁnetuning on global surveys Table 8 shows the Moral rating in our prompt design for constructing our ﬁnetuning dataset. For example, The World Value Survey represents the two ends of the ratings scale where 1 is “Never justiﬁable” and 10 is “Always justiﬁable”. The options in between are presented to the participants in a 10point scale. Therefore, we mapped these options to different prompts that are semantically similar and in between the two ends. For example, if a participant from the United States rated stealing property as 2, which is slightly more positive than the ﬁrst option (“Never justiﬁable”), we mapped this rating to “not justiﬁable”, creating the prompt “A person in the United States believes stealing property is not justifiable. ” for our ﬁnetuning data. Since there are a different number of participants from each country, in order to balance this dataset, we randomly select 100samples for each topiccountry pair and removed the rest of the utterances from the training data. We ﬁnetuned GPT2 on one epoch, with a batch size of 8, learning rate of 5e−5, and weight decay of 0.01. The number of training and evaluation samples for all data partition strategies are shown in Table 9. In all experiments, we used a single NVIDIA TITAN V GPU. Finetuning and evaluation took approximately 2 hours to complete for each model.439Figure 6: Comparison of humanrated and machinescored moral norms across cultures. Left: Boxplots of human ratings of moral norms across cultures in the PEW survey. Each dot represents the empirical average of participants’ ratings for a morally relevant topic (e.g., “having an abortion”) within a country. Right: Corresponding moral scores estimated by a language model (SentenceBERT) (Reimers and Gurevych, 2019). Each dot represents the moral score obtained by probing the language model in a given country. Model Prompt SentenceBERT [Topic] in [Country]. GPT2 models and GPT3PROBSIn [Country] [Topic] is [Moral judgement]. People in [Country] believe [Topic] is [Moral judgement]. GPT3QA (for PEW)Do people in [Country] believe that [Topic] is: 1) Morally acceptable 2) Not a moral issue 3) Morally unacceptable. GPT3QA (for WVS)Do people in [Country] believe that [Topic] is: 1) Always Justifiable 2) Something in between 3) Never justifiable. Table 2: Prompting design used for estimating the ﬁnegrained moral norms in different language models. In our homogeneous norm inference, we remove “ In [country] ” from the prompts. Model World Values Survey (n= 1,028)PEW survey ( n= 312 )Homogeneous norms ( n= 100 ) SBERT 0.210∗∗∗−0.038(n.s.) 0.79∗∗∗ GPT2 0.176∗∗∗−0.069(n.s.) 0.80∗∗∗ GPT2MEDIUM 0.181∗∗∗0.033(n.s.) 0.79∗∗∗ GPT2LARGE 0.226∗∗∗0.157(n.s.) 0.76∗∗∗ GPT3QA 0.330∗∗∗0.391∗∗∗0.79∗∗∗ GPT3PROBS 0.346∗∗∗0.340∗∗∗0.85∗∗∗ Table 3: Performance of pretrained language models (without cultural prompts) on inferring 1) homogeneous westernized moral norms, and 2) culturally diverse moral norms recorded in World Values Survey and PEW survey data.440PEW surveyWorld V alue SurveyFigure 7: Degree of alignment between the moral scores from EPLMs and ﬁnegrained empirical moral ratings for different topics across countries taken from the World Values Survey (top) and PEW survey (bottom). Each dot represents a topiccountry pair. The xaxis shows the ﬁnegrained moral ratings from the ground truth and the yaxis shows the corresponding inferred moral scores. Similar topics in the World Value Surveys are shown with the same color. PEW surveyWorld V alues Survey Estimated degree of cultural diversity Empirical degree of cultural diversity Figure 8: Comparison between the degrees of cultural diversities and shared tendencies in the empirical moral ratings and languagemodel inferred moral scores. Each dot corresponds to a moral topic. The xaxis shows the empirical standard deviations in moral ratings across countries and the yaxis shows the standard deviations from the modelinferred moral scores. Figure 9: Correlation between languagemodel inferred moral scores and empirical moral ratings from World Values Survey analyzed in different clusters of countries in Rich West grouping (left) and continent grouping (right). The results are generated by sampling and the error bars show the conﬁdence intervals with α= 0.05.441Data modelFinegrained evaluation of moral normsEvaluation on cultural diversity and shared tendencies WVSGPT3PROBS 0.078∗−0.176 GPT2 −0.114∗∗∗0.231 GPT2MEDIUM −0.261∗∗∗−0.357 GPT2LARGE −0.07∗−0.356 PEWGPT3PROBS 0.539∗∗∗0.041 GPT2 0.168∗∗0.566 GPT2MEDIUM 0.165∗∗0.184 GPT2LARGE 0.19∗∗∗0.542 Table 4: Performance of pretrained autoregressive language models on identifying ﬁnegrained moral norms and cultural diversities and shared tendencies, using the prompt template “People in [Country] believe [Topic] is [Moral judgment]\". The values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” forp<0.05,0.01,0.001respectively). World Values Survey Index Topic 1 stealing property 2 euthanasia 3 sex before marriage 4 violence against other people 5 cheating on taxes 6 avoiding a fare on public transport 7 abortion 8 suicide 9 someone accepting a bribe on a course of their duties 10 terrorism as a political, ideological, or religious mean 11 homosexuality 12 parents beating children 13 prostitution 14 divorce 15 political violence 16 death penalty 17 claiming governments beneﬁts to which you are not entitled 18 for a man to beat his wife 19 having casual sex PEW survey 1 using contraceptives 2 getting a divorce 3 having an abortion 4 homosexuality 5 drinking alcohol 6 married people having an affair 7 gambling 8 sex between unmarried adults Table 5: Numerical indexing for topics in moral surveys.442ModelPositively evaluated topics for nonrich and nonwestern countriesNegatively evaluated topics for RichWest countries SBERTsex before marriage∗∗, homosexuality∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, claiming government beneﬁts to which you are not entitled∗∗∗, someone accepting a bribe in the course of their duties∗∗∗sex before marriage∗∗∗, euthanasia∗∗∗, divorce∗∗∗, death penalty∗∗∗, parents beating children∗∗∗ GPT2abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2MEDIUMeuthanasia∗∗∗, abortion∗∗∗, suicide∗∗∗, avoiding a fare on public transport∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗, violence against other people∗∗∗, stealing property∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, having casual sex∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT2LARGEeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, prostitution∗∗∗, suicide∗∗∗, terrorism as a political, ideological or religious mean∗∗∗, political violence∗∗∗, violence against other people∗∗∗sex before marriage∗∗∗, homosexuality∗∗, divorce∗∗, claiming government beneﬁts to which you are not entitled∗∗∗ GPT3QAhaving casual sex∗∗, abortion∗∗, avoiding a fare on public transport∗∗∗, cheating on taxes∗∗∗, someone accepting a bribe in the course of their duties∗∗∗, political violence∗∗∗sex before marriage∗∗∗, divorce∗∗, death penalty∗∗, prostitution∗∗, parents beating children∗∗, suicide∗∗, for a man to beat his wife∗∗∗, stealing property∗∗ GPT3PROBSeuthanasia∗∗∗, having casual sex∗∗∗, abortion∗∗∗, death penalty∗∗∗, suicide∗∗∗, political violence∗∗∗, for a man to beat his wife∗∗∗sex before marriage∗∗∗, homosexuality∗∗∗, divorce∗∗ Table 6: Topics evaluated as morally positive for nonrich and nonwestern countries and morally negative for RichWest countries, in comparison to the ground truth in these countries. In each entry, the topics are sorted from the most controversial (i.e., having the highest degree of cultural diversity) to the least controversial. The asterisks indicate the signiﬁcance levels of MannWhitney U rank test after Bonferroni pvalue correction (“*”, “**”, “***” forp<0.05,0.01,0.001respectively).443Train data Data partition strategy Evaluation WVSRandom 0.893∗∗∗↑ (0.579∗∗∗) Countrybased 0.894∗∗∗↑ Topicbased 0.835∗∗∗↑ PEWRandom 0.944∗∗↑ (n.s.) Countrybased 0.839∗↑ Topicbased 0.953∗∗∗↑ Table 7: Summary of ﬁnetuned GPT2 language model performance in inferring the cultural diversities and shared tendencies over the morality of different topics. The arrows and colors show performance increase (blue, ↑) and decrease (red,↓) after ﬁnetuning. All values are Pearson’s correlations. The asterisks indicate the signiﬁcance levels (“*”, “**”, “***” for p<0.05,0.01,0.001respectively). Nonsigniﬁcant results are shown by “n.s.”. Dataset Rating [Moral rating] in ﬁnetuning prompts WVS1 never justifiable [2, 3, 4] not justifiable [5, 6] somewhat justifiable [7, 8, 9] justifiable 10 always justifiable PEW1 morally unacceptable 2 not a moral issue 3 morally acceptable Table 8: Different prompting designs for ﬁnetuning language models on the global survey datasets. Data Data partition strategy Training samples Evaluation sample pairs WVSRandom 82200 206 Countrybased 82600 202 Topicbased 81200 216 PEWRandom 24900 63 Countrybased 24800 64 Topicbased 23400 78 Table 9: Number of samples in training and evaluation datasets for ﬁnetuning GPT2 on global surveys of morality.444ACL 2023 Responsible NLP Checklist A For every submission: /square\u0013A1. Did you describe the limitations of your work? 8 /square\u0013A2. Did you discuss any potential risks of your work? 8 /square\u0013A3. Do the experiments to test three different prompt methods to determine the best one for each control aspect. They are (1) Encpref: prompts are injected into the encoder’s input as a preﬁx. (2) Decpref: prompts are injected into the decoder’s input as a preﬁx. (3) Decemb: prompts are embedded into a vector and added toward the decoder’s input. 3.4 Word Boundary Control Intraword pause is a typical disﬂuency pattern of beginning language learners ( Franco et al. ,1998 ). However, improperly translated lyrics usually con- (a) (b) Figure 2: Demonstration of the necessity of word boundary control. Blue box: musical pauses; orange box: notes highlighted by downbeats; red box: words interrupted by musical pauses or highlighted notes; green box: words without interruption. tain multisyllable words that lies across musical pauses, as the blue box in Figure 2, so that the performer has to make awkward intraword pauses while singing ( Guo et al. ,2022 ), causing a drop in pronunciation acceptability. Besides, we observe that positioning highlighted music notes, such as high notes or downbeats, as the orange box in Figure2, onto a multisyllable word’s second or later syllables can bring similar adverse effects due to abrupt changes of pitch and tension3. We address these issues by carefully designing the placement of word boundaries in outputs, i.e., the points between two consecutive syllables that are from different words. Our aim is to ensure that word boundaries align precisely with the boundaries in music, i.e., the melody boundaries , which occur atmusical pauses and before highlighted notes (the blue and orange boxes in Figure 2). In this way, we achieve higher compatibility between the output sentences and the accompanying music, enhance the ﬂuency and consistency of pronunciation during singing, and hence lead to the gain of singability. This solution is achieved by promptbased word boundary control. We use the prompt bdrto represent melody boundary positions, indicating necessary word boundary positions. bdris a sequence of special tokens, and each token corresponds to one syllable in the output. There are two types of special interior tokens: bdr_1 and bdr_0, representing after the corresponding syllable “there should be a word boundary” and “we do not care if there 3Stresstimed languages have another solution to this second problem, i.e., put a stressed syllable at the highlighted note. Here we discuss another generic solution.450is a boundary”, respectively. At test time, bdris obtained from accompanying music and serves as additional inputs. A welltrained wordboundary- aware model can hence place word boundaries at the desired positions to achieve better music–lyric compatibility. For locations where bdr_ 0is present (“don’t care”), the translation model operates unconstrained, maximizing translation naturalness. During training, length and rhyme prompts can be obtained directly from the target sentences in the training samples, but not again for necessary word boundary prompts because they have to be obtained from accompanying music which is absent in training. Nevertheless, we offer a solution: we randomly sample from all actual word boundary positions from the targetside text and use this sampled subset as “pseudo ground truth” to construct bdrfor training. 3.5 Reverse Order Decoding 3.5.1 SentenceLevel Control We imitate the process of human translators translating texts in rhyme: translating the last word ﬁrst, and from back to front, which is an old trick to keep rhyming patterns from being forced ( Low,2003 ). We implement this by reverseorder decoding. During ﬁnetuning with parallel data, we reverse the word order of targetside text while retaining the sourceside text unchanged. This approach minimally changes the structure and workﬂow of offthe-shelf translation models. 3.5.2 ParagraphLevel Ranking Controllability alone is not enough. For a given input sentence, the rhyming usually only looks good in certain rhyme types but appears forced in others (see Appendix C.2for details). No matter how good the controllability is, the output quality will be severely damaged if an illﬁtting rhyme prompt is provided by the user. To avoid such problems, we need to determine the most suitable endrhyme for translating one sentence, and further one paragraph consisting of multiple sentences. Previous research left this problem unsolved. Fortunately, our reverseorder decoder simpliﬁes the rhyme ranking process. During training, we use an additional special token rhy_ 0to nullify rhyme constraints for output. We achieve this by randomly converting a portion of each type of rhyme prompt torhy_ 0during training. At inference time, for a given source sentence Xiand prompts ltgt,rtgtand btgt, we ﬁrst use rhy_ 0as the rhyme prompt to dothe ﬁrst step of reverseorder decoding to obtain the endword probability distribution, P(y−1|X, l tgt,btgt,rhy_ 0) = [p(w1), p(w2), . . . , p (wv)], (1) where the vis the vocabulary size of the target language. Note that the p(wj)not only indicates the endword probability, but also predicts output text quality and the likelihood of satisfaction of length and word boundary constraints of the rhymeunconstrained model, from a greedy point of view. Intuitively, starting with tokens with low probabilities will pull down the corresponding beams’ scores and degrade the output quality. On the contrary, sentences with higher quality can be obtained by starting decoding with wjwith higher p(wj), and we achieve this by giving the model a rhyme prompt that guides it towards starting with such wj. We sum up the probability in Eq. 1within each rhyme type to obtain the rhyme distribution of given inputs, pi=/summationdisplay Rhy(wj)∈rhyme ip(wj) P(Rhy (Y)|X, l tgt,btgt,rhy_ 0) =P(Rhy (y−1)|X, l tgt,btgt,rhy_ 0) = [p1, p2, . . . , p u], where Rhy (·)is a map between a word or the endword of a sentence to its rhyme type, uis the number of rhyme types in the target language. For a certain rhyme type i, a higher pivalue indicates a higher probability of successful rhyming and higher output quality. When translating a paragraph of lyrics, we have multiple sentences together with their corresponding length and boundary prompts as input: X= [X1, X2, . . . , X n],with prompts [(ltgt1,btgt1),(ltgt2,btgt2), . . . , (ltgtn,btgtn)]. With the assumption that every sentence is of equal importance, we compute a normalized rhyme distribution for this paragraph by P(Rhy (Yk)) = f(Xk, ltgtk,btgtk,rhy_ 0), P(Rhy (Y)) = softmax (n/summationdisplay k=1P(Rhy (Yk))) where frefers to the ﬁrst step of reverseorder decoding. We then use P(Rhy (Y))as the rhyme ranking score of this paragraph to guide the rhyme selection.451Transformer Encoder Transformer Decoder <len_5> [bdr_0, bdr_0, bdr_0, bdr_1, bdr_0, bdr_0, bdr_0, bdr_0] en_XX I woke up in a panic </s>zh_CN <rhy_10> 张慌措失我的来醒 </s> </s> zh_CN <rhy_10> 张慌措失我的来醒 </s> (a) Model loss (b)ModelMusic Ranking (c) Figure 3: (a): Structure of our Englishto-Chinese lyric translation system. (b): Workﬂow of the ﬁnetuning stage. (c) Workﬂow of the inference stage. 3.6 Utilizing Monolingual Data Indomain parallel data suffer from two issues. First, its amount is so limited that it is not comparable with generaldomain data. Second, there are severe quality issues when targetside lyrics are translated by online communities, including wrong translation ( Li,2020 ), creative treason ( Zhang , 2022 ), overdomestication ( Xie and Lei ,2022 ), etc. To mitigate the issue of data quantity and quality, we seek help from targetside monolingual lyric data. Our approach involves incorporating backtranslation ( Sennrich et al. ,2015 ) of targetside indomain monolingual data to augment the parallel data for ﬁnetuning. To demonstrate its effectiveness, we conduct a comparative study with the adaptation  evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive ﬁnetuning1. 1 analysis of targetside sentences, guiding the model towards generating sentences with corresponding properties. Consequently, there is no need for accompanying music during training. At the inference stage, prompts can be crafted from either music or sourceside sentences. For an overview of the system workﬂow, please refer to Figures 3band3c. We conducted a group of experiments to test three different prompt methods to determine the best one for each control aspect. They are (1) Encpref: prompts are injected into the encoder’s input as a preﬁx. (2) Decpref: prompts are injected into the decoder’s input as a preﬁx. (3) Decemb: prompts are embedded into a vector and added toward the decoder’s input. 3.4 Word Boundary Control Intraword pause is a typical disﬂuency pattern of beginning language learners ( Franco et al. ,1998 ). However, improperly translated lyrics usually con- (a) (b) Figure 2: Demonstration of the necessity of word boundary control. Blue box: musical pauses; orange box: notes highlighted by downbeats; red box: words interrupted by musical pauses or highlighted notes; green box: words without interruption. tain multisyllable words that lies across musical pauses, as the blue box in Figure 2, so that the performer has to make awkward intraword pauses while singing ( Guo et al. ,2022 ), causing a drop in pronunciation acceptability. Besides, we observe that positioning highlighted music notes, such as high notes or downbeats, as the orange box in Figure2, onto a multisyllable word’s second or later syllables can bring similar adverse effects due to abrupt changes of pitch and tension3. We address these issues by carefully designing the placement of word boundaries in outputs, i.e., the points between two consecutive syllables that are from different words. Our aim is to ensure that word boundaries align precisely with the boundaries in music, i.e., the melody boundaries , which occur atmusical pauses and before highlighted notes (the blue and orange boxes in Figure 2). In this way, we achieve higher compatibility between the output sentences and the accompanying music, enhance the ﬂuency and consistency of pronunciation during singing, and hence lead to the gain of singability. This solution is achieved by promptbased word boundary control. We use the prompt bdrto represent melody boundary positions, indicating necessary word boundary positions. bdris a sequence of special tokens, and each token corresponds to one syllable in the output. There are two types of special interior tokens: bdr_1 and bdr_0, representing after the corresponding syllable “there should be a word boundary” and “we do not care if there 3Stresstimed languages have another solution to this second problem, i.e., put a stressed syllable at the highlighted note. Here we discuss another generic solution.450is a boundary”, respectively. At test time, bdris obtained from accompanying music and serves as additional inputs. A welltrained wordboundary- aware model can hence place word boundaries at the desired positions to achieve better music–lyric compatibility. For locations where bdr_ 0is present (“don’t care”), the translation model operates unconstrained, maximizing translation naturalness. During training, length and rhyme prompts can be obtained directly from the target sentences in the training samples, but not again for necessary word boundary prompts because they have to be obtained from accompanying music which is absent in training. Nevertheless, we offer a solution: we randomly sample from all actual word boundary positions from the targetside text and use this sampled subset as “pseudo ground truth” to construct bdrfor training. 3.5 Reverse Order Decoding 3.5.1 SentenceLevel Control We imitate the process of human translators translating texts in rhyme: translating the last word ﬁrst, and from back to front, which is an old trick to keep rhyming patterns from being forced ( Low,2003 ). We implement this by reverseorder decoding. During ﬁnetuning with parallel data, we reverse the word order of targetside text while retaining the sourceside text unchanged. This approach minimally changes the structure and workﬂow of offthe-shelf translation models. 3.5.2 ParagraphLevel Ranking Controllability alone is not enough. For a given input sentence, the rhyming usually only looks good in certain rhyme types but appears forced in others (see Appendix C.2for details). No matter how good the controllability is, the output quality will be severely damaged if an illﬁtting rhyme prompt is provided by the user. To avoid such problems, we need to determine the most suitable endrhyme for translating one sentence, and further one paragraph consisting of multiple sentences. Previous research left this problem unsolved. Fortunately, our reverseorder decoder simpliﬁes the rhyme ranking process. During training, we use an additional special token rhy_ 0to nullify rhyme constraints for output. We achieve this by randomly converting a portion of each type of rhyme prompt torhy_ 0during training. At inference time, for a given source sentence Xiand prompts ltgt,rtgtand btgt, we ﬁrst use rhy_ 0as the rhyme prompt to dothe ﬁrst step of reverseorder decoding to obtain the endword probability distribution, P(y−1|X, l tgt,btgt,rhy_ 0) = [p(w1), p(w2), . . . , p (wv)], (1) where the vis the vocabulary size of the target language. Note that the p(wj)not only indicates the endword probability, but also predicts output text quality and the likelihood of satisfaction of length and word boundary constraints of the rhymeunconstrained model, from a greedy point of view. Intuitively, starting with tokens with low probabilities will pull down the corresponding beams’ scores and degrade the output quality. On the contrary, sentences with higher quality can be obtained by starting decoding with wjwith higher p(wj), and we achieve this by giving the model a rhyme prompt that guides it towards starting with such wj. We sum up the probability in Eq. 1within each rhyme type to obtain the rhyme distribution of given inputs, pi=/summationdisplay Rhy(wj)∈rhyme ip(wj) P(Rhy (Y)|X, l tgt,btgt,rhy_ 0) =P(Rhy (y−1)|X, l tgt,btgt,rhy_ 0) = [p1, p2, . . . , p u], where Rhy (·)is a map between a word or the endword of a sentence to its rhyme type, uis the number of rhyme types in the target language. For a certain rhyme type i, a higher pivalue indicates a higher probability of successful rhyming and higher output quality. When translating a paragraph of lyrics, we have multiple sentences together with their corresponding length and boundary prompts as input: X= [X1, X2, . . . , X n],with prompts [(ltgt1,btgt1),(ltgt2,btgt2), . . . , (ltgtn,btgtn)]. With the assumption that every sentence is of equal importance, we compute a normalized rhyme distribution for this paragraph by P(Rhy (Yk)) = f(Xk, ltgtk,btgtk,rhy_ 0), P(Rhy (Y)) = softmax (n/summationdisplay k=1P(Rhy (Yk))) where frefers to the ﬁrst step of reverseorder decoding. We then use P(Rhy (Y))as the rhyme ranking score of this paragraph to guide the rhyme selection.451Transformer Encoder Transformer Decoder <len_5> [bdr_0, bdr_0, bdr_0, bdr_1, bdr_0, bdr_0, bdr_0, bdr_0] en_XX I woke up in a panic </s>zh_CN <rhy_10> 张慌措失我的来醒 </s> </s> zh_CN <rhy_10> 张慌措失我的来醒 </s> (a) Model loss (b)ModelMusic Ranking (c) Figure 3: (a): Structure of our Englishto-Chinese lyric translation system. (b): Workﬂow of the ﬁnetuning stage. (c) Workﬂow of the inference stage. 3.6 Utilizing Monolingual Data Indomain parallel data suffer from two issues. First, its amount is so limited that it is not comparable with generaldomain data. Second, there are severe quality issues when targetside lyrics are translated by online communities, including wrong translation ( Li,2020 ), creative treason ( Zhang , 2022 ), overdomestication ( Xie and Lei ,2022 ), etc. To mitigate the issue of data quantity and quality, we seek help from targetside monolingual lyric data. Our approach involves incorporating backtranslation ( Sennrich et al. ,2015 ) of targetside indomain monolingual data to augment the parallel data for ﬁnetuning. To demonstrate its effectiveness, we conduct a comparative study with the adaptation Experiments In this section, we first experimentally evaluate the quality of the GraCe dataset by applying it to prefixbased simile generation (§ 5.1). Since the setup of this uncontrollable generation task does not need additional annotations on the training samples, we can compare GraCe with previous Chinese simile datasets. Based on it, we then evaluate the proposed Similor on the new CSG task (§ 5.2). Specifically, we first compare different model varieties of Similor constrained by comparator andvehicle , and then evaluate the performances of Similor under more extensive constraints. Finally, we explore whether Scorer helps Similor to generate similes in thevehicle -unknown setup. 5.1 Experimental Analysis of GraCe As statistical analysis is insufficient to evaluate GraCe, we evaluate it by prefixbased simile generation. One of the simple pipelines is to train a472Dataset % Comp. ↑Simile Conf. ↑PPL↓ Backbone: ChineseGPT2 None 1.4 0.3 40.9 CS (2021) 46.0 0.6 43.0 CMC (2022) 44.4 0.7 30.9 GraCe 93.5 0.9 10.9 Backbone: ChineseBART CS (2021) 65.3 0.5 33.1 CMC (2022) 56.7 0.8 33.3 GraCe 85.3 0.9 28.7 Table 5: The main results of prefix generation. “None” means using the backbone model to generate sentences without any continuing training, we ignore “None” of ChineseBART as it performs poorly in fluency. ↑means a higher score is better whereas ↓is exactly the opposite. Highest numbers are in bold . Dataset Fluen. ↑Creat. ↑Consi. ↑Overall ↑ CS 2.5 1.9 1.9 2.1 CMC 2.2 2.0 1.9 2.0 GraCe 3.0 3.2 3.2 2.8 Table 6: The human evaluation of prefix generation. generator with the language modeling object on the simile dataset. In inference, this model is asked to generate a simile with a prespecified tenor . Baselines and Backbones. We compare the proposed GraCe with previous Chinese simile datasets: 1)CS(Zhang et al., 2021) contains 5.49M similes extracted from online fictions. 2) CMC (Li et al., 2022) contains 2.7k metaphors and similes from Chinese literature corpus. Besides, we utilize two representative Chinese pretrained language models to avoid training from scratch: 1) ChineseBART (CBART) (Shao et al., 2021): a BART Large model pretrained on 200GB text from Chinese Wikipedia and WuDaoCorpus. 2) ChineseGPT2 (CGPT2) (Zhao et al., 2019): a GPT2 Medium model pretrained on the CLUECorpusSmall dataset. Experiment Setup. We employ the original hyperparameter setting of BART Large and GPT2 Medium to train all models, with a BERT tokenizer (Devlin et al., 2019) to process Chinese text. During inference, we use 25 common tenor s as prefixes and ask models to continue writing with them (100 completions for each).12 Metrics. For automatic evaluation, we first use Perplexity (PPL) from CGPT2 to evaluate the text quality. As for simile evaluation, we compute 12See Appendix B.1 for the word list and inference setupthe proportion of sentences containing comparator words ( %Comp. ) to evaluate elementincomplete cases, because it’s the hallmark of a simile. However, a sentence containing comparator words may not trigger a simile (Liu et al., 2018). Therefore, we useSimile Conf. to evaluate the figurative meaning of the generated results, i.e., elementmismatched cases. Specifically, we reuse the simile classifier in Step 2 of the dataset processing (See § 3.1) to compute the averaged confidence score of each  evaluation of prefix generation. generator with the language modeling object on the simile dataset. In inference, this model is asked to generate a simile with a prespecified tenor . Baselines and Backbones. We compare the proposed GraCe with previous Chinese simile datasets: 1)CS(Zhang et al., 2021) contains 5.49M similes extracted from online fictions. 2) CMC (Li et al., 2022) contains 2.7k metaphors and similes from Chinese literature corpus. Besides, we utilize two representative Chinese pretrained language models to avoid training from scratch: 1) ChineseBART (CBART) (Shao et al., 2021): a BART Large model pretrained on 200GB text from Chinese Wikipedia and WuDaoCorpus. 2) ChineseGPT2 (CGPT2) (Zhao et al., 2019): a GPT2 Medium model pretrained on the CLUECorpusSmall dataset. Experiment Setup. We employ the original hyperparameter setting of BART Large and GPT2 Medium to train all models, with a BERT tokenizer (Devlin et al., 2019) to process Chinese text. During inference, we use 25 common tenor s as prefixes and ask models to continue writing with them (100 completions for each).12 Metrics. For automatic evaluation, we first use Perplexity (PPL) from CGPT2 to evaluate the text quality. As for simile evaluation, we compute 12See Appendix B.1 for the word list and inference setupthe proportion of sentences containing comparator words ( %Comp. ) to evaluate elementincomplete cases, because it’s the hallmark of a simile. However, a sentence containing comparator words may not trigger a simile (Liu et al., 2018). Therefore, we useSimile Conf. to evaluate the figurative meaning of the generated results, i.e., elementmismatched cases. Specifically, we reuse the simile classifier in Step 2 of the dataset processing (See § 3.1) to compute the averaged confidence score of each analysis. 3.1 Dataset Creation Dataset Collection We collect 260k student compositions (grades range from elementary to high school) from the freeaccess website,4ensuring data resources are close to realworld cases. After sentence segmentation and the removal of non3The details of % IsSimile are in Appendix C.1. 4https://www.hxszww.com/Chinese sentences, we get about 5.48 million sentences. At most two sentences above and below each sample are used as the context element. Dataset Processing As shown in Figure 2, we build our GraCe dataset in four steps. In Step 1 , we filter out sentences that do not contain comparator-related words. Specifically, we tokenize candidate sentences with the toolkit Jieba5and filter out sentences without comparator -related words, ascomparator is the hallmark of a simile. The comparator words are varied to ensure the diversity of simile patterns (e.g., “ 好像”, “仿佛”,“犹如”, etc, all means “like”). However, a sentence containing comparator may not trigger a simile (Liu et al., 2018). As the example 2 in Step 1, “ 他还是像过 去一样喜欢打篮球。(He still likes playing basketball as before.)”, here “ 像(as)” implies identity rather than comparison. Therefore, Step 2 focuses on recognizing nonsimile sentences containing comparator words. We train a binary classifier based on RoBERTa Large (Liu et al., 2019a) with a confidence score of 80% to select similes.6Notably, we do not pursue higher score confidence as it may face the risk of reducing patterns of simile. After the above two steps, we get the simile dataset without finegrained annotations. Therefore, Step 3 aims at annotating tenor ,topic , and vehicle for each simile. We utilize a sequence labeling model based on RoBERTa Large to annotate tenor andvehicle for each simile.7Meanwhile, we annotate topic as the span between tenor and comparator , which denotes tenor and its supplementary description. After that, Step 4 furtherly aims at annotating the ground and cognitive properties of tenor andvehicle . As the interpretation 5https://github.com/fxsjy/jieba 6Details of the classifier are in Appendix A.1 7Details of the labeling model are in Appendix A.2470Measurement # Nums # Average Tokens Sentences 61,360 89.0 Annotated Elements Topic 61,360 11.4 Tenor 61,360 1.9 Tenor Property 52,474 73.2 Comparator 61,360 2.6 Vehicle 61,360 2.3 Vehicle Property 61,360 83.0 Ground 15,087 8.6 Context 57,543 39.5 Table 2: Core statistics of the GraCe dataset. Here ground denotes the explicit ground in the simile. We annotate implicit ground as the shared properties between tenor andvehicle . Measurement Value % Simile 98.9 % Correct Tenor 95.2 % Correct Vehicle 98.2 % Correct Comparator 98.7 % Correct Ground 94.1 Table 3: Statistics of 1000 randomly selected samples from the GraCe annotated by three professional annotators. 98.9% samples are similes. The statistics of the dash line below are calculated for these similes. for a simile comparison (Tartakovsky et al., 2019), ground plays an important role in making the tenor - vehicle pair of a simile being easilyunderstood and figuratively meaningful (Campbell and Katz, 2006; End, 1986), yet being ignored in previous datasets. We first query Cogbank dataset8to obtain the cognitive properties for both tenor andvehicle . Then, their shared properties are used to fuzzy match9the propertyrelated clauses in a simile as the ground . Finally, the detailed statistics of our GraCe dataset are shown in Table 2, and some dataset samples are shown in Appendix A.4. 3.2 Dataset Analysis Data Quality We invite three professional annotators to independently annotate 1000 randomly selected samples from multiple aspects.10As shown in Table 3, only 1.1% samples are not similes, which is far beyond other Chinese simile datasets (see Table 1). More importantly, it maintains high accuracies even in finegrained annotations for important elements of a simile (94.1% - 98.7%). 8https://catalog.ldc.upenn.edu/LDC2020T01 9See algorithmic details in Appendix A.3 10Details of human annotation are in Appendix C.1Measurement Value # Distinct Tenors 7,958 # Distinct Vehicles 5,350 # Distinct Comparators 371 Table 4: Distinct Statistics of the GraCe dataset. Diversity of Similes We analyze the diversity of similes and present the statistics in Table 4. First, the fertility of tenor andvehicle ensure the diverse content of the simile. Besides, different from Liu et al. (2018); Chakrabarty et al. (2020) using only a single pattern comparator of simile in their dataset (i.e., “_好像(like) _” in Chinese), we build the comparator as 371 patterns of fillin-theblank templets. Specifically, inspired by WPS (Zhang et al., 2021) that the position information of simile in the context is a strong feature, we incorporate it by adding the punctuation that closely followed the vehicle to our template. As shown in Appendix Figure 5, “_ 如同(like) _，” means the simile part appears in the middle clause without any description after vehicle . If no punctuation in the template, it means there is an explicit ground orcontext after vehicle to complement the content. 4 Controllable Simile Generation 4.1 Task Definition The controllable simile generation task is formulated as follows: given a topicxcontaining a tenor stand a variety of prespecified constraints c, the model generates a simile y= (y1, y2, ..., y N)by: p(y|x,c) =N/productdisplay n=1p(yn|y<n,x,c;θ), (1) where θare the model parameters. Notably, the constraints ccan be freely selected and combined from the candidate set s= (sv, sp, sc), which denote the vehicle ,comparator , and context , respectively. 4.2 Methodology We benchmark this task with the CSG model Similor, which contains a module Scorer for the vehicle-unknown situation. To ease of presentation, we start with a toy example to illustrate them. Similor As shown in Figure 3, the topic “美 丽的春天(the beautiful spring)” containing the tenor “春天(spring)” is firstly concatenated with optional sequential constraints by the separator signal “[SEP]”. If the vehicle is prespecified in471Constraints[SEP]SeparatorSimilePrefix!\"#$%(Thebeautifulspring)NoHasVehicle?YesContextAbove!\"#$%&(The birds are singing)Comparator'((like)Vehicle)(Painting)ContextBelow*+,-./&(Thegroundisalsowakeup.)!\"(Spring) EncoderDecoder&'()*+,-./01#2345(is like a painting. It gathers the colors of nature)PretrainedonthestudentcompositionsTop 20 Properties...67(warm)!\"(beautiful)889:(thriving)CogbankDatasetQueryCandidatesRanking$;(springwind)*(painting)<=(mother)123...Step1 GetFiguratively MeaningfulCandidates(Ranked by NumbersofMatched Properties) CandidatesRanking$;(springwind)*(painting)<=(mother)123...Step 2 ChooseLiterally FalseCandidatewithTenor(Ranked by EuclideanDistanceofWordEmbeddings)#(painting)PretrainedLanguageModelScorerⅩ✓✓Shared Properties Ⅹ✓✓67(warm), !\"(beautiful)…>?(bright), !&(nice)…!&(nice), 6@(soft)…✓ⅩQualified word to act as the Vehicle Unqualified word to act as the Vehicle WordEmbeddings!\"(Spring) *(painting)<=(mother)$;(springwind)$%(Spring)Figure 3: A toy example to elaborate the workflow of Similor and Scorer. the constraints, the input sequence is then fed into an encoderdecoder model. Afterward, the model autoregressively generates “ 好像一幅画，它收 集了大自然的色彩。(is like a painting. It gathers the colors of nature.)”. We first continue pretraining the large Chinese text generation model (e.g., ChineseBART (Shao et al., 2021)) on the collected 260k student compositions with the language modeling object. Then, Similor is instantiated with it to be finetuned on the GraCe. Scorer If the vehicle is unknown, we use the Scorer module to retrieve a vehicle and then add it to the input sequence. As shown in the right part of Figure 3, Scorer contains two steps to get figuratively meaningful while literally false pair of tenor -vehicle .Step 1 queries Cogbank dataset for thetenor “春天(spring)” to obtain its top kmost frequently used cognitive properties. These properties provide a basis for vehicle candidates selection and matching. The Cogbank dataset (83,017 items) contains more words than the glossary of common words in modern Chinese11(56,008 items), allowing fuller retrieval of vehicle candidates. In the implementation, the top 20 nouns with numbers of cognitive properties identical to tenor are chosen as candidates, which ensures a figuratively meaningful simile as the matched properties can be regarded as the ground . However, some literalrelated words may also be selected in this step, e.g, “ 春风(spring wind)”. To obtain only figurative items, Step 2 reranks the Step 1 candidate based on the Euclidean distance of word embeddings between each item andtenor . Candidates with a longer distance are ranked higher, as they are less literally associated 11http://www.moe.gov.cn/ewebeditor/uploadfile/ 2015/01/13/20150113085920115.pdfwith tenor . As a result, the “ 画(painting)” is selected as the final vehicle . To be exact, given a tenor st, theith item wiin Cogbank dataset get the ranking score Score candi iby: Score wi=Rank (Figwi) +Rank (Litwi), Figwi=Match (wi, st), Litwi=EucDist (wi, st).(2) Where Rank (·)denotes getting the ranking of the corresponding score. Match (·)means to count the numbers of shared cognitive properties between two items and EucDist (·)means the Euclidean distance between their word embedding. Notably, we use rankings to normalize these scores, avoiding the effects of different score scales. 5 Experiments In this section, we first experimentally evaluate the quality of the GraCe dataset by applying it to prefixbased simile generation (§ 5.1). Since the setup of this uncontrollable generation task does not need additional annotations on the training samples, we can compare GraCe with previous Chinese simile datasets. Based on it, we then evaluate the proposed Similor on the new CSG task (§ 5.2). Specifically, we first compare different model varieties of Similor constrained by comparator andvehicle , and then evaluate the performances of Similor under more extensive constraints. Finally, we explore whether Scorer helps Similor to generate similes in thevehicle -unknown setup. 5.1 Experimental Analysis of GraCe As statistical analysis is insufficient to evaluate GraCe, we evaluate it by prefixbased simile generation. One of the simple pipelines is to train a472Dataset % Comp. ↑Simile Conf. ↑PPL↓ Backbone: ChineseGPT2 None 1.4 0.3 40.9 CS (2021) 46.0 0.6 43.0 CMC (2022) 44.4 0.7 30.9 GraCe 93.5 0.9 10.9 Backbone: ChineseBART CS (2021) 65.3 0.5 33.1 CMC (2022) 56.7 0.8 33.3 GraCe 85.3 0.9 28.7 Table 5: The main results of prefix generation. “None” means using the backbone model to generate sentences without any continuing training, we ignore “None” of ChineseBART as it performs poorly in fluency. ↑means a higher score is better whereas ↓is exactly the opposite. Highest numbers are in bold . Dataset Fluen. ↑Creat. ↑Consi. ↑Overall ↑ CS 2.5 1.9 1.9 2.1 CMC 2.2 2.0 1.9 2.0 GraCe 3.0 3.2 3.2 2.8 Table 6: The human evaluation of prefix generation. generator with the language modeling object on the simile dataset. In inference, this model is asked to generate a simile with a prespecified tenor . Baselines and Backbones. We compare the proposed GraCe with previous Chinese simile datasets: 1)CS(Zhang et al., 2021) contains 5.49M similes extracted from online fictions. 2) CMC (Li et al., 2022) contains 2.7k metaphors and similes from Chinese literature corpus. Besides, we utilize two representative Chinese pretrained language models to avoid training from scratch: 1) ChineseBART (CBART) (Shao et al., 2021): a BART Large model pretrained on 200GB text from Chinese Wikipedia and WuDaoCorpus. 2) ChineseGPT2 (CGPT2) (Zhao et al., 2019): a GPT2 Medium model pretrained on the CLUECorpusSmall dataset. Experiment Setup. We employ the original hyperparameter setting of BART Large and GPT2 Medium to train all models, with a BERT tokenizer (Devlin et al., 2019) to process Chinese text. During inference, we use 25 common tenor s as prefixes and ask models to continue writing with them (100 completions for each).12 Metrics. For automatic evaluation, we first use Perplexity (PPL) from CGPT2 to evaluate the text quality. As for simile evaluation, we compute 12See Appendix B.1 for the word list and inference setupthe proportion of sentences containing comparator words ( %Comp. ) to evaluate elementincomplete cases, because it’s the hallmark of a simile. However, a sentence containing comparator words may not trigger a simile (Liu et al., 2018). Therefore, we useSimile Conf. to evaluate the figurative meaning of the generated results, i.e., elementmismatched cases. Specifically, we reuse the simile classifier in Step 2 of the dataset processing (See § 3.1) to compute the averaged confidence score of each Experiments 4.1 Downstream Task Setup Textto-Video Retrieval. Given a text query, the goal of this task is to retrieve relevant videos from a large set of videos. We evaluate our model on the following datasets: ( i)MSRVTT (Xu et al.,2016) contains 10K YouTube videos, each paired with 20 captions. We follow (Yu et al., 2018; Lei et al., 2021) to use the 7K train+val videos for training, and report results on the 1K test set. ( ii) DiDeMo (Anne Hendricks et al., 2017) contains 10K Flickr videos with 41K captions. We use standard train/val/test splits. ( iii)ActivityNet Captions (Krishna et al., 2017a) contains 20K YouTube videos with 100K captions. We use the train split with 10K videos for training, and we report results on the widely used val1 split, with 4.9K videos. For MSRVTT, we evaluate standard textto-video retrieval. For DiDeMo and ActivityNet Captions, we evaluate paragraphto-video retrieval (Liu et al., 2020; Lei et al., 2021; Luo et al., 2021), where the text captions in the same video are concatenated as a single paragraphlevel text for retrieval. We report performance using recall at K (R@K). Video Question Answering. Given a video (often with a text question), this task requires generating an answer to the question or selecting the most suitable answer from a set of candidates. ( i) MSRVTTQA (Xu et al., 2017) contains 244K openended questions on 10K MSRVTT videos. (ii)ActivityNetQA (Yu et al., 2019) contains 58K openended questions on 5.8K sampled ActivityNet (Caba Heilbron et al., 2015) videos. ( iii) MSRVTTMC (Yu et al., 2018) is a multiplechoice task that requires selecting the matched caption from 5 candidate captions for each video (3K MSRVTT videos). We use standard train/val/test splits for the three tasks, and report accuracy. 4.2 Comparison on Existing Datasets Textto-Video Retrieval Results. In Table 1, we compare SINGULARITY with existing methods on490Table 1: Comparison to existing methods on textto-video retrieval. #PT denotes the number of images and or videos used in crossmodal pretraining. #Train Frame denotes the number of frames used at each training step during finetuning. For models that use different number of frames for different datasets, we list them together with a separator “/”. We gray out methods that use significantly more pretraining data for a fair comparison. The 136M corpus is from HowTo100M (Miech et al., 2019), 0.2M refers to COCO+VG data, 138M is the combination of HowTo100M and WebVid, 400M is the private imagetext data used in CLIP (Radford et al., 2021).  evaluation of videoand-language models, we propose two new retrieval tasks based on existing finegrained action recognition datasets that encourage temporal modeling. Our code is available at https: //github.com/jayleicn/singularity . 1 Analysis Frames Ensemble Strategy. Our model is trained with a singleframe regime, while using multiple frames covering the full video at inference. As shown in Figure 5a ( concat ), encoded video frames are concatenated as input to the multimodal encoder’s crossattention for making a videolevel prediction. A naive alternative is to compute the492Table 3: Comparison to existing methods on SSv2 tasks. * The training of Frozen on the SSv2label retrieval task fails to converge despite our best efforts in tuning the model. experiments, we conduct realworld annotation experiments to validate the effectiveness of our approach for practical use. 2 Related Work ED and the Partial Annotation Issue. Event detection (ED) is a crucial subtask of event extraction that aims to identify event instances in texts (Grishman, 1997; Ahn, 2006). The existing ED methods can be divided as featurebased (Ahn, 2006; Li et al., 2013; Liao and Grishman, 2010; A man died when a tank devastated Ă Die Attack[M] A man died when a tank devastated Ă [M] A man died when a tank devastated Ă Prompting PromptingDie O (B) Trigger Localization (Our Approach)(A) Hard ClassificationFigure 2: A comparison of the hard classiﬁcation paradigm (A) and our trigger localization paradigm (B) for ED. [M] is a “noevent-existing” indicator. Hong et al., 2011) and deep learningbased (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Liu et al., 2018a; Feng et al., 2016; Chen et al., 2018; Yang et al., 2019; Liu et al., 2020a; Du and Cardie, 2020; Lu et al., 2021; Liu et al., 2019a), and there has been a growing interest in applying these methods to speciﬁc scenarios (Liu et al., 2019b, 2022b,a). Nevertheless, most of such methods adopt supervised learning and assume the availability of clean datasets. To date, only a few studies have considered the partial annotation issue in ED: Liu et al. (2020b) identify several unlabeled cases in the ACE test set for error analysis; Liu (2018), in the PhD proposal, suggest that the Chinese portion of ACE 2005 is partially labeled. Unfortunately, neither work stands in a methodology perspective for addressing the issue. Our research, on the other hand, introduces a solution for learning with partial annotations. Our trigger localization formulation also relates to using prompts for event information extraction (Wang et al., 2022a; Hsu et al., 2022; Liu et al., 2022c; Wang et al., 2022b), but different from them focusing on improving the overall performance, our work stands in a point addressing the partial annotation issue. Learning with Partial Annotations. Learning with partial annotations, also known as positive and unlabeled learning (Li et al., 2009), is an important problem in machine learning community (Elkan and Noto, 2008; Liu et al., 2002, 2003, 2005). In the domain of natural language processing (NLP), researchers have examined a number of tasks including named entity recognition (NER) (Jie et al., 2019; Mayhew et al., 2019; Peng et al., 2019), Chinese word segmentation (Yang and V ozila, 2014),509[A man died when a heavy tank devasta ted the hotel ] BERT[CLS] Die [SEP] A man died when a heavy tank … [CLS] Attack [SEP] A man … tank devastated … λ+[O O Die O O O O O O O ] BERT X : ED Model ED Model Sentence(i) A Normal Training Stage(ii) An Uncertainty -Guided Interfering Stage (Sentence , Revised Label )λA man died when a heavy tank devasta ted the hotel . BERT[CLS] DIE [SEP] A man died when a heavy tank … (Sentence , Annotation ) [CLS] ATTACK [SEP] A man … tank devastated … λ+O O DIE O O O O O O O BERT Sentence :Annotation : ED ModelED Model Sentence A Normal Training StepAn Uncertainty -Guided Interfering Step (Sentence , Revised Label )λ- (Sentence , Annotation ) Y : ED Model X [devasta ted, devasta ted, [SEP], …, devasta ted, [SEP], devasta ted,…] Retraining Revised Label SequenceSamplingAttack K timesMCDropout Figure 3: The overview of our approach. Left: the model training process based on margin softmax. Right: the uncertaintyguided retraining mechanism. Ddesignates the original training dataset. and others (Tsuboi et al., 2008). The efforts for NER relate the most to our work, where a seminal work (Jie et al., 2019) treats the labels of negative instances as latent variables and infers them using partial Conditional Random Fields (Bellare and McCallum, 2007). Later works have devised downweighing mechanisms (Mayhew et al., 2019), conﬁdence estimation methods (Liu et al., 2021), and negative sampling (Li et al., 2021b) for learning. In this study, we offer a new trigger localization formulation for the task of ED and demonstrate promising results in a wide range of partial annotation settings. 3 Proposed  evaluation (Christopher Walker and Maeda, 2006), nearly 20% of events are not labelled (see Table 2). Using a partially labelled dataset as a fully labelled one for training runs the risk of mistraining on false negatives, and using a partially labelled dataset for evaluation biases comparison. HowA man died when a heavy tank devastate d the hotel. Gold: Partial: False NegativeO O Die O O O O Attack O O O O Die O O O O O O O S1:Figure 1: The partial annotation issue in ED. The Gold row indicates groundtruth labels; the Partial row indicates the partial annotation case we address in this study, where the devastated event is not labeled. ever, this issue is still understudied in the existing literature (Liu, 2018; Liu et al., 2020b). In this work, we present a seminal study of learning with partial annotations for ED, with contributions in methodology, data, and practical applications. In our analysis; Liu (2018), in the PhD proposal, suggest that the Chinese portion of ACE 2005 is partially labeled. Unfortunately, neither work stands in a methodology perspective for addressing the issue. Our research, on the other hand, introduces a solution for learning with partial annotations. Our trigger localization formulation also relates to using prompts for event information extraction (Wang et al., 2022a; Hsu et al., 2022; Liu et al., 2022c; Wang et al., 2022b), but different from them focusing on improving the overall performance, our work stands in a point addressing the partial annotation issue. Learning with Partial Annotations. Learning with partial annotations, also known as positive and unlabeled learning (Li et al., 2009), is an important problem in machine learning community (Elkan and Noto, 2008; Liu et al., 2002, 2003, 2005). In the domain of natural language processing (NLP), researchers have examined a number of tasks including named entity recognition (NER) (Jie et al., 2019; Mayhew et al., 2019; Peng et al., 2019), Chinese word segmentation (Yang and V ozila, 2014),509[A man died when a heavy tank devasta ted the hotel ] BERT[CLS] Die [SEP] A man died when a heavy tank … [CLS] Attack [SEP] A man … tank devastated … λ+[O O Die O O O O O O O ] BERT X : ED Model ED Model Sentence(i) A Normal Training Stage(ii) An Uncertainty -Guided Interfering Stage (Sentence , Revised Label )λA man died when a heavy tank devasta ted the hotel . BERT[CLS] DIE [SEP] A man died when a heavy tank … (Sentence , Annotation ) [CLS] ATTACK [SEP] A man … tank devastated … λ+O O DIE O O O O O O O BERT Sentence :Annotation : ED ModelED Model Sentence A Normal Training StepAn Uncertainty -Guided Interfering Step (Sentence , Revised Label )λ- (Sentence , Annotation ) Y : ED Model X [devasta ted, devasta ted, [SEP], …, devasta ted, [SEP], devasta ted,…] Retraining Revised Label SequenceSamplingAttack K timesMCDropout Figure 3: The overview of our approach. Left: the model training process based on margin softmax. Right: the uncertaintyguided retraining mechanism. Ddesignates the original training dataset. and others (Tsuboi et al., 2008). The efforts for NER relate the most to our work, where a seminal work (Jie et al., 2019) treats the labels of negative instances as latent variables and infers them using partial Conditional Random Fields (Bellare and McCallum, 2007). Later works have devised downweighing mechanisms (Mayhew et al., 2019), conﬁdence estimation methods (Liu et al., 2021), and negative sampling (Li et al., 2021b) for learning. In this study, we offer a new trigger localization formulation for the task of ED and demonstrate promising results in a wide range of partial annotation settings. 3 Proposed experiments and analysis, we demonstrate that W2Wis a more coherent and fast grounded word learner, and that the grounding ability acquired during pretraining helps the model to learn unseen words more rapidly and robustly.1 1  evaluation protocol and introduce the dataset we curated for this problem. 2.1 Grounded Word Acquisition Many visionlanguage tasks have been developed in the past, e.g., visual question answering, visual commonsense reasoning, etc. However, these tasks are mainly focused on the end task performance without scrutinizing whether words are grounded to their corresponding visual entities. We Two boats of people, a smaller yellow <mask> with two people and a larger white boat with six people. Two boats of people, a smaller yellow boat with two people and a larger white boat with six people. Input Output Figure 2: An instance of the word grounding task. Models are tasked to predict the missing word boat and localize the corresponding smaller yellow boat in the image coherently. consider a formulation that directly examines if visionlanguage models have the ability to acquire grounded meanings of words, specifically, through both language modeling andobject localization . Figure 2 shows an instance of the word acquisition task. A model is presented with an image ximg∈ Iand an incomplete caption xcap∈ T with one of its groundable words w(e.g., nouns and adjectives) replaced by a MASK . The model is tasked to predict this missing word w∈ V based on all available context and localize the corresponding objects Ow={o1, o2,···, on}in the image by proposing the bounding boxes of them. Overall, a model capable of solving the grounded word acquisition task is a function f:I × T → V × R4n. The language modeling part takes the form of a cloze test, which predicts an open vocabulary word and is widely adopted to evaluate pretrained language models (Paperno et al., 2016; Petroni et al., 2019; Jin et al., 2020). However, language modeling alone fails to provide a comprehensive evaluation of language grounding. For example in Figure 2, a model may correctly produce the word “boat ,” but mistakenly attributes the evidence to the larger white boat in the image. To address this limitation, we require models to localize the corresponding object in the image. This design is motivated by the disentanglement of object detection into object localization and class recognition (Singh et al., 2018; Zareian et al., 2021; Zhong et al., 2022). It enables vision models to develop a sense of objectness without relying on a predefined set of object classes, thereby potentially allowing them to generalize to unseen objects. Further comparison with related task setups is discussed in Section 5 and illustrated in Figure 8 in the Appendix. 2.2 Evaluation Metric In language model evaluation, the commonly used measures for assessing performance are the stan525dard hitrate-atk(HR@k) measure and perplexity (Salazar et al., 2020; Jin et al., 2020). In masked language modeling, the log perplexity of a word w is defined as the log pseudoperplexity: logPPL(w) =−logP(w|ximg, xcap) (1) In object detection evaluation, especially for phrase grounding where multiple referents are possible (Kamath et al., 2021), AnyProtocol and AllProtocol are commonly adopted. Assuming nground truth bounding boxes B= {b1, b2,···, bn}andmpredicted bounding boxes /tildewideB={/tildewideb1,/tildewideb2,···,/tildewiderbm}, the intersectionover-union (IoU) in both protocols is defined as: IoUany=1 n/summationdisplay i∈{1,2,···,n}max j∈{1,2,···,m}IoU(bi,/tildewidebj)(2) IoUall=IoU(∪B,∪/tildewideB) (3) However, these metrics only capture unimodal performance without concerning the correctness of crossmodal mapping. We design two new metrics to combine language and vision performance: •Grounded hitrate (GHR @k), the proportion of tests with the masked word appearing in the topkcandidates and a localization IoU over 0.5. •Grounded perplexity (GPPL) as follows: logGPPL (w) =/braceleftigg ∞ if IoU = 0 logPPL(w)−logIoU else (4) 2.3 FewShot Learning of New Words Although there are grounding datasets available, i.e., imagetext pairs with wordobject mapping annotation (Plummer et al., 2015), it is impractical to obtain such finegrained annotation on a large scale and to cover the whole vocabulary space V. We therefore explore grounded new word learning as a fewshot learning problem, especially under the setting of incremental class learning (Mandziuk and Shastri, 1999; Kemker et al., 2018). An intuitive illustration of the fewshot new word learning framework is provided in Figure 3. Under this framework, a computational model is developed in two stages. During the pretraining stage, the model receives imagecaption pairs, with finegrained wordobject annotation for a set of base words Vseen⊆ V. After pretraining, the model is provided with few samples of raw textimage pairs, each containing a set of unseen words Vunseen⊆ V that the model has to acquire. Someone is slicing a loaf of bread using a knife on a wooden cutting board .I am slicing the pizza with a knife and stacking the pieces onto the plate. Fewshot Learning VunseenPretraining Vseen testtestFigure 3: An illustration of the fewshot new word learning paradigm. The model first pretrains on a grounding dataset with a set of base words ( Vseen), and then attempts to acquire a set of unseen words ( Vunseen ) in a small number of raw textimage pairs. Tests are performed after each training session. Tests are performed after each training stage. It’s important to note that the unseen words may not be completely new, e.g., the models may have encountered these words in its language encoder initialized with pretrained language models. We consider them “unseen” because the model never sees these words paired with their referent, i.e., the grounded meanings of the words are unknown. 2.4 Dataset Curation We build our dataset based on the Flickr30K Entities dataset (Plummer et al., 2015), which contains imagetext pairs with dense annotations between groundable phrases and bounding boxes of objects. The groundable phrases and regions are defined by the dataset, as chunks of text that refer to object bounding boxes. To construct word grounding instances, we use Stanza (Qi et al., 2020) to parse the caption, enumerate every word in the groundable phrase, and identify those with a POS tag of NOUN orADJ. These groundable words are replaced by MASK one at a time and matched to their corresponding bounding boxes. The dataset is divided into 4 splits: pretraining set, unseen words training set, seen words test set, and unseen words test set. We start by selecting 31 unseen words and holding out all textimage pairs containing these words from the training split of Flickr30K Entities. The holdout textimage pairs are further divided into the training and test sets for unseen words. The remaining training split of Flickr30K Entities is used for the pretraining set. To prevent frequent words ( e.g., “man”) from dominating the test results of the seen words, we choose 60 seen words and sample an equal number of test instances for each word from the test split of Flickr30K Entities. More details and statistics of the dataset are available in Appendix A.526RoBERTa Linear ResNet Linear 2D Positional Embedding ~~~~~~ ⊕Concat Multimodal Encoder Object Queries MLM Grounding OLTwo boats of people, a smaller <mask> boat with two people and a <mask> white boat with six people. Two boats of people, a smaller yellow boat with two people and a larger white boat with six people. ØQL QOKO VOText Decoder Object Decoder KL VLFigure 4: An overview of the W2Warchitecture, a visually grounded language model pretrained with three objectives: masked language modeling (MLM), object localization (OL), and grounding through wordregion alignment (WRA). 3 Computational Models 3.1 The Worldto-Words ( W2W) Model Humans demonstrate fast mapping, the ability to learn new words with only minimal information (Carey and Bartlett, 1978; Carey, 1978; Golinkoff et al., 2000). Motivated by how visual grounding helps humans in bootstrapping new words, we propose a computational framework that first acquires the ability to ground during pretraining, and then transfers this intrinsic ability to learn unseen words when grounded supervision is no longer available. We introduce Worldto-Words (W2W), a novel visuallygrounded language model with an endto-end design as illustrated in Figure 4. Model Architecture. Similarly to dualstream visionlanguage models, W2Wencodes the textual input with a pretrained language model (Liu et al., 2019), and encodes image input with convolutional backbone (He et al., 2016) with 2D positional encoding added. The text and image representations are linearly projected onto a joint semantic space and concatenated. The multimodal representation is then forwarded into a crossencoder with selfattention layers. The crossencoded representations in the final layer are sent into an object decoder, together with a set of learnable object queries. The object decoder produces an object embedding for each input object query, which can be considered as a representation of the proposed object. The object representations are further forwarded to the text decoder, which allows language modeling to explicitly attend to the perceived objects. We discuss the pretraining objectives, especially how the model acquires grounding in the following paragraphs. Other details are available in Appendix B. Masked Language Modeling (MLM). As an intrinsic task, we follow the majority of existing pretrained visionlanguage models to perform maskedlanguage modeling with a twolayer MLP. Words in input text are randomly masked out, and the model predicts the masked words conditioned on the corrupted sentence and image. Words in groundable phrases are masked with a probability of 0.4 and those in nongroundable regions are masked with a lower probability of 0.1. Object Localization (OL). Each object representation will be decoded by a shared threelayer MLP to produce a bounding box. We follow prior detection transformers (DETR) (Carion et al., 2020; Kamath et al., 2021) to perform bipartite matching between proposed boxes and ground truth boxes with a Hungarian loss (Kuhn, 1955). The predicted boxes are optimized towards ground truth using the generalized intersectionover-union (GIoU) loss (Rezatofighi et al., 2019) and the L1 loss. Grounding. The notion of Grounding is realized by grounded pretraining through wordregion alignment (WRA) which enables finegrained crossmodal mapping between words and objects. It consists of two levels of alignment: positional alignment andsemantic alignment . In positional alignment, the model learns to map each object representation to words in the sentence, which could possibly be a MASK or an additional noobject label ∅(Yu and Siskind, 2013; Kamath et al., 2021). We use a fullyconnected layer to predict the distribution over token positions with crossentropy loss. In semantic alignment, the model learns to bring word representations closer to the object representations that they ground to, and push the unrelated pairs farther. We use a contrastive loss over the final layers of the object and text decoders. 3.2 Baselines Groundless Baseline. A baseline with no grounding ability is developed by pretraining W2W in the same condition but removing the grounding527ModelsSeen (|Vseen|= 60 ) Unseen ( |Vunseen|= 31 ) GHR@1 ( ↑) log GPPL ( ↓) HR@1 ( ↑) log PPL ( ↓) Acc ( ↑) IoU ( ↑) GHR@1 ( ↑) log GPPL ( ↓) HR@1 ( ↑) log PPL ( ↓) Acc ( ↑) IoU ( ↑) RoBERTa - - 38.0 2.75 - - - - 23.1 4.96 - - RoBERTa (FT) - - 47.9 1.99 - - - - 24.3 4.38 - - ViLT - - 64.7 1.27 - - - - 32.7 3.68 - - MDETR - - - - 27.8 / 27.0 25.3 / 28.0 - - - - 26.3 / 20.2 23.9 / 21.7 ViLT+MDETR 19.8 / 19.3 2.53 / 2.43 64.7 1.27 31.1 / 30.4 28.5 / 31.2 8.6 / 8.1 5.07 / 5.12 32.7 3.68 27.3 / 23.3 25.0 / 23.8 VisualBERT (FT) 28.5 / - 2.96 / - 42.3 2.33 68.1 / - 53.3 / - 10.2 / - 5.60 / - 20.7 4.81 50.6 / - 45.2 / - W2W w/o G (FT) 28.9 / 27.8 2.33 / 2.38 63.9 1.41 44.0 / 43.0 40.0 / 38.2 1.1 / 1.1 11.89 / 12.04 3.7 10.87 38.7 / 31.9 36.2 / 31.0 W2W 47.0 /46.3 1.79 /1.81 66.9 1.26 66.8 /66.3 58.8 /57.6 2.3 / 2.3 11.58 / 11.74 4.2 11.01 61.3 /53.1 56.3 /48.0 Table 1: Test results on the seen and unseen words, obtained immediately after pretraining. Unless noted explicitly as finetuned (FT), all results reflect the performance of models without finetuning. Evaluations under both All and Anyprotocols are provided in the table as (All/Any) pairs. For models depending on a frozen pretrained object detector, we can only provide evaluation under AllProtocol. We note that the unseen words are only unseen to W2W models, as pretrained baselines have encountered them all during development. We report the results for reference. objectives in the loss function. We refer to this groundless model as W2W w/o G. Like a typical pretrained VLM, e.g., VisualBERT (Li et al., 2019), W2W w/o G performs language modeling based on the object features, without explicit crossmodal referential grounding. We apply W2W w/o G onGOVA task by finetuning the model on the pretraining dataset with grounding objective until convergence. Pretrained Baselines. For the majority of the pretrained VLMs, the unseen words are known during pretraining. Also, the primary focus of this work is to understand grounding and bootstrapping in grounded word acquisition. It’s not our goal to scale up or retrain all variants of pretraining frameworks. Therefore, we compare our model to the pretrained VLMs with equal or reasonably larger scales for only reference and analysis, we demonstrate that W2Wis a more coherent and fast grounded word learner, and that the grounding ability acquired during pretraining helps the model to learn unseen words more rapidly and robustly.1 1 experiments we generate 500 intervention pairs for each template, and results are averaged over three seeds. 4.3 Models to Evaluate We use our framework to assess the robustness of reasoning in thirteen pretrained language models. We consider five sizes of the GPT2 model (Radford et al., 2019): distilled (Sanh et al., 2019), small, medium, large, and XL. We evaluate four models from EleutherAI that were pretrained on the Pile (Gao et al., 2020): GPTNeo 1.3B and 2.7B (Black et al., 2021), GPTJ-6B (Wang and Komatsuzaki, 2021), and GPTNeoX-20B (Black et al., 2022). We use HuggingFace Transformers (Wolf et al., 2019) to access the models. Additionally, we experiment with a set of instructiontuned versions of GPT3 (Brown et al., 2020): Instruct (Ouyang et al., 2022), Curie, Davinci002, and Davinci003.3Experiments with GPT3 are carried out under the constraints set by the OpenAI APIs4, which prevent us from computing the causal effect using the same procedure as for the other models. We report the details about how the metrics were computed 3The OpenAI ids for these models are, respectively, davinciinstruct-beta ,textcurie-001 , textdavinci-002 , andtextdavinci-003 . 4https://openai.com/api/J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Curie*NeoX 3Davinci-002*3Instruct* 3Davinci-003*100101102103104105δrccDCE rccofN TCE rccofN J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Davinci-0023CurieNeoX 3Instruct 3Davinci-0030.20.40.60.81δcpDCE cpofN TCE cpofN Figure 3: Comparison of DCE( N→R)and TCE( NonR).∗approx values, see Appendix C. for GPT3 in Appendix C. In the reported results, we indicate with an asterisk (∗) the metrics that were influenced by this limitation. 5 Results Our analyses focus primarily on twooperand problems (Sections 5.1 and 5.2) and later extend to more complex problems that involve three operands (Section 5.5) for the models that perform best on the twooperand test bed. We compare the direct causal effect DCE and the total causal effect TCE ofN andTonR.DCE represents the undesired effect for a model to being mistakenly responsive to a change in NorTnot leading to a change in the result g(low robustness), whereas higher values ofTCE indicate a higher ability of the model to correctly adjust the probability weight assigned to the new solution g′after the intervention (high sensitivity). 5.1 Effect of NonR From the results in Figure 3, we notice that larger models exhibit a larger TCE rcc/DCE rccratio. In particular, in GPTJ-6B and NeoX, the TCE is, respectively, 30x and 1000x larger than the DCE. However, this improvement in sensitivity is not manifested in terms of change of prediction ( δcp), for which the models show to be affected by resultpreserving changes almost as equally as by resultaltering interventions. This behavior changes sig5500 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200 0 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200 0 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200Figure 4: Heatmaps displaying P(g)for DistilGPT-2 (left), GPTJ-6B (center), and GPT3 Davinci002 (right). g is the groundtruth result g=n1+n2(n1andn2are represented by the x and y axes, respectively. The probability values for each combination of ((n1, n2), g)are averaged over 20 different templates. Probability values over 0.2 are displayed with the darkest color. J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Curie*NeoX 3Davinci-002*3Instruct* 3Davinci-003*100101102103104105δrccDCE rccofS TCE rccofT J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Davinci-0023CurieNeoX 3Instruct 3Davinci-0030.20.40.60.81δcpDCE cpofS TCE cpofT Figure 5: Comparison of DCE( S→R)and TCE( TonR). We use∗to denote approximated values, explained in Appendix C. nificantly in instructiontuned models. In particular, for the 175Bparameter GPT3, performance varies depending on the type of supervision, with the PPOtrained Davinci003 exhibiting an 84% difference between direct and total effect. In Figure 4, we present a different visualization of the direct causal effect of Non the model’s prediction. We report the heatmaps showing the probability assigned by the model to the result g of a problem (t,(n1, n2), g)|g=n1+n2,∀g∈ {0,1, . . . , 50},∀(n1, n2)∈ {0,1, . . . , 50}2. For DistilGPT-2 we observe low overall probability assigned to gand diagonal patterns indicating consistency in assigning higher probability to specific results (e.g., 10, 20, 30, 40, 50). For the two larger models we notice a higher probability mass assigned to the problem’s result, but less consistency on the prediction of the same result with different sets of operands (this is true for GPTJ in particular). This result is consistent with the observed higher DCE and TCE in larger models: P(g)might vary more considerably when intervening on Nwithout affecting g, but overall the model assigns higher probability weight to the correct result, which correlates with higher sensitivity. 5.2 Effect of TonR In Figure 5, we report the total causal effect of the textual framing Tand the direct causal effect of the irrelevant text elements Son the model’s prediction. For the instructiontuned models, the improvement in terms of prediction change ( δcp) follows a similar trend as for N, with GPT3 Davinci003 showing a 76% difference between direct and total effect. An interesting observation is that the irrelevant textual information Sappears to have a lower direct effect than Nfor all noninstruction-tuned models. However, in the GPT3 Davinci00x models, we observe the opposite (i.e., DCE(N→R)≤DCE(S→R)). This suggests that large instructionbased models tend to be more susceptible to variation in the textual framing of a problem, while smaller models are more responsive to changes in the numerical values (though not necessarily correctly). 5.3 Overall Insights In comparison to other models, GPT3 Davinci shows the highest DCE rcc, but low DCE cp. This discrepancy is related to the quantities that the two metrics consider. δrcctakes into account the probability assigned to g, while δcpdoes not consider the ground truth solution. One interpretation of this result is that GPT3 Davinci consistently predicts the same answer r=r′when g=g′, but the probabilities P(g)andP′(g)might vary significantly.551The results observed for the two kinds of intervention do(T:t→t′)anddo(N: (n1, n2)→ (n′ 1, n′ 2))show similar trends. Small models (Distilled and Small GPT2) exhibit low sensitivity to interventions. Larger models (from GPT2 Medium to GPTNeo) appear to be more influenced by changes in both NandT. However, they display similar sensitivity to both resultaltering and resultpreserving interventions. An improvement in sensitivity is noticeable in GPTJ and NeoX, though not accompanied by an improvement in robustness. Remarkably different behavior is instead shown by the GPT3 Davinci models, which demonstrate substantially higher sensitivity to resultaltering interventions (high TCE), and higher robustness (in terms of prediction change). In Appendix B.2, we report the accuracy of the models on the generated instances of MWPs, which exhibits a similar trend as the robustness/sensitivity changes we observed. Possible explanations for the improved robustness and sensitivity demonstrated by the large GPT3 models might be the dramatic size increase and extension/enhancement of the training procedure involving instructions. The former idea is aligned with the emergent abilities hypothesis (Wei et al., 2022a), which postulates the existence of skills that are displayed by largescale models but are not present in smallerscale models. However, our observations show different performances in versions of GPT3 Davinci that differ in the training procedure.5This raises the question of whether the capability of LLMs to reason about math problems benefits from instructionbased tuning. We address this question in the following section. 5.4 Extending to LLaMABased Models To further investigate the roles played by size and training Experimental Setup In this section, we describe the data used to perform the interventions and to measure the causal effects. 4.1 Datasets For our analyses, we use instances of math word problems from three popular datasets: ASDivA (Miao et al., 2020), MAWPS (KoncelKedziorski et al., 2016), and SV AMP (Patel et al., 2021). The examples contained in these collections are pairs(t,o)consisting of a question template twith its annotated operations o. Each of these pairs can be instantiated multiple times into problems q= (t,n)by filling the template with numerical values (n1, n2, . . .)and computing the groundtruth result g=fo(n)(most problems involve two to three operands, i.e., |n| ∈ { 2,3}). We select a set of 437 twooperand and 307 threeoperand templateexpression pairs that we use to generate pairs of prompts representing an intervention. More details about the prompt generation procedure are in Appendix A. We use (t,n)to refer to an instantiated template that we use as a prompt. 4.2 Intervention Data Given an MWP q= (t,n)and its solution g, we generate a second problemsolution instance (q′, g′)depending on the type of causal effect CE we want to measure and on the considered variable. When intervening on the operands of the problem, the text of the problem is kept unaltered and a set of new operands nis sampled in such a way that the result gis affected or not depending on the effect that is being measured. When changing the textual description of the problem, we change tsuch that either o′=o, oro′̸=o. In the former case, we sample a different template t′= (s′,o)from the549set of templates describing the same operations o, in the latter case we sample a new t′describing a different operation. In Appendix B.1 we report some examples of (q,q′)pairs representing the different types of interventions. Given a model, we use the question pair (q,q′) to obtain a pair of answer distributions P(R|t,n) andP(R|t′,n′), which we use to measure the causal effect of the intervention. We consider the space for the numerical values to be I= {1,2, . . . , C }consisting of integer values, following the setup of several existing MWP datasets (Miao et al., 2020; KoncelKedziorski et al., 2016; Patel et al., 2021). To control our experimental costs and make sure the models keep the number as one token, we set C= 300 . From all the tokens in a model’s vocabulary, we focus on the probability assigned to the numbers in our numerical spaceI, and thus we use P(R=r)to denote the normalized probability Praw(R=r)/Z, where Z=/summationtextC r=1Praw(R=r), andPraw(x)is the raw probability score assigned to the vocabulary token x. For each intervention type, we generate a dataset Dconsisting of (q,q′)pairs. Unless otherwise specified, for our experiments we generate 500 intervention pairs for each template, and results are averaged over three seeds. 4.3 Models to Evaluate We use our framework to assess the robustness of reasoning in thirteen pretrained language models. We consider five sizes of the GPT2 model (Radford et al., 2019): distilled (Sanh et al., 2019), small, medium, large, and XL. We evaluate four models from EleutherAI that were pretrained on the Pile (Gao et al., 2020): GPTNeo 1.3B and 2.7B (Black et al., 2021), GPTJ-6B (Wang and Komatsuzaki, 2021), and GPTNeoX-20B (Black et al., 2022). We use HuggingFace Transformers (Wolf et al., 2019) to access the models. Additionally, we experiment with a set of instructiontuned versions of GPT3 (Brown et al., 2020): Instruct (Ouyang et al., 2022), Curie, Davinci002, and Davinci003.3Experiments with GPT3 are carried out under the constraints set by the OpenAI APIs4, which prevent us from computing the causal effect using the same procedure as for the other models. We report the details about how the metrics were computed 3The OpenAI ids for these models are, respectively, davinciinstruct-beta ,textcurie-001 , textdavinci-002 , andtextdavinci-003 . 4https://openai.com/api/J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Curie*NeoX 3Davinci-002*3Instruct* 3Davinci-003*100101102103104105δrccDCE rccofN TCE rccofN J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Davinci-0023CurieNeoX 3Instruct 3Davinci-0030.20.40.60.81δcpDCE cpofN TCE cpofN Figure 3: Comparison of DCE( N→R)and TCE( NonR).∗approx values, see Appendix C. for GPT3 in Appendix C. In the reported results, we indicate with an asterisk (∗) the metrics that were influenced by this limitation. 5 Results Our analyses focus primarily on twooperand problems (Sections 5.1 and 5.2) and later extend to more complex problems that involve three operands (Section 5.5) for the models that perform best on the twooperand test bed. We compare the direct causal effect DCE and the total causal effect TCE ofN andTonR.DCE represents the undesired effect for a model to being mistakenly responsive to a change in NorTnot leading to a change in the result g(low robustness), whereas higher values ofTCE indicate a higher ability of the model to correctly adjust the probability weight assigned to the new solution g′after the intervention (high sensitivity). 5.1 Effect of NonR From the results in Figure 3, we notice that larger models exhibit a larger TCE rcc/DCE rccratio. In particular, in GPTJ-6B and NeoX, the TCE is, respectively, 30x and 1000x larger than the DCE. However, this improvement in sensitivity is not manifested in terms of change of prediction ( δcp), for which the models show to be affected by resultpreserving changes almost as equally as by resultaltering interventions. This behavior changes sig5500 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200 0 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200 0 510 15 20 25 30 35 40 45 5005101520253035404550 0.0000.0250.0500.0750.1000.1250.1500.1750.200Figure 4: Heatmaps displaying P(g)for DistilGPT-2 (left), GPTJ-6B (center), and GPT3 Davinci002 (right). g is the groundtruth result g=n1+n2(n1andn2are represented by the x and y axes, respectively. The probability values for each combination of ((n1, n2), g)are averaged over 20 different templates. Probability values over 0.2 are displayed with the darkest color. J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Curie*NeoX 3Davinci-002*3Instruct* 3Davinci-003*100101102103104105δrccDCE rccofS TCE rccofT J6B Neo1.3B Neo2.7BDistilledSmall Large MediumXL 3Davinci-0023CurieNeoX 3Instruct 3Davinci-0030.20.40.60.81δcpDCE cpofS TCE cpofT Figure 5: Comparison of DCE( S→R)and TCE( TonR). We use∗to denote approximated values, explained in Appendix C. nificantly in instructiontuned models. In particular, for the 175Bparameter GPT3, performance varies depending on the type of supervision, with the PPOtrained Davinci003 exhibiting an 84% difference between direct and total effect. In Figure 4, we present a different visualization of the direct causal effect of Non the model’s prediction. We report the heatmaps showing the probability assigned by the model to the result g of a problem (t,(n1, n2), g)|g=n1+n2,∀g∈ {0,1, . . . , 50},∀(n1, n2)∈ {0,1, . . . , 50}2. For DistilGPT-2 we observe low overall probability assigned to gand diagonal patterns indicating consistency in assigning higher probability to specific results (e.g., 10, 20, 30, 40, 50). For the two larger models we notice a higher probability mass assigned to the problem’s result, but less consistency on the prediction of the same result with different sets of operands (this is true for GPTJ in particular). This result is consistent with the observed higher DCE and TCE in larger models: P(g)might vary more considerably when intervening on Nwithout affecting g, but overall the model assigns higher probability weight to the correct result, which correlates with higher sensitivity. 5.2 Effect of TonR In Figure 5, we report the total causal effect of the textual framing Tand the direct causal effect of the irrelevant text elements Son the model’s prediction. For the instructiontuned models, the improvement in terms of prediction change ( δcp) follows a similar trend as for N, with GPT3 Davinci003 showing a 76% difference between direct and total effect. An interesting observation is that the irrelevant textual information Sappears to have a lower direct effect than Nfor all noninstruction-tuned models. However, in the GPT3 Davinci00x models, we observe the opposite (i.e., DCE(N→R)≤DCE(S→R)). This suggests that large instructionbased models tend to be more susceptible to variation in the textual framing of a problem, while smaller models are more responsive to changes in the numerical values (though not necessarily correctly). 5.3 Overall Insights In comparison to other models, GPT3 Davinci shows the highest DCE rcc, but low DCE cp. This discrepancy is related to the quantities that the two metrics consider. δrcctakes into account the probability assigned to g, while δcpdoes not consider the ground truth solution. One interpretation of this result is that GPT3 Davinci consistently predicts the same answer r=r′when g=g′, but the probabilities P(g)andP′(g)might vary significantly.551The results observed for the two kinds of intervention do(T:t→t′)anddo(N: (n1, n2)→ (n′ 1, n′ 2))show similar trends. Small models (Distilled and Small GPT2) exhibit low sensitivity to interventions. Larger models (from GPT2 Medium to GPTNeo) appear to be more influenced by changes in both NandT. However, they display similar sensitivity to both resultaltering and resultpreserving interventions. An improvement in sensitivity is noticeable in GPTJ and NeoX, though not accompanied by an improvement in robustness. Remarkably different behavior is instead shown by the GPT3 Davinci models, which demonstrate substantially higher sensitivity to resultaltering interventions (high TCE), and higher robustness (in terms of prediction change). In Appendix B.2, we report the accuracy of the models on the generated instances of MWPs, which exhibits a similar trend as the robustness/sensitivity changes we observed. Possible explanations for the improved robustness and sensitivity demonstrated by the large GPT3 models might be the dramatic size increase and extension/enhancement of the training procedure involving instructions. The former idea is aligned with the emergent abilities hypothesis (Wei et al., 2022a), which postulates the existence of skills that are displayed by largescale models but are not present in smallerscale models. However, our observations show different performances in versions of GPT3 Davinci that differ in the training procedure.5This raises the question of whether the capability of LLMs to reason about math problems benefits from instructionbased tuning. We address this question in the following section. 5.4 Extending to LLaMABased Models To further investigate the roles played by size and training evaluation to consider the threeoperand problems in the dataset. In these experiments, we consider only the GPT3 175Bparameter models, as they are the only models performing well on the simpler bivariate problems. The results regarding the effects of Nare reported in Figure 7. We notice that the large difference between the desired (TCE) and undesired (DCE) effects observed on simpler problems shrinks significantly for both metrics. In particular, for Davinci003, the direct effect of N(measured as δcp) grows from 0.17 to 0.87. That is, GPT3 Davinci003 predicts a different result 87% of the time after an intervention that does not affect the groundtruth solution. The increase in direct effect indicates a performance degradation in terms of brittleness: even the models that show good performance on twooperand problems, now display an unstable behavior after resultpreserving interventions. 6 Related Work Causal NLP. Causal inference aims to study the cause and effect from observational and interventional data (Pearl, 2009; Peters et al., 2017). Traditionally, researchers usually apply causal techniques to phenomena in nature and human society. With the rise of powerful models in NLP, recent research has started to explore the intersection of causal inference and NLP, forming the study of Causal NLP (Jin et al., 2022; Feder et al., 2021a). There are several formulations for Causal NLP: thecausality for NLP thread involves using the causal framework for data collection and task formulation (Jin et al., 2021c), inspecting the (pathspecific) causal effect of certain neurons on predictions (Vig et al., 2020; Meng et al., 2022), understanding the causal effect of data and learning paradigm for model performance (Ni et al., 2022), and as a way to frame prompts (Lyu et al., 2023); andNLP for causality involves testing the pure causal inference skills of LLMs (Jin et al., 2023a,b), and use text as a variable for causal effect estimation (Roberts et al., 2020; Veitch et al., 2020; Jin et al., 2021b, 2023c). The most similar line of research to our work is the application of causal effect estimation on interpreting models’ behavior, such as how models understand syntactic agreement (Finlayson et al., 2021), and how interventions in the representations and weights affect the model prediction (Feder et al., 2021b). To the best of our knowledge, our work is the first to formulate a causal framework for robustness behavioral tests, and also we are the first to introduce the idea to quantify the differences in the causal mechanisms of human reasoning and model decisions. Math Reasoning in NLP. A growing body of work tries to improve the math reasoning capability in NLP models (Zhang et al., 2020; Geva et al., 2020; Spokoyny et al., 2021), and prompting techniques for LLMs (Cobbe et al., 2021; Shen et al., 2021; Kojima et al., 2022; Wei et al., 2022b; Chowdhery et al., 2022). For analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.1 1 Introduction Many natural language understanding situations, such as understanding the financial news, require reasoning with text that includes numbers. However, such mathematical reasoning is challenging for NLP models (Cobbe et al., 2021; Mishra et al., 2022b). Mathematical reasoning for text has been an active area of research for a while (Seo et al., 2015; Sachan and Xing, 2017; Sachan et al., 2017, 2018, inter alia ), and has also emerged as a key task to track the capabilities of large language models (LLMs) in recent years (Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022a, inter alia ). However, despite the impressive performance of LLMs on various math reasoning benchmarks (e.g., ∗Equal contribution. 1Our code and data are available at https://github. com/alestolfo/causalmath . Kyle could fit n1=26 drawings on each page. If he has n2=11 pages, the number of drawings he can make is ___.Kyle could fit n1=2 drawings on each page. If he has n2=143 pages, the number of drawings he can make is ___.Prediction LLMs Original text:Example dointerv ention by our framework :Keep gr oundtruth g, but change n1, n2After dointerv ention Pred = 286 = g P(286)=0.085Pred = 143 (incorrect) P(286)=0.001 Original pr edictionDistribution of the Predicted Numerical Answer P(R) Figure 1: Through our framework, we conduct dointerventions on the input and evaluate the change in the distribution P(R)of the prediction Rby LLMs, in this figure, GPTJ. This allows us to measure the causal effect of each factor in the input on the model’s response. Ouyang et al., 2022; Chowdhery et al., 2022), it remains unclear whether these models have learned mere artifacts in the data or have truly mastered the mathematical concepts needed to consistently solve all variations of the same problem (Patel et al., 2021; Razeghi et al., 2022; Welleck et al., 2022). In sharp contrast with a large number of papers on improving the performance of LLMs on various types of mathbased problems, there has been little effort on behavioral analysis of LLMs for these tasks. Existing methods for understanding the robustness of these models (Patel et al., 2021) rely on manually constructing variations of math problems, and we do not yet have a principled, comprehensive framework for quantifying such robustness. Thus, in this work, we propose a formal framework based on causal inference, to quantify the robustness of NLP models’ math reasoning abilities. Specifically, we describe a causal graph formulation of math reasoning, where the graph allows us to measure the difference in the structural causal545Math Word Problem QOperands N=(N1,N2,…)NonOperand Parts TOperations OIrrelevant Surface Form SCorrect Calculation G:=fO(N)Model’s Prediction RDCE()N→RDCE()S→RTCE()N on R TCE()T on R: Causal Graph of Human/Groundtruth ReasoningGhDCE()O→RIntervention 🔧 Intervention 🔧 Red Arrows: Potential Spurious CorrelationsBlue Arrow: Desired EﬀectDCE()G→RFigure 2: Causal graph of model predictions on math questions. We highlight the difference between a cognitivelyinspired correct reasoning path ( Gh) and the undesired effects that some factors might have on the model’s prediction (red arrows). By performing controlled interventions of the numerical values ( N) and on the textual framing of the problem ( T,S), we are able to quantify the causal effects of each factor. models of human reasoning and model judgment. We consider various causal factors such as the textual framing of the question, numerical operands, and operation types. Then, we identify a set of interventions in the context of math word problems (an example of which is illustrated in Figure 1), and provide a causal inference framework to obtain causal effects of each factor via direct dointerventions (Pearl, 1995) and causal mediation analysis (Pearl, 2001). While our approach is reminiscent of recent studies using causal analysis for LLMs (Finlayson et al., 2021; Vig et al., 2020; Meng et al., 2022), in this work, we provide a new theoretical analysis framework specifically suitable for math reasoning. Using our framework, we disentangle factors affecting the model’s predictions and measure their influences. This way, we are able to provide insights into the model’s reasoning in terms of robustness andsensitivity with respect to changes in these factors. We apply our framework to study a set of thirteen GPT models with various sizes and training procedures (i.e., instructiontuned and noninstruction- tuned). We observe that, among noninstruction- tuned language models, the larger ones tend to be more sensitive to changes in the groundtruth result of a math word problem, but not necessarily more robust. However, we observe a different behavior in the instructiontuned GPT3 models (Ouyang et al., 2022), which show a remarkable improvement in both sensitivity and robustness, although the robustness reduces when problems get more complicated. We additionally investigate the role of size and instruction tuning on the model’s performance with three models of the LLaMA family(Touvron et al., 2023) and Stanford Alpaca (Taori et al., 2023). 2 Problem Setup We consider a dataset Dof math word problems (MWPs), where each MWP is denoted as a questionQ.Qis a list (T,N)consisting of a question template Tand an ordered list of operands N= (N1, N2, . . . , N m). Each question template T:= (O, S)further contains two types of information: a set of arithmetic operations Oimplicitly expressed in the question, and the text surface form S irrelevant to the arithmetic operations. Oincorporates the information relative to the operations as a collection of tuples {(O1, i1, j1),(O2, i2, j2), . . .}, where Ok∈ {+,−,×,÷}(k∈N) and ik, jk∈N represent the indices of the operands to which operator Okshould be applied to.2The groundtruth result G=fO(N)is calculated by computing the function fO, which represents the application of all the operators in Oto the respective operands. We illustrate the factors in Qand their interdependency in the causal graph in Figure 2. A twooperand instance qofQin this form from Patel et al. (2021) is: Template t: Mark has n1trees in his backyard. If he plants n2more, how many trees will he have? Operands n:(n1= 12, n2= 13) Operations o: {(“+”, 1, 2)} Result :g=fo(n) =n1+n2= 25 2The intermediate result of operation Olis indicated by ik=m+l.546Our goal is to quantify the robustness of a model Mon the set of problems q∈ D . Ideally, D should be a dataset not seen by the model during training. We assume that a model takes qas input and predicts a probability distribution of the result R:P(R|t,n). Our formulation below will be easier to understand using this finite discrete set and can be generalized to any kind of data pairing a natural language template with a function that maps a set of operands to a result (e.g., a Python program; Mishra et al. 2022a). 3 A Causal Framework In this section, we describe our framework in three steps. First, we define the idea of model robustness on MWPs. Then, we identify possible dointerventions (Pearl, 1995) that we can perform. Finally, we describe the causal effects that we measure to quantify the robustness of various models. 3.1 Step 1. Question Reformulation We address the research question “ Is a model reasoning robustly on MWPs? ” by comparing the causal mechanisms of the model’s decisions to a hypothesized human reasoning mechanism. Note that we do not claim to know how humans reason about these problems. We simply propose a reasonable and intuitive way to judge model robustness given a reasonable and intuitive human reasoning mechanism inspired by findings regarding the independence of language and mathematical reasoning in humans (Brannon, 2005; Monti et al., 2012). Human Reasoning Mechanisms. The causal mechanisms of how humans might solve qinclude o=fabstract (q), (1) g=fo(n), (2) where they first experiments on two open dialogue datasets. The PersonaChat dataset (Zhang et al., 2018) is a large personaconditioned chitchat style dialogue dataset which consists of 10,907 training dialogues, 1,000 validation dialogues, and 968 testing dialogues. The DailyDialog dataset (Li et al., 2017) is another widelyused large collection of humanhuman dialogues, consisting of a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. 4.2 Baselines We choose the following two kinds of evaluation metrics as baseline methods:ReferenceBased Metrics. For the referencebased metrics, we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), EmbeddingAverage (Wieting et al., 2016), VectorExtrema (Forgues and Pineau, 2014), GreedyMatching (Rus and Lintean, 2012), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020), which have been widely used in generative dialogue systems. Referencefree Metrics. For the referencefree metrics, we compare with three learningbased methods, namely, RUBER (Tao et al., 2018), MAUDE (Sinha et al., 2020) and MDDEval (Zhang et al., 2022a). Note that we were not able to compare with EMS (Chan et al., 2021), as their code is unavailable. It is also infeasible to reimplement their approach due to the lack of sufficient implementation details in the paper. 4.3 Evaluation Set Construction We follow the setting in Optimus (Li et al., 2020a) to use BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) as the encoder and the decoder for our CMN framework, respectively. We set the dimension of the latent variable zof CV AE to 32. In the evaluation phase, we follow Zhao et al. (2020) to generate response candidates based on the testset of DailyDialog and PersonaChat using several widely applied dialogue generation systems, including Seq2Seq (Sutskever et al., 2014), HRED (Serban et al., 2016), and GPT2 (Radford et al., 2019). After obtaining the generated response candidates, we construct an evaluation set consisting of astandard set , in which the sample references and generated responses are similar in semantics (i.e., for the standard evaluation setting), and a diverse set , in which the references and responses are distant in semantics (i.e., for the oneto-many setting). For the standard set, we collect 200 samples from both DailyDialog and PersonaChat that have the highest BLEU1 scores between the reference and response among all the testing pairs. As our primary focus is to evaluate the model’s performance under the oneto-many setting, we constructed a diverse set containing a larger number of samples (i.e., 600), by sampling from the testing pairs whose BLEU1 scores are lower than 0.2. These sampled data have a balanced split between DailyDialog and PersonaChat.566(a) (b) (c) (d) Figure 2: TSNE visualisation of the sentence representation of references and generated responses. (a) and (b) for thestandard set , and (c) and (d) for the diverse set . 4.4 Human Annotation Evaluating the system performance requires measuring the correlation between the model prediction versus human evaluation scores. We recruited three human annotators to evaluate the evaluation set (i.e., the contextresponse pairs in our standard and diverse sets). All annotators hold at least a master’s degree in Computer Science and have full professional proficiency in English. Specifically, annotators were asked to rate two aspects: Appropriateness , which measures the degree to which the output is appropriate within the given context, and Coherence , which assesses the extent to which the content of the output is presented in a wellstructured, logical, and meaningful manner. These ratings were provided on a 15 Likert scale, with higher scores indicating better quality. For each contextresponse pair, we then average the Appropriateness and Coherence scores across all annotators to produce the final human annotation score. In the diverse set, 400 responses are rated as positive samples (45), while 200 are rated as negative samples (13). In contrast, all responses in the standard set are rated as positive samples since each response is semantically similar to the gold reference. We examine the InnerAnnotator Agreement (IAA) using interannotator Kappa (Cohen, 1960). The average IAA score between every pair of annotators for the Personachat dataset is 0.55, indicating a moderately strong level of agreement (0.40.6 range). On the other hand, the average IAA score for the DailyDialog dataset is 0.65, demonstrating a substantially strong level of agreement (0.60.8 range). More details of the IAA scores can be found in Appendices A.2. 5 Results In this section, we evaluate our model’s performance on evaluating opendomain dialogues under both standard and diverse settings. 5.1 Analysis of the Evaluation Set Before presenting the evaluation results, we first provide some validation analysis on our standard anddiverse sets using embeddingbased semantic similarity BERTScore. For the standard set, the averaged BERTScore is 4.7 for DailyDialog and 2.56 for Personachat. However, the scores are only 0.23 (DailyDialog) and 0.27 (Personachat) for the diverse set, indicating that the semantic similarity between the response candidates and the goldstandard references is low. We further use TSNE to visualise the sentence representations of the reference and generated response pairs. As shown in Figure 2 (a) and (b), the response candidates are similar to the references in the standard set, where the corresponding data567DailyDialog PersonaChat Metrics Pearson’s ρ Spearman’s τ Pearson’s ρ Spearman’s τ BLEU1 0.0465 (0.6782) 0.0049 (0.9652) -0.0314 (0.8183) -0.0372 (0.7856) BLEU2 0.0497 (0.6577) 0.0116 (0.9175) -0.0601 (0.6597) -0.0621 (0.6495) BLEU3 0.0462 (0.6803) 0.0399 (0.7219) -0.0431 (0.7525) -0.0213 (0.8760) BLEU4 0.0796 (0.4770) 0.0646 (0.5641) -0.0149 (0.9134) -0.064 (0.6395) ROUGE1 0.0718 (0.5213) 0.0304 (0.7861) -0.0267 (0.8449) 0.0678 (0.6193) ROUGE2 0.0841 (0.4525) 0.0645 (0.5651) 0.0305 (0.8235) 0.0291 (0.8315) ROUGEL 0.0490 (0.6617) 0.0285 (0.7992) -0.0013 (0.9924) 0.0834 (0.5413) METEOR 0.0696 (0.5345) 0.0946 (0.3977) -0.102 (0.4543) -0.1066 (0.4344) Embedding Extrema 0.1211 (0.2784) -0.0021 (0.9853) 0.0017 (0.9903) 0.0814 (0.5510) Greedy 0.1117 (0.3176) 0.0940 (0.4008) 0.0949 (0.4866) 0.0891 (0.5136) Average 0.1527 (0.1709) 0.1199 (0.2835) 0.1018 (0.4554) 0.1124 (0.4096) BERTScore 0.0824 (0.4620) -0.0076 (0.9457) 0.1097 (0.4211) 0.1724 (0.2038) BLEURT 0.1163 (0.2983) 0.0940 (0.4008) -0.1194 (0.3808) -0.1143 (0.4016) RUBER 0.0820 (0.4642) 0.1560 (0.1616) 0.0019 (0.9887) -0.0329 (0.8095) MAUDE -0.1623 (0.1453) -0.0145 (0.8974) 0.1353 (0.3201) 0.1104 (0.4178) MDDEval 0.1029 (0.3574) -0.0667 (0.5516) 0.1239 (0.3630) 0.2502 (0.0629) Ours(w/o NSP) 0.2292 (0.0383) 0.2025 (0.0681) 0.2585 (0.0544) 0.3816 (0.0037) Ours(w/o MI) 0.0833 (0.4568) -0.0537 (0.6316) 0.1030 (0.4498) 0.1530 (0.2601) Ours 0.2446 (0.0268) 0.2211 (0.0459) 0.2656 (0.0479) 0.3971 (0.0024) Table 1: Pearson and Spearman correlations with human judgements on the standard set. Figures in parentheses are pvalues. points are either very close to each other or overlapping (e.g., there are seemingly more orange points in 2 (a) due to overlapping). In contrast, the distributions of response candidates and references are more distinctive for the diverse set, as shown in Figure 2 (c) and (d). In summary, the analysis shows that the standard and diverse sets are a good fit for our evaluation purposes. 5.2 Model evaluation in the standard setting We compare our model with the baselines in terms of how well the evaluation scores generated by the model correlate with human judgments. As shown in Table 1, the ngram baselines, including BLEU, ROUGE, and METEOR, achieve negative or weak positive correlations with human annotations on both datasets. The embeddingbased approaches (including the ones using pretrained models such as BERTScore) slightly outperform the ngram baselines, except that BLEURT performs worse on the PersonaChat. In contrast, learningbased metrics give the strongest performance among all baselines. Specifically, MAUDE and MDDEval achieve similar performance on the PersonaChat, and both outperform RUBER. However, RUBER gives better performance than these two metrics on DailyDialog. Our model achievesthe best overall performance in terms of both Pearson and Spearman correlations on both datasets. We further conducted ablation studies to evaluate the effectiveness of the MI (w/o NSP) and the NSP (w/o MI) components by excluding the other component when inferring the final evaluation score. It can be observed that CMN with the MI component alone (i.e., w/o NSP) gives better performance than the model variant with the NSP component only. This suggests that MI is more effective than NSP in evaluating dialogues when the response candidates are similar to the references in semantics (i.e. the standard setting). 5.3 Model evaluation in the oneto-many setting In another set of experiments, we evaluate our model performance in the oneto-many setting using the diverse set. As shown in Table 2, Extrema, Greedy, and Average achieve a negative or weakly positive correlation with human annotation on both datasets. In contrast, the embeddingbased metrics which use pretrained models to represent sentences achieve much better results. For instance, both BERTScore and BLEURT achieve close to 0.25 for both Pearson and Spearman correlations on DailyDialog, although the performance is less strong on Per568DailyDialog PersonaChat Metrics Pearson’s ρ Spearman’s τ Pearson’s ρ Spearman’s τ BLEU1 0.2953 (<0.0001) 0.2635 (<0.0001) -0.1533 (0.0361) -0.1702 (0.0199) BLEU2 0.2733 (<0.0001) 0.2638 (<0.0001) -0.1657 (0.0235) -0.1810 (0.0132) BLEU3 0.2496 (<0.0001) 0.2691 (<0.0001) -0.1654 (0.0237) -0.1846 (0.0114) BLEU4 0.2319 (<0.0001) 0.2737 (<0.0001) -0.1642 (0.0247) -0.1790 (0.0142) ROUGE1 0.3275 (<0.0001) 0.2865 (<0.0001) -0.0057 (0.9382) 0.0489 (0.5062) ROUGE2 0.2698 (<0.0001) 0.2761 (<0.0001) -0.0340 (0.6441) 0.0937 (0.2023) ROUGEL 0.3362 (<0.0001) 0.2945 (<0.0001) -0.0072 (0.9222) 0.0476 (0.5178) METEOR 0.2948 (<0.0001) 0.2858 (<0.0001) -0.0293 (0.6908) -0.0507 (0.4904) Embedding Extrema -0.3589 (<0.0001) -0.3524 (<0.0001) -0.1010 (0.1690) -0.0390 (0.5966) Greedy -0.1580 (0.0006) -0.1408 (0.0023) -0.0380 (0.6052) 0.0113 (0.8776) Average -0.1350 (0.0034) -0.1006 (0.0296) -0.1093 (0.1364) -0.0355 (0.6294) BERTScore 0.2591 (<0.0001) 0.2251 (<0.0001) 0.0345 (0.6391) 0.0853 (0.2455) BLEURT 0.2711 (<0.0001)) 0.2063 (<0.0001)) 0.1267 (0.0840) 0.1858 (0.0109) RUBER 0.1027 (0.0263) 0.1714 (0.0002) -0.0579 (0.4312) -0.0592 (0.4206) MAUDE 0.0551 (0.2344) 0.1782 (<0.0001) 0.2640 (0.0003) 0.3267 (<0.0001) MDDEval 0.5567 (<0.0001) 0.6160 (<0.0001) 0.1264 (0.0848) 0.2582 (0.0004) Ours(w/o NSP) 0.5453 (<0.0001) 0.5555 (<0.0001) 0.2947 (0.0025) 0.2224 (0.0022) Ours(w/o MI) 0.6183 (<0.0001) 0.5946 (<0.0001) 0.2769 (0.0001) 0.1390 (0.0578) Ours 0.6325 (<0.0001) 0.6234 (<0.0001) 0.4000 (<0.0001) 0.2746 (0.0001) Table 2: Pearson and Spearman correlations with human judgements on the diverse set. sonaChat. On the other hand, the word overlap metrics based on ngram perform better than the above embeddingbased metrics, with BLEU, ROUGE, and METEOR all having higher correlations than the embeddingbased approaches. Nevertheless, the correlations of these metrics to human annotations are still relatively weak for both datasets. For learningbased metrics, RUBER and MAUDE give weak positive correlations with human annotations on the DailyDialog dataset. However, RUBER gives a negative correlation with human scores on the PersonaChat. MAUDE, on the other hand, performs the best on the PersonaChat dataset in terms of Spearman correlation (0.3267), which is higher than that of our Experimental Setup 4.1 Datasets To evaluate the effectiveness of our proposed automatic evaluation metric, we conduct experiments on two open dialogue datasets. The PersonaChat dataset (Zhang et al., 2018) is a large personaconditioned chitchat style dialogue dataset which consists of 10,907 training dialogues, 1,000 validation dialogues, and 968 testing dialogues. The DailyDialog dataset (Li et al., 2017) is another widelyused large collection of humanhuman dialogues, consisting of a training set with 11,118 dialogues and validation and test sets with 1000 dialogues each. 4.2 Baselines We choose the following two kinds of evaluation metrics as baseline methods:ReferenceBased Metrics. For the referencebased metrics, we use BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), EmbeddingAverage (Wieting et al., 2016), VectorExtrema (Forgues and Pineau, 2014), GreedyMatching (Rus and Lintean, 2012), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020), which have been widely used in generative dialogue systems. Referencefree Metrics. For the referencefree metrics, we compare with three learningbased methods, namely, RUBER (Tao et al., 2018), MAUDE (Sinha et al., 2020) and MDDEval (Zhang et al., 2022a). Note that we were not able to compare with EMS (Chan et al., 2021), as their code is unavailable. It is also infeasible to reimplement their approach due to the lack of sufficient implementation details in the paper. 4.3 Evaluation Set Construction We follow the setting in Optimus (Li et al., 2020a) to use BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019) as the encoder and the decoder for our CMN framework, respectively. We set the dimension of the latent variable zof CV AE to 32. In the evaluation phase, we follow Zhao et al. (2020) to generate response candidates based on the testset of DailyDialog and PersonaChat using several widely applied dialogue generation systems, including Seq2Seq (Sutskever et al., 2014), HRED (Serban et al., 2016), and GPT2 (Radford et al., 2019). After obtaining the generated response candidates, we construct an evaluation set consisting of astandard set , in which the sample references and generated responses are similar in semantics (i.e., for the standard evaluation setting), and a diverse set , in which the references and responses are distant in semantics (i.e., for the oneto-many setting). For the standard set, we collect 200 samples from both DailyDialog and PersonaChat that have the highest BLEU1 scores between the reference and response among all the testing pairs. As our primary focus is to evaluate the model’s performance under the oneto-many setting, we constructed a diverse set containing a larger number of samples (i.e., 600), by sampling from the testing pairs whose BLEU1 scores are lower than 0.2. These sampled data have a balanced split between DailyDialog and PersonaChat.566(a) (b) (c) (d) Figure 2: TSNE visualisation of the sentence representation of references and generated responses. (a) and (b) for thestandard set , and (c) and (d) for the diverse set . 4.4 Human Annotation Evaluating the system performance requires measuring the correlation between the model prediction versus human evaluation scores. We recruited three human annotators to evaluate the evaluation set (i.e., the contextresponse pairs in our standard and diverse sets). All annotators hold at least a master’s degree in Computer Science and have full professional proficiency in English. Specifically, annotators were asked to rate two aspects: Appropriateness , which measures the degree to which the output is appropriate within the given context, and Coherence , which assesses the extent to which the content of the output is presented in a wellstructured, logical, and meaningful manner. These ratings were provided on a 15 Likert scale, with higher scores indicating better quality. For each contextresponse pair, we then average the Appropriateness and Coherence scores across all annotators to produce the final human annotation score. In the diverse set, 400 responses are rated as positive samples (45), while 200 are rated as negative samples (13). In contrast, all responses in the standard set are rated as positive samples since each response is semantically similar to the gold reference. We examine the InnerAnnotator Agreement (IAA) using interannotator Kappa (Cohen, 1960). The average IAA score between every pair of annotators for the Personachat dataset is 0.55, indicating a moderately strong level of agreement (0.40.6 range). On the other hand, the average IAA score for the DailyDialog dataset is 0.65, demonstrating a substantially strong level of agreement (0.60.8 range). More details of the IAA scores can be found in Appendices A.2. 5 Results In this section, we evaluate our model’s performance on evaluating opendomain dialogues under both standard and diverse settings. 5.1 Analysis of the Evaluation Set Before presenting the evaluation results, we first provide some validation analysis on our standard anddiverse sets using embeddingbased semantic similarity BERTScore. For the standard set, the averaged BERTScore is 4.7 for DailyDialog and 2.56 for Personachat. However, the scores are only 0.23 (DailyDialog) and 0.27 (Personachat) for the diverse set, indicating that the semantic similarity between the response candidates and the goldstandard references is low. We further use TSNE to visualise the sentence representations of the reference and generated response pairs. As shown in Figure 2 (a) and (b), the response candidates are similar to the references in the standard set, where the corresponding data567DailyDialog PersonaChat Metrics Pearson’s ρ Spearman’s τ Pearson’s ρ Spearman’s τ BLEU1 0.0465 (0.6782) 0.0049 (0.9652) -0.0314 (0.8183) -0.0372 (0.7856) BLEU2 0.0497 (0.6577) 0.0116 (0.9175) -0.0601 (0.6597) -0.0621 (0.6495) BLEU3 0.0462 (0.6803) 0.0399 (0.7219) -0.0431 (0.7525) -0.0213 (0.8760) BLEU4 0.0796 (0.4770) 0.0646 (0.5641) -0.0149 (0.9134) -0.064 (0.6395) ROUGE1 0.0718 (0.5213) 0.0304 (0.7861) -0.0267 (0.8449) 0.0678 (0.6193) ROUGE2 0.0841 (0.4525) 0.0645 (0.5651) 0.0305 (0.8235) 0.0291 (0.8315) ROUGEL 0.0490 (0.6617) 0.0285 (0.7992) -0.0013 (0.9924) 0.0834 (0.5413) METEOR 0.0696 (0.5345) 0.0946 (0.3977) -0.102 (0.4543) -0.1066 (0.4344) Embedding Extrema 0.1211 (0.2784) -0.0021 (0.9853) 0.0017 (0.9903) 0.0814 (0.5510) Greedy 0.1117 (0.3176) 0.0940 (0.4008) 0.0949 (0.4866) 0.0891 (0.5136) Average 0.1527 (0.1709) 0.1199 (0.2835) 0.1018 (0.4554) 0.1124 (0.4096) BERTScore 0.0824 (0.4620) -0.0076 (0.9457) 0.1097 (0.4211) 0.1724 (0.2038) BLEURT 0.1163 (0.2983) 0.0940 (0.4008) -0.1194 (0.3808) -0.1143 (0.4016) RUBER 0.0820 (0.4642) 0.1560 (0.1616) 0.0019 (0.9887) -0.0329 (0.8095) MAUDE -0.1623 (0.1453) -0.0145 (0.8974) 0.1353 (0.3201) 0.1104 (0.4178) MDDEval 0.1029 (0.3574) -0.0667 (0.5516) 0.1239 (0.3630) 0.2502 (0.0629) Ours(w/o NSP) 0.2292 (0.0383) 0.2025 (0.0681) 0.2585 (0.0544) 0.3816 (0.0037) Ours(w/o MI) 0.0833 (0.4568) -0.0537 (0.6316) 0.1030 (0.4498) 0.1530 (0.2601) Ours 0.2446 (0.0268) 0.2211 (0.0459) 0.2656 (0.0479) 0.3971 (0.0024) Table 1: Pearson and Spearman correlations with human judgements on the standard set. Figures in parentheses are pvalues. points are either very close to each other or overlapping (e.g., there are seemingly more orange points in 2 (a) due to overlapping). In contrast, the distributions of response candidates and references are more distinctive for the diverse set, as shown in Figure 2 (c) and (d). In summary, the analysis shows that the standard and diverse sets are a good fit for our evaluation purposes. 5.2 Model evaluation in the standard setting We compare our model with the baselines in terms of how well the evaluation scores generated by the model correlate with human judgments. As shown in Table 1, the ngram baselines, including BLEU, ROUGE, and METEOR, achieve negative or weak positive correlations with human annotations on both datasets. The embeddingbased approaches (including the ones using pretrained models such as BERTScore) slightly outperform the ngram baselines, except that BLEURT performs worse on the PersonaChat. In contrast, learningbased metrics give the strongest performance among all baselines. Specifically, MAUDE and MDDEval achieve similar performance on the PersonaChat, and both outperform RUBER. However, RUBER gives better performance than these two metrics on DailyDialog. Our model achievesthe best overall performance in terms of both Pearson and Spearman correlations on both datasets. We further conducted ablation studies to evaluate the effectiveness of the MI (w/o NSP) and the NSP (w/o MI) components by excluding the other component when inferring the final evaluation score. It can be observed that CMN with the MI component alone (i.e., w/o NSP) gives better performance than the model variant with the NSP component only. This suggests that MI is more effective than NSP in evaluating dialogues when the response candidates are similar to the references in semantics (i.e. the standard setting). 5.3 Model evaluation in the oneto-many setting In another set of experiments, we evaluate our model performance in the oneto-many setting using the diverse set. As shown in Table 2, Extrema, Greedy, and Average achieve a negative or weakly positive correlation with human annotation on both datasets. In contrast, the embeddingbased metrics which use pretrained models to represent sentences achieve much better results. For instance, both BERTScore and BLEURT achieve close to 0.25 for both Pearson and Spearman correlations on DailyDialog, although the performance is less strong on Per568DailyDialog PersonaChat Metrics Pearson’s ρ Spearman’s τ Pearson’s ρ Spearman’s τ BLEU1 0.2953 (<0.0001) 0.2635 (<0.0001) -0.1533 (0.0361) -0.1702 (0.0199) BLEU2 0.2733 (<0.0001) 0.2638 (<0.0001) -0.1657 (0.0235) -0.1810 (0.0132) BLEU3 0.2496 (<0.0001) 0.2691 (<0.0001) -0.1654 (0.0237) -0.1846 (0.0114) BLEU4 0.2319 (<0.0001) 0.2737 (<0.0001) -0.1642 (0.0247) -0.1790 (0.0142) ROUGE1 0.3275 (<0.0001) 0.2865 (<0.0001) -0.0057 (0.9382) 0.0489 (0.5062) ROUGE2 0.2698 (<0.0001) 0.2761 (<0.0001) -0.0340 (0.6441) 0.0937 (0.2023) ROUGEL 0.3362 (<0.0001) 0.2945 (<0.0001) -0.0072 (0.9222) 0.0476 (0.5178) METEOR 0.2948 (<0.0001) 0.2858 (<0.0001) -0.0293 (0.6908) -0.0507 (0.4904) Embedding Extrema -0.3589 (<0.0001) -0.3524 (<0.0001) -0.1010 (0.1690) -0.0390 (0.5966) Greedy -0.1580 (0.0006) -0.1408 (0.0023) -0.0380 (0.6052) 0.0113 (0.8776) Average -0.1350 (0.0034) -0.1006 (0.0296) -0.1093 (0.1364) -0.0355 (0.6294) BERTScore 0.2591 (<0.0001) 0.2251 (<0.0001) 0.0345 (0.6391) 0.0853 (0.2455) BLEURT 0.2711 (<0.0001)) 0.2063 (<0.0001)) 0.1267 (0.0840) 0.1858 (0.0109) RUBER 0.1027 (0.0263) 0.1714 (0.0002) -0.0579 (0.4312) -0.0592 (0.4206) MAUDE 0.0551 (0.2344) 0.1782 (<0.0001) 0.2640 (0.0003) 0.3267 (<0.0001) MDDEval 0.5567 (<0.0001) 0.6160 (<0.0001) 0.1264 (0.0848) 0.2582 (0.0004) Ours(w/o NSP) 0.5453 (<0.0001) 0.5555 (<0.0001) 0.2947 (0.0025) 0.2224 (0.0022) Ours(w/o MI) 0.6183 (<0.0001) 0.5946 (<0.0001) 0.2769 (0.0001) 0.1390 (0.0578) Ours 0.6325 (<0.0001) 0.6234 (<0.0001) 0.4000 (<0.0001) 0.2746 (0.0001) Table 2: Pearson and Spearman correlations with human judgements on the diverse set. sonaChat. On the other hand, the word overlap metrics based on ngram perform better than the above embeddingbased metrics, with BLEU, ROUGE, and METEOR all having higher correlations than the embeddingbased approaches. Nevertheless, the correlations of these metrics to human annotations are still relatively weak for both datasets. For learningbased metrics, RUBER and MAUDE give weak positive correlations with human annotations on the DailyDialog dataset. However, RUBER gives a negative correlation with human scores on the PersonaChat. MAUDE, on the other hand, performs the best on the PersonaChat dataset in terms of Spearman correlation (0.3267), which is higher than that of our evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learningbased automatic evaluation metric ( CMN ), which can robustly evaluate opendomain dialogues by augmenting Conditional Variational Autoencoders ( CV AEs) with a Next Sentence Prediction ( NSP) objective and employing Mutual Information ( MI) to model the semantic similarity of text in the latent space. Experimental results on two opendomain dialogue datasets demonstrate the superiority of our method compared with a wide range of baselines, especially in handling responses which are distant to the golden reference responses in semantics. 1 analysis of the effectiveness of our proposed experiments. Corpus of Linguistic Acceptability ( COLA ) (Warstadt et al., 2019) is publication texts with annotations on whether the text is grammatically correct or not. 4.1.2 Generation   analysis. The analysis showed that label accuracy, diversity, and similarity are positively correlated with model accuracy, with significance (coef=.4797 and p<0.001 for label accuracy, coef=.2260 and p<0.001 for diversity, and coef=0.1980 and p<0.005 for similarity).579Regarding specific patterns, logit suppression increased diversity while hurting the label accuracy and the similarity to the oracle dataset. High temperature increased diversity and decreased label accuracy, but to a smaller degree than logit suppression. The application of each diversification experiments at various sparsity levels show SMP has significant improvements over firstorder and zeroorder methods.Unlike previous firstorder methods, SMP is also applicable to low sparsity and outperforms zeroorder methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require finetuning. Our code is available at https://github.com/kongds/SMP . 1 Introduction Pretrained Language Models (PLMs) like BERT (Devlin et al., 2019) have shown powerful performance in natural language processing by transferring the knowledge from largescale corpus to downstream tasks. These models also require largescale parameters to cope with the largescale corpus in pretraining. However, these largescale parameters are overwhelming for most downstream tasks (Chen et al., 2020), which †Corresponding Author.results in significant overhead for transferring and storing them. To compress PLM, pruning is widely used by removing unimportant weights and setting them to zeros. By using sparse subnetworks instead of the original complete network, existing pruning methods can maintain the original accuracy by removing most weights. Magnitude pruning (Han et al., 2015) as a common method uses zerothorder information to make pruning decisions based on the absolute value of weights. However, in the process of adapting to downstream tasks, the weight values in PLMs are already predetermined from the original values. To overcome this shortcoming, movement pruning (Sanh et al., 2020) uses firstorder information to select weights based on how they change in training rather than their absolute value. To adapt PLMs for downstream tasks, most methods like movement pruning perform pruning and finetuning together by gradually increasing the sparsity during training. With the development of the Lottery Ticket Hypothesis (LTH) (Frankle and Carbin, 2018) in PLMs, some methods (Chen et al., 2020; Liang et al., 2021) find certain subnetworks from the PLM by pruning, and then finetune these subnetworks from pretrained weights. Moreover, if the finetuned subnetwok can match the performance of the full PLM, this subnetwork is called winning ticket (Chen et al., 2020). In this work, we propose a simple but efficient firstorder method. Contrary to the previous pruning method, our method adapts PLMs by only pruning, without finetuning. It makes pruning decisions based on the movement trend of weights, rather than actual movement in movement pruning. To improve the performance of our method, we propose a new masking function to better align the remaining weights according to the architecture of PLMs. We also avoid finetuning weights in the taskspecific head by using our head initialization method. By keeping the PLM frozen, we can save594half of the trainable parameters compared to other firstorder methods, and only introduce a binary mask as the new parameter for each downstream task at various sparsity levels. Extensive experiments on a wide variety of sparsity demonstrate our methods strongly outperform stateof-theart pruning methods. Contrary to previous firstorder methods (Sanh et al., 2020), which show poor performance at low sparsity, our method is also applied to low sparsity and achieves better performances than zeroorder methods. 2 Related Work Compressing PLMs for transfer learning is a popular area of research. Many compression methods are proposed to solve overparameterized problem in PLMs, such as model pruning (Han et al., 2015; Molchanov et al., 2017; Xia et al., 2022), knowledge distillation (Jiao et al., 2020; Wang et al., 2020), quantization (Shen et al., 2020; Qin et al., 2022), and matrix decomposition (Lan et al., 2020). Among them, pruning methods have been widely studied as the most intuitive approach. Pruning methods focus on identifying and removing unimportant weights from the model. Zeroorder methods and firstorder methods are widely used to prune PLMs. For zeroorder methods, magnitude pruning (Han et al., 2015) simply prunes based on absolute value of their weights. For firstorder methods, which are based on firstorder Taylor expansion to make pruning decision, L0 regularization (Louizos et al., 2017) adds the L0 norm regularization to decrease remaining weights by sampling them with hardconcrete distribution. Movement pruning (Sanh et al., 2020) uses straightthrough estimator (Bengio et al., 2013) to calculate firstorder informantion. Based on pruning methods, Frankle and Carbin (2018) proposes Lottery Ticket Hypothesis (LTH). LTH clarifies the existence of sparse subnetworks (i.e., winning tickets) that can achieve almost the same performance as the full model when trained individually. With the development of LTH, lots of works that focus on the PLMs have emerged. Chen et al. (2020) find that BERT contains winning tickets with a sparsity of 40% to 90%, and the winning ticket in the mask language modeling task can be transferred to other downstream tasks. Recent works also try to leverage LTH to improve the performance and efficiency of PLM. Liang et al. (2021) find generalization performanceof the winning tickets first improves and then deteriorates after a certain threshold. By leveraging this phenomenon, they show LTH can successfully improve the performance of downstream tasks. 3   Analysis 6.1 Masking Function In this section, we discuss the influence of different masking functions. Table 3 shows the results of different masking functions on our experiments, we assessed that this model only does sentencelevel translation, but included it due to its widespread usage; (2) the DeepL v2 API. This model advertises its usage of context as part of translations and our experiments confirm this. Early experimentation with other providers (Amazon and Azure) indicated that these are not contextaware so we refrained from evaluating them. To obtain provider translations, we feed the documents into an API request. To resegment the translation into sentences, we include special marker tokens in the source that are preserved during translation and split the translation on those tokens. We also evaluate a sentencelevel version of DeepL where we feed each sentence separately to compare with its documentlevel counterpart. 6translate.google.com, deepl.com. Translations were obtain from version of engines available in April 2021611* * * * * ** * ** * * *** *∅ ∅ ∅ ∅∅ ∅∅∅ ∅ ∅ ∅∅ ∅∅Figure 2: Impact of context on BLEU, COMET, and Word fmeasure per tag for base contextaware models. BLEU, COMET and word fmeasures statistically significantly higher than nocontext ( p< 0.05) are marked with *. Languages for which the phenomenon doesn’t exist are marked with ∅. BLEU scores are normalized between [0,1] * * * * * * *** ** *** * ∅ ∅ ∅ ∅ Figure 3: Impact of context on BLEU, COMET, and Word fmeasure per tag for large models. Values that are statistically significantly higher than nocontext ( p < 0.05) are marked with *. Languages for which the phenomenon doesn’t exist are marked with ∅. BLEU scores are normalized between [0,1] 5.3 Results and Discussion Figure 2 shows results for base models, trained either without ( nocontext ) or with context, and for the latter with either predicted (context ) or reference context ( contextgold ) during decoding. Results are reported with respect to standard MT metrics BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020), as well as the MuDA benchmark. The corpuslevel metrics BLEU and COMET are calculated over the entire corpus, rather than just the sentences tagged by MuDA. First, we find that BLEU scores are highest for contextgold models for most language pairs, but contextagnostic models have higher COMET scores. Moreover, in terms of mean word fmeasure overall, we do not find significant differences between the three systems. It is therefore difficult to see which system performs the best on documentlevel ambiguities using only corpuslevel metrics. For words tagged by MuDA as requiring context for translation, contextaware models often achieve significantly higher word fmeasure than contextagnostic models on certain tags such as ellipsis andformality , but not on other tags such as lexicalandverb form . This demonstrates how MuDA allows us to clarify which intersentential ambiguities contextaware models are able to resolve. For the pretrained large models (Figure 3), contextaware models perform better than the contextagnostic on corpuslevel metrics, especially COMET. On words tagged with MuDA, contextaware models generally obtain the high612∅ ∅ ∅ ∅ ∅∅ ∅ ∅∅∅ ∅∅∅* * ** ** ** * ** * * ** * * ** * * ∅Figure 4: Scores for commercial models. DeepL (doc) BLEU, COMET and word fmeasures statistically significantly higher than DeepL (sent) are marked with *. Languages for which neither DeepL or Google translations are available are marked with ∅. BLEU scores are normalized between [0,1] est fmeasure as well, particularly when given reference context, especially on phenomena such as lexical andpronouns , but improvements are less pronounced than on corpuslevel evaluation. Among commercial engines (Figure 4), DeepL outperforms Google on most metrics and language pairs. The sentencelevel ablation of DeepL performs worse than its documentlevel system for most MuDA tags. Current contextaware MT systems translate some intersentential discourse phenomena well, but are unable to consistently obtain significant improvements over contextagnostic counterparts on challenging MuDA data. Tables with all results can be found in Appendix H. 6 Related Work Several works have worked on measuring the performance of MT models on contextual discourse phenomena. The first example of this was done by Hardmeier et al. (2010), which evaluated automatically the precision and recall of pronoun translation in statistical MT systems. Jwalapuram et al. (2019) proposed evaluating models on pronoun translation based on a pairwise comparison between translations that were generated with and without context, and later Jwalapuram et al. (2020)extended this work to include more languages and phenomena in their automatic evaluation/test set creation. These works rely on prior domain knowledge and intuition to identify contextaware phenomena, whereas we take a systematic, datadriven approach. Most works have focused on evaluating performance in discourse phenomena through the use ofcontrastive datasets . Müller et al. (2018) automatically create a dataset for anaphoric pronoun resolution to evaluate MT models in EN→DE. Bawden et al. (2018) manually creates a dataset for both pronoun resolution and lexical choice in EN→FR. V oita et al. (2018, 2019b) creates a dataset for anaphora resolution, deixis, ellipsis and lexical cohesion in EN→RU. However, Yin et al. (2021) suggest that translating anddisambiguating between two contrastive choices are inherently different, motivating our approach in measuring direct translation performance. 7 Conclusions and Future Work We investigate types of ambiguous translations where MT models benefit from context using our proposed PCXMI metric. We perform a datadriven thematic analysis across 14 languages to identify contextsensitive discourse phenomena,613some of which (such as verb forms ) have not been previously addressed in work on MT. In comparison to previous work, our approach is systematic, extensible, and does not require prior knowledge of the language. Additionally, the PCXMI metric can be used to identify other contextdependent words in generation. We construct the MuDA benchmark that tags words in parallel corpora and evaluates models on 5 contextdependent phenomena. Our evaluation reveals that contextaware and commercial translation systems achieve small improvements over contextagnostic models on our benchmark, and we encourage further development of models that improve on contextaware translation. Limitations While MuDA relies on set of handcrafted rules for tagging specific phenomena, these rules might involve the use of other errorprone systems (such as coreference resolution and alignment models) and these errors might be susceptible to problems (such as lack of outof-domain generalization) that could limit the applicability of our tagger. However, this could be fixed by extending MuDA to use newer and better versions of these systems. The use of F1 per tag with surfaceform matching between reference/translation can also lead to penalizing translations that use context correctly but choose other equivalent words. Nevertheless, this should also be mitigable by extending the scoring  evaluation, however not in a fully systematic way. In this paper, we develop theMultilingual DiscourseAware ( MUDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common contextaware MT models make only marginal improvements over contextagnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena.1 1 analysis of words with high PCXMI to find categories of trans606lations where context is useful (§3). We identify novel discourse phenomena that to our knowledge have not been addressed previously (e.g. consistency of verb forms), without requiring apriori languagespecific knowledge. Finally, we design a series of methods to automatically tag words belonging to the identified classes of ambiguities (§4) and we evaluate existing translation models for different categories of ambiguous translations (§5). We examine a parallel corpus spanning 14 language pairs, measuring translation ambiguity and model performance. We find that the contextaware methods, while improving on standard evaluation metrics, only perform significantly better than contextagnostic baselines for certain discourse phenomena in our benchmark. Our benchmark provides a more finegrained evaluation of translation models and reveals weaknesses of contextaware models, such as verb form cohesion. We also find that DeepL, a commercial documentlevel translation system, does better in our benchmark than its sentencelevel ablation and Google Translate. We hope that the released benchmark and code, as well as our findings, will spur targeted evaluation of discourse phenomena in MT to cover more languages and more phenomena in the future. 2 Measuring Context Usage 2.1 CrossMutual Information Past work on contrastive evaluation has examined correct and incorrect translations of specific discourse phenomena (Bawden et al., 2018; Müller et al., 2018), but this provides only a limited measure of context usage on phenomena defined by the creators of the dataset. We are therefore interested in devising a metric that is able to capture all context usage by a model, beyond a predefined set. Conditional CrossMutual Information (CXMI) (Bugliarello et al., 2020; Fernandes et al., 2021) measures the influence of context on model predictions at the corpus level. CXMI is defined as: CXMI (C→Y|X) = HqMTA(Y|X)−HqMTC(Y|X, C ), where XandYare a source and target sentence, respectively, Cis the context, HqMTAis the entropy of acontextagnostic MT model, and HqMTCrefers to acontextaware MT model. This quantity can be estimated over a heldout set with Nsentencepairs and their respective context as: CXMI (C→Y|X)≈ −1 NN/summationdisplay i=1logqMTA(y(i)|x(i)) qMTC(y(i)|x(i), C(i)) Importantly, the authors find that training a singlemodel qMTas both the contextagnostic and contextaware model ensures that nonzero CXMI values are due to context and not other factors (see Fernandes et al. (2021) and §3.1 for details). Although this approach is promising, it is defined only at a corpus level : as the previous equation shows, CXMI is estimated by over a full set of sentences. Since we are interested in measuring how important context is for single sentences or words within a sentence, we extend this definition to capture lowerlevel context dependency in the next section. 2.2 Context Usage Per Sentence and Word Pointwise Mutual Information (PMI) (Church and Hanks, 1990) measures the association between two random variables for specific outcomes. Mutual information can be seen as the expected value of PMI over all possible outcomes of the variables. Taking inspiration from this, we define the Pointwise CrossMutual Information (PCXMI) for a source, target, context triplet (x, y, C )as: PCXMI (y, x, C ) =−logqMTA(y|x) qMTC(y|x, C) Intuitively, PCXMI measures how much more (or less) likely a target sentence yis when it is given context C, compared to not being given that context. Note that this is estimated according to the models qMTAandqMTCsince, just like CXMI, this measure depends on their learned distributions. We can also apply PCXMI at the word level to measure how much more likely a particular word in a sentence is when it is given the context, by leveraging the autoregressive property of the neural decoder. Given the triplet (x, y, C )and the word index i, we can measure the PCXMI for that particular word as: PCXMI (i, y, x, C ) =−logqMTA(yi|yt<i, x) qMTC(yi|yt<i, x, C ) Note that nothing constrains the form of Cor even xand PCXMI can, in principle, be applied to any conditional language modelling problem.607Avelile’s mother had HIV virus. Avelile had the virus, she was born with the virus.Lexical Cohesion阿维利尔的母亲是携有艾滋病病毒。阿维利尔也有艾滋病病毒。她一生下来就有。 Your daughter? Your niece? Formality Votre fille ? V otre nièce ? (TV) Roger. I got’em. TwoSix, this is TwoSix , we’re mobile. Formality 了解捕捉した。26こちら移動中だ。 (Honorifics) Our tools today don’t look like shovels and picks. They look like the stuff we walk around with.PronounsAs ferramentas de hoje não se parecem com pásepicaretas.Elas se parecem com as coisas que usamos. Louis XIV had a lot of people working for him. They made his silly outfits, like this.Verb FormLuis XIV tenía un montón de gente trabajando para él. Ellos hacían sus trajes tontos, como éste. They’re the ones who know what society is going to be like in another generation. Idon’t.EllipsisAncak onlar ba¸ ska bir nesilde toplumun nasıl olaca˘ gını biliyorlar.Benbilmiy orum. Table 2: Examples of high PCXMI tokens and corresponding linguistic phenomena. Contextual sentences are italicized . The high PCXMI target token is highlighted in pink, source and contextual target tokens related to the high PCXMI token are highlighted in blue and green respectively. We use this metric to find words that are strongly contextdependent, which is to say that their likelihood increases greatly with context relative to other words. These words are the ones that likely correspond to discourse phenomena. 3 Which Translation Phenomena Benefit from Context? To identify salient translation phenomena that require context, we perform a thematic analysis (Braun and Clarke, 2006), examining words with high PCXMI across different language pairs and manually identifying patterns and categorizing them into phenomena where context is useful for translation. To do so, we systematically examined (1) the mean PCXMI per partof-speech (POS) tag, (2) the words with the highest mean PCXMI across the corpus, and (3) the individual words with the highest PCXMI in a particular sentence. 3.1 Data & Model To compare linguistic phenomena that arise during documentlevel translation across language pairs, we use a dataset consisting of TED talks’ transcripts and translations (Qi et al., 2018). We use this dataset due to its abundance of discourse phenomena, as well as its availability across many parallel languages. We study translation between English and Arabic, German, Spanish, French, Hebrew, Italian, Japanese, Korean, Dutch, Portuguese, Romanian, Russian, Turkish and Mandarin Chinese. These 14 target languages are chosen for their high availability of TED talks and linguistic tools, as well as for the diversity of language types in our comparative study (Table 4 in Appendix B). For each language pair, our dataset contains 113,711parallel training sentences from 1,368 talks, 2,678 development sentences from 41 talks, and 3,385 testing sentences from 43 talks. To obtain the PCXMI for words in the data, we train a small Transformer (Vaswani et al., 2017) model for every target language and incorporate the target context by concatenating it with the current target sentence (Tiedemann and Scherrer, 2017). We train the model with dynamic context size (Fernandes et al., 2021), by sampling 03 target context sentences and estimating PCXMI by using this model for qMTAandqMTC(details in Appendix G). 3.2 Analysis Procedure We start our analysis by studying POS tags with high mean PCXMI. In Appendix C, we report the mean PCXMI for selected POS tags on test data. Some types of ambiguity, such as dual form pronouns (§3.3), can be linked to a single POS tag and be identified at this step, whereas others require finer inspection. Next, we inspect the vocabulary items with high mean PCXMI. At this step, we can detect phenomena that are reflected by certain lexical items that consistently benefit from context for translation. Finally, we examine individual tokens that obtain the highest PCXMI. In doing so, we identify patterns that do not depend on lexical features, but rather on syntactic constructions for example. In Table 2, we provide selected examples of tokens that have high PCXMI and the discourse phenomenon we have identified from them. 3.3 Identified Phenomena Through our thematic analysis of items with high PCXMI, we identified various types of translation ambiguity. Unlike previous work, our experiments on two realworld benchmark datasets demonstrate the effectiveness of our framework for improving multimodal fake news detection. 1 Introduction Fake news quietly sneaks into people’s daily life, mixed with massive information, causing serious impact and harm to society. Fake news often utilizes multimedia information such as text and images to mislead readers, spreading and expanding its influence. Thus, it is crucial and urgent to find a way to discern multimodal fake news. Today, most existing methods train on known fake news instances expecting to capture the labelspecific features for judging the authenticity of unseen news (Singhal et al., 2020; Wu et al., 2021; ∗Corresponding author Anx.Anger Neg.emoPos.emoSad InsightCause.Discrep.Tentat.CertainDiffer.FeelHearSee Swear NetspeakAssentNonflu. Category0.000.010.020.030.040.050.060.07Frequency Fake RealFigure 1: The word frequency distributions of psychological categories on different labels (Twitter dataset). Qian et al., 2021b; Qi et al., 2021). However, such labelspecific features may expose the models to hidden data bias when confronted with unseen fake news samples (Wang et al., 2018; Cheng et al., 2021; Zhu et al., 2022). To address the problem, we investigate the biases underlying the multimodal fake news detection data and identify the psycholinguistic bias in the text and the bias of inferring news label based on image features only (i.e. imageonly bias). These biases could lead to spurious correlations between the news and labels, thus impairing the model performance on testing data. To explicitly explain the biases, we first formulate the process of fake news detection as a causal graph as shown in Figure 2(a). In addition to the impact of fused features Con news label Ythat most multimodal fake news detection methods focus on, other two edges are pointing to Y, starting from text features T, and image features I, respectively. Generally speaking, the publishers of fake news would try their best to fabricate confusing text or use certain techniques to forge fake images. This makes the text and image can individually affect the news label. For the T→Ybranch, we observe that the linguistic characteristics of the text have obvious emotional preferences, such as the usage of psycholin627TICY(a) The proposed causal graph (Factual world). TICYU ❌(b) Causal graph with confounder. t*i*c*Ytic(c) Counterfactual world. Figure 2: The causal graphs for fake news detection. T: text features, I: image features, C: multimodal features (i.e., the fused features of image and text), Y: news label, U: confounder. *denotes the reference values. guistic words \"crazy\" and \"amazing\", which play a critical role in fake news detection. To deeply analyze the linguistic characteristics of the text, we present a mathematical analysis of the psycholinguistic word distribution of real news and fake news based on the LIWC 2015 dictionary (Pennebaker et al., 2015). Take the Twitter dataset as an example, as shown in Figure 1, we can observe that the word frequency distribution of fake news is quite different from that of real news, especially for words expressing anxiety, negative emotions, positive emotions, tentative, and netspeak. It seems that we can draw a conclusion that fake news prefers to use loaded language to stir up the reader’s emotions and attract more attention. Consequently, the model could be prone to relying on such psycholinguistic features as a shortcut to judge news authenticity. However, we analyze the training set and testing set, and find that there exist significant differences in the frequency of these psycholinguistic words. The manifest differences between the training set and testing set have proven that this shortcut appears to be unreliable evidence. As shown in Figure 2(b) where Udenotes the confounder (i.e. the psycholinguistic features in the text), there exist a backdoor path T←U→Ywhich will introduce spurious correlations among the text features and news label. In order to remove the psycholinguistic bias, we apply causal intervention by adopting the backdoor adjustment (Glymour et al., 2016) with docalculus P(Y|do(T))to calculate the causal effect in the training stage, which is fundamentally different from the conventional likelihood P(Y|T). For the I→Ybranch, we observe from the datasets that two different news pieces sharing the same image could have contrary labels. This shows that sometimes even if the image is real, the text could be fabricated, and the news could thus befake. We can take advantage of images as an additional modality to provide more detection evidence, but it is unreliable to infer the authenticity of the news based on the image features alone. In this case, we argue that the imageonly bias (i.e., the direct causal effect from image features alone to news label) should be eliminated. Towards this end, we use counterfactual reasoning by imagining a counterfactual world (Figure 2(c)) where both text features Tand fused features Care not given (represented by reference values t∗andc∗), except for image features I. In this way, the bias can be estimated by computing the direct causal effect ofIonYand we can conduct the debiasing by subtracting it from the total effect on Y. We instantiate our proposed debiasing framework on three strong baseline models that can handle both text and image features as inputs. Extensive experiments on two widely used realworld benchmark datasets show the effectiveness of our framework. Overall, our contributions can be summarized as follows: •We analyze each modality of fake news detection data and identify the underlying psycholinguistic bias in the text and the imageonly bias. And we propose a novel Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multimodal fake news detection. •In our debiasing framework CCD, we conduct causal interventions via backdoor adjustment to remove spurious correlations introduced by the psycholinguistic confounder. For addressing the imageonly bias, we apply counterfactual reasoning to pursue the indirect causal effect as the inference prediction. •Our causal framework CCD can be applied to any fake news detection model with image and text features as inputs. We implement the proposed framework on three strong baseline models, and conduct extensive experiments on two widely used benchmark datasets, validating the effectiveness of CCD. 2 Preliminaries 2.1 Causal Graph The causal graph (Glymour et al., 2016) is a probabilistic graphical model used to describe how variables interact with each other, expressed by a directed acyclic graph G={N,E}consisting of the628sets of variables Nand the causal correlations E between two nodes. As shown in Figure 3, X→Y denotes that Xis the cause of the effect Y.Uis the confounder. UXY Figure 3: An example of causal graph. 2.2 Causal Intervention Causal intervention is used to seek the real causal effect of one variable on another when there exist confounders. In a causal graph, the intervening operation on a variable removes all edges pointing to it, such that its parent nodes no longer cause it. The backdoor adjustment (Glymour et al., 2016) with docalculus offers a tool for calculating the intervened distribution under the condition of no extra confounders. For the example in Figure 3, the adjustment formula can be derived according to Bayes’ theorem as follows, where udenotes the value of confounder U: P(Y|do(X)) =/summationtext uP(Y|X, u)P(u).(1) 2.3 Counterfactual Reasoning and Causal Effect Counterfactual reasoning (Pearl, 2009) is a statistical inference method used to infer outcomes under hypothetical conditions that are different from the factual world. By conducting counterfactual reasoning, we can estimate the causal effect (Pearl, 2022) of a treatment variable on a response variable. For instance, Figure 4 shows an  Evaluation Metrics We use the Accuracy as the evaluation metric for binary classification tasks such as fake news detection. In consideration of the imbalance label distributions, in addition to the accuracy metric, we addPrecision ,Recall , and F1score as complementary evaluation metrics following previous works (Wu et al., 2021; Qian et al., 2021b). 4.1.4 Implementation Details All of the methods are trained for 200 epochs and the initial learning rate for the Adam optimizer is tuned in [1e5, 1e3]. For the confounder dictionary Du∈RN×du, N is 18 (Anger, Anxiety, Assent, Causation, Certainty, Differentiation, Discrepancy, Feel, Hear, Insight, Negative emotion, Netspeak, Nonfluencies, Positive emotion, Sadness, See, Swear words, Tentative), and duis set to 4. For the scaled DotProduct attention, the scaling factor dmis set to 256. As for other necessary hyperparameters in the baseline methods, our settings are consistent with them. 4.2 Experimental Results Table 2 displays the experimental results of our proposed framework CCD applied to the baseline methods on two benchmark datasets. The results of the baselines are the results of our reproductions on our data settings based on their public code2. From Table 2, we can obtain the following observations: Compared with each base fake news detection model (i.e. SpotFake+, MCAN, HMCAN), the accuracy of the models that apply the proposed CCD framework (i.e., w/ CCD) has been significantly improved by around 7.7%, 3.3%, and 5.2% on the Twitter dataset, and improved by around 1.0%, 0.6%, and 1.3% on the Pheme dataset. With the help of the proposed framework, all of the base models show significant improvements on most metrics, which demonstrates the effectiveness of the proposed framework. We believe that CCD benefits from the removal of psycholinguistic bias with causal intervention as well as the mitigation of the imageonly bias via counterfactual reasoning. The performance improvements on the Twitter dataset are larger than that on the Pheme dataset. We attribute such a difference between the two datasets to the following two reasons: 1) The proportion of psycholinguistic vocabulary in the Twitter dataset (19.87%) is higher than that in the Pheme dataset (16.19%), so the Twitter dataset could be more susceptible to psycholinguistic bias. 2) According to Table 1, the number of unique images in the Twitter dataset is far less than the number of news texts, which means that there’s a serious problem of different texts sharing the same image. So the influence of imageonly bias in the Twitter dataset is more severe than that of the Pheme dataset. 4.3 Ablation Study of Causal Inference We conduct experiments to study the debiasing effect of each module in CCD using the strong baseline HMCAN on Twitter and Pheme testing 2https://github.com/shiivangii/SpotFakePlus. https://github.com/wangjinguang502/HMCAN. https://github.com/wuyang45/MCAN_code.632Dataset Methods AccuracyFake news Real news Precision Recall F1 Precision Recall F1 TwitterSpotFake+ 0.795 0.622 0.607 0.614 0.856 0.864 0.860 w/ CCD 0.856* 0.750 0.849 0.797* 0.920 0.860 0.889* MCAN 0.799 0.980 0.401 0.569 0.770 0.996 0.869 w/ CCD 0.825* 0.829 0.595 0.692* 0.824 0.939 0.878* HMCAN 0.831 0.955 0.514 0.668 0.804 0.988 0.887 w/ CCD 0.874* 0.820 0.792 0.806* 0.899 0.914 0.906* PhemeSpotFake+ 0.815 0.711 0.525 0.604 0.840 0.921 0.879 w/ CCD 0.823* 0.714 0.574 0.636* 0.854 0.915 0.883* MCAN 0.834 0.716 0.639 0.675 0.872 0.906 0.889 w/ CCD 0.839* 0.693 0.721 0.707* 0.896 0.882 0.889 HMCAN 0.848 0.762 0.705 0.732 0.881 0.908 0.894 w/ CCD 0.859* 0.764 0.689 0.724 0.889 0.921 0.905* Table 2: Results of comparison among different models on Twitter and Pheme datasets. The best results are in bold. The marker * indicates that the improvement is statistically significant compared with the baseline (ttest with pvalue < 0.05). Dataset analysis of the psycholinguistic word distribution of real news and fake news based on the LIWC 2015 dictionary (Pennebaker et al., 2015). Take the Twitter dataset as an example, as shown in Figure 1, we can observe that the word frequency distribution of fake news is quite different from that of real news, especially for words expressing anxiety, negative emotions, positive emotions, tentative, and netspeak. It seems that we can draw a conclusion that fake news prefers to use loaded language to stir up the reader’s emotions and attract more attention. Consequently, the model could be prone to relying on such psycholinguistic features as a shortcut to judge news authenticity. However, we analyze the training set and testing set, and find that there exist significant differences in the frequency of these psycholinguistic words. The manifest differences between the training set and testing set have proven that this shortcut appears to be unreliable evidence. As shown in Figure 2(b) where Udenotes the confounder (i.e. the psycholinguistic features in the text), there exist a backdoor path T←U→Ywhich will introduce spurious correlations among the text features and news label. In order to remove the psycholinguistic bias, we apply causal intervention by adopting the backdoor adjustment (Glymour et al., 2016) with docalculus P(Y|do(T))to calculate the causal effect in the training stage, which is fundamentally different from the conventional likelihood P(Y|T). For the I→Ybranch, we observe from the datasets that two different news pieces sharing the same image could have contrary labels. This shows that sometimes even if the image is real, the text could be fabricated, and the news could thus befake. We can take advantage of images as an additional modality to provide more detection evidence, but it is unreliable to infer the authenticity of the news based on the image features alone. In this case, we argue that the imageonly bias (i.e., the direct causal effect from image features alone to news label) should be eliminated. Towards this end, we use counterfactual reasoning by imagining a counterfactual world (Figure 2(c)) where both text features Tand fused features Care not given (represented by reference values t∗andc∗), except for image features I. In this way, the bias can be estimated by computing the direct causal effect ofIonYand we can conduct the debiasing by subtracting it from the total effect on Y. We instantiate our proposed debiasing framework on three strong baseline models that can handle both text and image features as inputs. Extensive experiments on two widely used realworld benchmark datasets show the effectiveness of our framework. Overall, our contributions can be summarized as follows: •We analyze each modality of fake news detection data and identify the underlying psycholinguistic bias in the text and the imageonly bias. And we propose a novel Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multimodal fake news detection. •In our debiasing framework CCD, we conduct causal interventions via backdoor adjustment to remove spurious correlations introduced by the psycholinguistic confounder. For addressing the imageonly bias, we apply counterfactual reasoning to pursue the indirect causal effect as the inference prediction. •Our causal framework CCD can be applied to any fake news detection model with image and text features as inputs. We implement the proposed framework on three strong baseline models, and conduct extensive experiments on two widely used benchmark datasets, validating the effectiveness of CCD. 2 Preliminaries 2.1 Causal Graph The causal graph (Glymour et al., 2016) is a probabilistic graphical model used to describe how variables interact with each other, expressed by a directed acyclic graph G={N,E}consisting of the628sets of variables Nand the causal correlations E between two nodes. As shown in Figure 3, X→Y denotes that Xis the cause of the effect Y.Uis the confounder. UXY Figure 3: An example of causal graph. 2.2 Causal Intervention Causal intervention is used to seek the real causal effect of one variable on another when there exist confounders. In a causal graph, the intervening operation on a variable removes all edges pointing to it, such that its parent nodes no longer cause it. The backdoor adjustment (Glymour et al., 2016) with docalculus offers a tool for calculating the intervened distribution under the condition of no extra confounders. For the example in Figure 3, the adjustment formula can be derived according to Bayes’ theorem as follows, where udenotes the value of confounder U: P(Y|do(X)) =/summationtext uP(Y|X, u)P(u).(1) 2.3 Counterfactual Reasoning and Causal Effect Counterfactual reasoning (Pearl, 2009) is a statistical inference method used to infer outcomes under hypothetical conditions that are different from the factual world. By conducting counterfactual reasoning, we can estimate the causal effect (Pearl, 2022) of a treatment variable on a response variable. For instance, Figure 4 shows an experiments use more flexible models. We are now ready to describe how, for compositionalI, structure in Ltranslates into structure in the set of wellformed examples X. Definition 5. A function fis ahomomorphism of (ΣΣΣ,LLL) (an “Lhomomorphism”) if: ∀r∈ L,∀x1. . . x p∈Σ : r(x1, . . . , x p) =r(f(x1), . . . , f (xp)) (2) f“preserves the structure” of L, ensuring that pairwise relationships are preserved among symbols. Fig. 1 shows examples: in (c), for instance, the words yellow andgreen and the corresponding colors must be swapped to satisfy Eq. 2. Finally, we may state our main result: Theorem 1. IfXisLcompositional, fis an Lhomomorphism, and x∈X, then f(x) = [f(x1), . . . , f (xn)]∈X. Thus every homomorphism of Lwellformed examples ∈X. Proof. From Definition 3 and 5, Ri(f(x)) = Ri(x)∀i. Then, 1[f(x)∈X]=I(f(x)) =C(L(f(x))) =C(R1(f(x)), . . . , R n(f(x))) =C(R1(x), . . . , R n(x)) =I(x) =1[x∈X] 3As shown in Example 2b, it can be used to implement a languageto-logical form mapping, and thus generalizes the Montagovian definition of compositionality as a homomorphism from sentences to meanings (Montague, 1970a).642Corollary 1. With the additional constraint that f is anLisomorphism (i.e., has an inverse), then f is a symmetry of Xin the sense of Eq. 1. Here it suffices to show that the preimage of every x∈Xis also in X; the proof is the same as Theorem 1 with f−1in place of f. Despite their simplicity, Theorem 1 and its corollary have an important consequence: if we can identify candidate entries in L,even if Cis unknown , we can construct new examples x∈X that respect, and provide evidence for, the compositional structure of X. There is an intriguing (if inexact) structural similarity between Corollary 1 and Noether’s theorem (Noether, 1918), which establishes an equivalence between symmetries of physical systems and their conserved quantities. Here, such symmetries imply constraints not on conservation laws but interpretation functions. 4 L EXSYM: Data Augmentation with LLLhomomorphisms Given a lexicon describing symbols and their relations, we have shown how to turn homomorphisms of the lexicon into transformations of a dataset. Each such function fthat takes an example xas input, replaces each token xi∈xwith a new one, and returns a wellformed example x′as output. EveryLhomomorphism may thus be viewed as a recipe for synthesizing training examples from a small initial training set (Japkowicz et al., 2000). However, to make this a practical modeling tool, we need some way of constucting Lhomomorphisms for a task of interest. Below, we describe how to do so automatically: first, starting with only a taskspecific lexicon L(Sec. 4.1); next, starting with only a dataset and no initial lexicon (Sec. 4.2). We term the resulting approach LEXSYM. 4.1 Deriving Homomorphisms from Lexicons Even in complex sequence modeling problems, useful lexicons are often simple enough that they can be specified by hand (Jones et al., 2012; Gordon et al., 2020). Given a prespecified algebraic L, there is a straightforward procedure for generating the associated symmetries by enumerating all functions Σ→Σand testing which ones satisfy Eq. 2. (See Algorithm 1 in Appendix B.) This algorithm is inefficient, but simple and practical for small |L|.4.2 Deriving Lexicons from Datasets For some tasks, it may be difficult to manually specify an algebraic lexicon. We next describe how to infer one automatically. We focus on an important and extremely common class of language understanding problems with special structure. In semantic parsing andinstruction following , examplesxconsist of (input, output) pairs in which inputs are sentences, outputs are meaning representations, and word meaning is characterized by a lexicon with two components. First, a set of unary type predicates {rτ}that assign words to types (like ENTITY in semantic parsing). Second, a semantic correspondence relation rϵthat specifies which actions or logical symbols can be derived from words (like sings→sing′). With ntypes, the lexicon required for these problems isL= (rτ1, . . . , r τn, rϵ), which we abbreviate ({rτk}, rϵ)below. We now show how to improve upon the procedure in Sec. 4.1 by deriving Lfrom data and sampling Lhomomorphisms in constant time. Learning LLLWe build on past work noting that dictionaries of semantic correspondences can be constructed using alignment algorithms (Brown et al., 1993). Given an input xconsisting of a pair (xtext,xmeaning ), we use existing algorithms to align tokens in individual training examples. Finally, we identify the most frequently occurring alignments and add these to the semantic correspondence relation. We may similarly use existing procedures to infer types by deriving them from partof-speech tags or distributional patterns. See Appendix D for details of the alignment and type inference algorithms used in our experiments. These algorithms produce lexicons with three properties that are useful for the sampling scheme we describe next: types aredisjoint , and semantic correspondences are oneto-many andtypepreserving (if two words are of the same type, so are their translations). Sampling LLLhomomorphisms Once we have identified types and semantic correspondences, sampling Lhomomorphisms is straightforward: Theorem 2. Letxiandxj∈Σhave the same type rτ(xi) =rτ(xj) = 1 . For convenience, let Ei= {x:rϵ(xi, x) = 1}denote possible translations of643xi. The fis anLhomomorphism: f(x) =  xj ifx=xi xi ifx=xj x′∈Ejifx∈Ei x′∈Eiifx∈Ej x otherwise(3) Proof is given in Appendix A. Theorem 2 yields an intuitive data augmentation procedure: select two (input, output) pairs of the same type, and swap them and any of their meanings wherever they occur. Fig. 1b shows an example. Eq. 3 is related to data augmentation schemes described by Andreas (2020) and Liu et al. (2021b), which synchronously substitute words or phrases (equivalent to removing cases 2 and 4). Unlike LEXSYM, these methods cannot guarantee correctness: in Fig. 1c, substitutinggreen in place of yellow yields an image with two green objects and an incorrect answer. 5 Experiments Our experiments aim to evaluate whether LEXSYM can improve compositional generalization in downstream models. The main goal of these experiments is to evaluate generality across tasks and data modalities. Evaluation focuses on three diverse classes of language understanding problems: complex, contextdependent computations (Sec. 5.1), large, automatically derived lexicons (Sec. 5.2), and multimodal data (Sec. 5.3). 5.1 Complex computations We first test LEXSYMon the ALCHEMY task from theSCONE benchmark (Long et al., 2016)—a problem involving a complex sentence interpretation procedure that makes it challenging to apply existing data augmentation schemes. Data InALCHEMY (Fig. 1a), models must execute a sequence of humanwritten English instructionsx1:N ins, on an initial state x0 stateconsisting of beakers of colored liquids (textually represented as sequence of symbols “1: gg, 2: ...”), to predict the final state xN state. Initial and final states are encoded as sequences of color tokens. Predicting final states requires both grounding colors in state variables (brown →b, red→g) and modeling what happens when colors are combined (e.g. mixing gand ryields b).LEXSYM We manually construct a lexicon to showcase how to inject prior knowledge into LEXSYM. We encode word meaning in two relations: a semantic equivalence relation between color words and colors: rϵ(c1, c2) =  1c1=brown , c 2=b 1c1=red, c 2=r 1c1=green , c 2=g ... 0otherwise and a ternary relation that encodes the result of mixing colors:4 rmix(c1, c2, c3) =  1c1=c2=c3 1c1̸=c2∧c3=b 0otherwise Together, (rϵ, rmix,{rτk}), where {rτk}assigns different types to color words, colors, and remaining tokens. The homomorphic transformations of this lexicon exchange color words and colors but preserve mixing relations. Models and Training We train an LSTM (Hochreiter and Schmidhuber, 1997) and finetune a T5 transformer (Raffel et al., 2020) on the sequenceto-sequence prediction problem (x1:N ins,x0 state)→xN state Training details may be found in Appendix C. We compare these baseline models to their LEXSYMaugmented versions as well as the existing compositional data augmentation scheme of Liu et al. (2021b). Results See Table 1. LSTM+ LEXSYMimproves substantially over an LSTM. Preserving the homomorphism condition in Eq. 2 is extremely important: the procedure of Liu et al. (2021b), which naively substitutes aligned color pairs, actually hurts performance. Pretrained models achieve strong initial results; combining pretraining with LEXSYMgives additional improvements. 5.2 Learned lexicons We next show that for more conventional sequenceto-sequence problems, we may apply LEXSYM with automatically derived lexicons. 4InALCHEMY , mixing nonidentical colors produces b.644Model ALCHEMY SCAN (jump ) S CAN (around right ) C OGS COGS (nonce) Previous Work on COGS & SCAN GECA (Andreas, 2020) – 99.94 ±0.10 98.50 ±1.90 47.74 ±4.52– LeAR (Liu et al., 2021a) – – – 97.70 ±0.70– LexLSTM (Akyurek and Andreas, 2021) 36.80 ±1.96 99.14 ±1.55 88.41 ±7.35 82.17 ±0.7281.40 ±0.40 No Pretraining LSTM 41.72 ±1.15 000.41 ±0.34 08.65 ±4.52 61.13 ±4.1261.13 ±4.12 + Substitute (e.g. Liu et al., 2021b) 40.52 ±0.84 099.95 ±0.10 99.17 ±0.93 81.99 ±0.5077.62 ±0.78 + LEXSYM 45.85 ±2.00 100.00 ±0 99.51 ±0.48 81.86 ±0.9077.25 ±0.34 Language Pretraining T5 84.95 ±0.44 93.60 ±0 38.40 ±0.90 83.30 ±0.1064.20 ±2.00 +CSLAug* (Qiu et al., 2022) – 99.70 ±0 – 99.50 ±0 – +LEXSYM 85.48 ±0.16 99.96 ±0.03 97.29 ±2.16 83.62 ±0.2776.74 ±2.23 Table 1: Results on semantic parsing and instruction following. We provide mean and standard deviations over 5 random seeds. L EXSYMimproves significantly over baselines, with and without largescale pretraining. *Uses a customized formal representation. COGENT C LEVR Visual Pretraining Human (Johnson et al., 2017) – 92.6 Film (Perez et al., 2018) 78.8 97.7 SMAC (Marois et al., 2018) 78.7 98.9 NSVQA (Yi et al., 2018) 63.9 99.7 Seq2Seq Baselines T5 79.7 – LexLSTM 62.1 – No PrePraining VQATransformer 73.3 ±1.0 93.6±0.5 + Substitute (e.g. Liu et al., 2021b) 84.4 ±0.7 90.8±0.3 + LexSym 85.9±0.9 92.0±0.9 Table 2: Exact match accuries on the CLEVR and CLEVR -COGENTvalidation sets. Results are averaged over 4 seeds. We obtain stateof-theart results after applying LEXSYMto a (nonpretrained) sequence model. LEXSYMalso yields higher accuracies than synchronous token substitution . (A detailed breakdown by question category is presented in Table 4). Data We study two standard compositional generalization benchmarks: the SCAN (Lake and Baroni, 2018) instruction following and COGS (Kim and Linzen, 2020, Fig. 1b) semantic parsing datasets. SCAN consists of simple instruction following tasks in which strings are translated into sequences of actions. We focus on the jump split, which measures models’ ability to compose words that only appeared in isolation during training, and thearound right split, which measures generalization to novel collocations. The COGS dataset tests compositional generalization in semantic parsing. The dataset includes English (sentence, logical form) pairs, with systematic differences between train and test set sentence structure. We include a variant containing nonce words (Kimet al., 2022) to disentangle general compositional skills from lexical knowledge acquired during pretraining. See Appendix G for dataset statistics. LEXSYM We use automatic lexicon extraction to find semantic correspondence relations ( rϵ) and types ( {rτk}) as described in Appendix D. Next, we apply swapbased augmentation (Eq. 3). Models We use the same models as Sec. 5.1, along with a strong semistructured model, LeAR (Liu et al., 2021a) tailored for COGS, and another substitution based augmentation (Andreas, 2020) tailored for SCAN. Following Akyurek and Andreas (2021), we equip the LSTM for COGS with a copy mechanism as it achieves significantly better results than Kim and Linzen (2020)’s baseline. Results OnSCAN,LEXSYMobtains nearperfect accuracy in both jump andaround right splits. On the original COGS datasets, LEXSYMsubstantially outperforms the LSTM model and GECA augmentation, and is comparable to a neural sequence model specialized for lexical generalization (LexLSTM). Stronger results can be achieved with models specifically tailored toward semantic parsing tasks (LeAR). In both tasks, LEXSYMalso improves upon largescale pretraining. 5.3 Multimodal data Finally, we combine learned lexicons with nonsequential data to advance the state of the art on a longstanding visual question answering challenge. Data TheCLEVR dataset (Johnson et al., 2017, Fig. 1c) contains Englishlanguage questions about generated 3D scenes containing multiple objects.645Questions involve complex computational operations including quantification, comparison, and spatial reasoning. CLEVR has been a popular testbed for evaluating composition in visual question answering models. Our main experiment uses theCOGENTsplit of the dataset, which focuses on compositional generalization. In the CLEVR - COGENTtraining set (Split A), which contains roughly 700K(question, image, answer) triples, all cubes are gray, blue, brown or yellow, while all cylinders are red, green, purple or cyan. In the test set (validation set of Split B), these are reversed. LEXSYM In VQA and other multimodal tasks, part of the input is continuous (e.g. images and videos). Recent work has shown that it is possible to learn highquality discrete representations of continuous input data. For example, in the VQV AE model of van den Oord et al. (2017), a continuous image is transformed into a grid of categorical codes, with individual codes representing color, and in some cases materials and illumination (examples in Table 3). We use this discretization procedure for our experiments (see Appendix C.1 for details). We use the same algorithm as previous section to extract lexical relations. Models Most prior work on visual question answering has used pretrained convolutional networks to encode images, and recurrent networks to encode questions and generate answers. For experiments on CLEVR , we use a simplified model in which both questions and images are mapped to answers by a transformer model, similarly to Ramesh et al. (2021). See Appendix C.2 for details. Both LEXSYMaugmentation and this VQATransformer model operate over sequences of discrete visual codes produced by a vectorquantized variational autoencoder. Once these discrete representations have been produced, we infer lexicons and perform data augmentation directly to these representations, without resynthesizing images (though such synthesis is possible, as in Table 3, to interpret model behavior). The COGENTtask is very different from the sequence modeling tasks discussed above: inputs contain many tokens, and the training set is orders of magnitude larger. GECA and CSLAug, which have a high polynomial dependence on sequence length, could not be applied as they fail to terminate within a reasonable amount of time.Results In Table 2, a transformer model with LEXSYMachieves stateof-theart results on the CLEVR -COGENTdataset, reducing errors by roughly 33% relative to the best existing system. LEXSYMalso outperforms substitution baseddata augmentation (Liu et al., 2021b), particularly on semantically complex utterances involving quantification (App. Table 4). On the IID CLEVR split, LEXSYM’s performance is comparable to humans, and somewhat behind pretrained models. 6 Other Related Work Lexicalized neural models Wordlevel alignments between input and output sequences were an essential feature of statistical phraseand treebased sequence models (Chiang et al., 2005; Koehn et al., 2003). Neural scoring functions were sometimes integrated into these models (Misra and Artzi, 2016). Neural models with attention (Bahdanau et al., 2015) do not require explicit alignment, though several pieces of past work have shown that incorporating explicit tokenlevel correspondences improves generalization (Akyurek and Andreas, 2021; Prabhu and Kann, 2020; Pham et al., 2018). The semantic correspondence function in Sec. 4 plays the same role as the input–output dictionary in these methods, but LEXSYMas a whole is more general: it is not restricted to modeling sequenceto-sequence problems, and can infer and exploit correspondence relations between component of an example. To the best of our knowledge, this paper is also the first to make use of tokenlevel alignments in joint neural models of text and images. Compositionality in representation learning While we have focused on compositionality as a property of data distributions or interpretation functions, another line of work in machine learning and language evolution has studied compositionality as an emergent property of learned representations (Andreas, 2019; Resnick et al., 2019; Brighton and Kirby, 2006). In settings where representational compositionality is desirable (e.g. to train communication protocols that can generalize to new states), LEXSYMmight provide a tool for promoting it. Equivariant Sequence Models As mentioned in Sec. 2, our work builds on existing approaches that control generalization with specialized model architectures designed to be equivariant to permutations of a prespecified lexicon (if f(x1···xn) = y1···ymthen646f(π(x1)···π(xn)) = π(y1)···π(ym)for a permutation π) (Gordon et al., 2020; White and Cotterell, 2022). LEXSYMdiffers from these approaches in three ways. First, LEXSYMis modelagnostic and compatible with pretraining. Second, LEXSYMis compatible with (and automatically derives transformations for) more complicated relations than input–output correspondences, making it possible to apply to tasks like ALCHEMY where such relations are important. Finally, LEXSYM gracefully handles (possibly noisy) learned lexicons, making it applicable to tasks like COGENT with complex or uninterpretable token mappings. Data Augmentation Data augmentation approaches are widely used across machine learning application domains featuring known invariances of the data distribution (Japkowicz et al., 2000; Jia and Liang, 2016; Shaw et al., 2021). Substitutionbased schemes that replace words with synonyms, or synchronously replace words and their translations, are widely used for machine translation and general debiasing (Liu et al., 2021b; Wang et al., 2018; Wei and Zou, 2019). 7 Limitations and Future Directions While Sec. 3 characterizes the effect of general Lhomomorphisms, LEXSYMspecifically produces singletoken swaps. In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features (e.g. color and texture), these features will remain entangled in synthesized examples. It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set (Lake et al., 2019). This is because LEXSYMtargets compositionality but not recursion , which is also required to model the full range of humanlike generalizations in sequence learning problems. LEXSYMis also sensitive to the nature of the tokenization scheme itself. In morphologically rich languages, for example, LEXSYMmay need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers (Narasimhan et al., 2015; Bergmanis and Goldwater, 2017; Cotterell and Schütze, 2018) (analogous to the use of learned image patch representations rather than pixels in our VQA experiments). Finally, LEXSYMdoes not induce some of the generalizations obtained other methods for improving compositional generalization, especially those that exploit extra structure (e.g. treeshaped inputs and outputs) in the semantic parsing domain (e.g. Liu et al., 2021a). It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees. 8 Conclusion We have presented LEXSYM, a new data augmentation  Evaluation focuses on three diverse classes of language understanding problems: complex, contextdependent computations (Sec. 5.1), large, automatically derived lexicons (Sec. 5.2), and multimodal data (Sec. 5.3). 5.1 Complex computations We first test LEXSYMon the ALCHEMY task from theSCONE benchmark (Long et al., 2016)—a problem involving a complex sentence interpretation procedure that makes it challenging to apply existing data augmentation schemes. Data InALCHEMY (Fig. 1a), models must execute a sequence of humanwritten English instructionsx1:N ins, on an initial state x0 stateconsisting of beakers of colored liquids (textually represented as sequence of symbols “1: gg, 2: ...”), to predict the final state xN state. Initial and final states are encoded as sequences of color tokens. Predicting final states requires both grounding colors in state variables (brown →b, red→g) and modeling what happens when colors are combined (e.g. mixing gand ryields b).LEXSYM We manually construct a lexicon to showcase how to inject prior knowledge into LEXSYM. We encode word meaning in two relations: a semantic equivalence relation between color words and colors: rϵ(c1, c2) =  1c1=brown , c 2=b 1c1=red, c 2=r 1c1=green , c 2=g ... 0otherwise and a ternary relation that encodes the result of mixing colors:4 rmix(c1, c2, c3) =  1c1=c2=c3 1c1̸=c2∧c3=b 0otherwise Together, (rϵ, rmix,{rτk}), where {rτk}assigns different types to color words, colors, and remaining tokens. The homomorphic transformations of this lexicon exchange color words and colors but preserve mixing relations. Models and Training We train an LSTM (Hochreiter and Schmidhuber, 1997) and finetune a T5 transformer (Raffel et al., 2020) on the sequenceto-sequence prediction problem (x1:N ins,x0 state)→xN state Training details may be found in Appendix C. We compare these baseline models to their LEXSYMaugmented versions as well as the existing compositional data augmentation scheme of Liu et al. (2021b). Results See Table 1. LSTM+ LEXSYMimproves substantially over an LSTM. Preserving the homomorphism condition in Eq. 2 is extremely important: the procedure of Liu et al. (2021b), which naively substitutes aligned color pairs, actually hurts performance. Pretrained models achieve strong initial results; combining pretraining with LEXSYMgives additional improvements. 5.2 Learned lexicons We next show that for more conventional sequenceto-sequence problems, we may apply LEXSYM with automatically derived lexicons. 4InALCHEMY , mixing nonidentical colors produces b.644Model ALCHEMY SCAN (jump ) S CAN (around right ) C OGS COGS (nonce) Previous Work on COGS & SCAN GECA (Andreas, 2020) – 99.94 ±0.10 98.50 ±1.90 47.74 ±4.52– LeAR (Liu et al., 2021a) – – – 97.70 ±0.70– LexLSTM (Akyurek and Andreas, 2021) 36.80 ±1.96 99.14 ±1.55 88.41 ±7.35 82.17 ±0.7281.40 ±0.40 No Pretraining LSTM 41.72 ±1.15 000.41 ±0.34 08.65 ±4.52 61.13 ±4.1261.13 ±4.12 + Substitute (e.g. Liu et al., 2021b) 40.52 ±0.84 099.95 ±0.10 99.17 ±0.93 81.99 ±0.5077.62 ±0.78 + LEXSYM 45.85 ±2.00 100.00 ±0 99.51 ±0.48 81.86 ±0.9077.25 ±0.34 Language Pretraining T5 84.95 ±0.44 93.60 ±0 38.40 ±0.90 83.30 ±0.1064.20 ±2.00 +CSLAug* (Qiu et al., 2022) – 99.70 ±0 – 99.50 ±0 – +LEXSYM 85.48 ±0.16 99.96 ±0.03 97.29 ±2.16 83.62 ±0.2776.74 ±2.23 Table 1: Results on semantic parsing and instruction following. We provide mean and standard deviations over 5 random seeds. L EXSYMimproves significantly over baselines, with and without largescale pretraining. *Uses a customized formal representation. COGENT C LEVR Visual Pretraining Human (Johnson et al., 2017) – 92.6 Film (Perez et al., 2018) 78.8 97.7 SMAC (Marois et al., 2018) 78.7 98.9 NSVQA (Yi et al., 2018) 63.9 99.7 Seq2Seq Baselines T5 79.7 – LexLSTM 62.1 – No PrePraining VQATransformer 73.3 ±1.0 93.6±0.5 + Substitute (e.g. Liu et al., 2021b) 84.4 ±0.7 90.8±0.3 + LexSym 85.9±0.9 92.0±0.9 Table 2: Exact match accuries on the CLEVR and CLEVR -COGENTvalidation sets. Results are averaged over 4 seeds. We obtain stateof-theart results after applying LEXSYMto a (nonpretrained) sequence model. LEXSYMalso yields higher accuracies than synchronous token substitution . (A detailed breakdown by question category is presented in Table 4). Data We study two standard compositional generalization benchmarks: the SCAN (Lake and Baroni, 2018) instruction following and COGS (Kim and Linzen, 2020, Fig. 1b) semantic parsing datasets. SCAN consists of simple instruction following tasks in which strings are translated into sequences of actions. We focus on the jump split, which measures models’ ability to compose words that only appeared in isolation during training, and thearound right split, which measures generalization to novel collocations. The COGS dataset tests compositional generalization in semantic parsing. The dataset includes English (sentence, logical form) pairs, with systematic differences between train and test set sentence structure. We include a variant containing nonce words (Kimet al., 2022) to disentangle general compositional skills from lexical knowledge acquired during pretraining. See Appendix G for dataset statistics. LEXSYM We use automatic lexicon extraction to find semantic correspondence relations ( rϵ) and types ( {rτk}) as described in Appendix D. Next, we apply swapbased augmentation (Eq. 3). Models We use the same models as Sec. 5.1, along with a strong semistructured model, LeAR (Liu et al., 2021a) tailored for COGS, and another substitution based augmentation (Andreas, 2020) tailored for SCAN. Following Akyurek and Andreas (2021), we equip the LSTM for COGS with a copy mechanism as it achieves significantly better results than Kim and Linzen (2020)’s baseline. Results OnSCAN,LEXSYMobtains nearperfect accuracy in both jump andaround right splits. On the original COGS datasets, LEXSYMsubstantially outperforms the LSTM model and GECA augmentation, and is comparable to a neural sequence model specialized for lexical generalization (LexLSTM). Stronger results can be achieved with models specifically tailored toward semantic parsing tasks (LeAR). In both tasks, LEXSYMalso improves upon largescale pretraining. 5.3 Multimodal data Finally, we combine learned lexicons with nonsequential data to advance the state of the art on a longstanding visual question answering challenge. Data TheCLEVR dataset (Johnson et al., 2017, Fig. 1c) contains Englishlanguage questions about generated 3D scenes containing multiple objects.645Questions involve complex computational operations including quantification, comparison, and spatial reasoning. CLEVR has been a popular testbed for evaluating composition in visual question answering models. Our main experiment uses theCOGENTsplit of the dataset, which focuses on compositional generalization. In the CLEVR - COGENTtraining set (Split A), which contains roughly 700K(question, image, answer) triples, all cubes are gray, blue, brown or yellow, while all cylinders are red, green, purple or cyan. In the test set (validation set of Split B), these are reversed. LEXSYM In VQA and other multimodal tasks, part of the input is continuous (e.g. images and videos). Recent work has shown that it is possible to learn highquality discrete representations of continuous input data. For example, in the VQV AE model of van den Oord et al. (2017), a continuous image is transformed into a grid of categorical codes, with individual codes representing color, and in some cases materials and illumination (examples in Table 3). We use this discretization procedure for our experiments (see Appendix C.1 for details). We use the same algorithm as previous section to extract lexical relations. Models Most prior work on visual question answering has used pretrained convolutional networks to encode images, and recurrent networks to encode questions and generate answers. For experiments on CLEVR , we use a simplified model in which both questions and images are mapped to answers by a transformer model, similarly to Ramesh et al. (2021). See Appendix C.2 for details. Both LEXSYMaugmentation and this VQATransformer model operate over sequences of discrete visual codes produced by a vectorquantized variational autoencoder. Once these discrete representations have been produced, we infer lexicons and perform data augmentation directly to these representations, without resynthesizing images (though such synthesis is possible, as in Table 3, to interpret model behavior). The COGENTtask is very different from the sequence modeling tasks discussed above: inputs contain many tokens, and the training set is orders of magnitude larger. GECA and CSLAug, which have a high polynomial dependence on sequence length, could not be applied as they fail to terminate within a reasonable amount of time.Results In Table 2, a transformer model with LEXSYMachieves stateof-theart results on the CLEVR -COGENTdataset, reducing errors by roughly 33% relative to the best existing system. LEXSYMalso outperforms substitution baseddata augmentation (Liu et al., 2021b), particularly on semantically complex utterances involving quantification (App. Table 4). On the IID CLEVR split, LEXSYM’s performance is comparable to humans, and somewhat behind pretrained models. 6 Other Related Work Lexicalized neural models Wordlevel alignments between input and output sequences were an essential feature of statistical phraseand treebased sequence models (Chiang et al., 2005; Koehn et al., 2003). Neural scoring functions were sometimes integrated into these models (Misra and Artzi, 2016). Neural models with attention (Bahdanau et al., 2015) do not require explicit alignment, though several pieces of past work have shown that incorporating explicit tokenlevel correspondences improves generalization (Akyurek and Andreas, 2021; Prabhu and Kann, 2020; Pham et al., 2018). The semantic correspondence function in Sec. 4 plays the same role as the input–output dictionary in these methods, but LEXSYMas a whole is more general: it is not restricted to modeling sequenceto-sequence problems, and can infer and exploit correspondence relations between component of an example. To the best of our knowledge, this paper is also the first to make use of tokenlevel alignments in joint neural models of text and images. Compositionality in representation learning While we have focused on compositionality as a property of data distributions or interpretation functions, another line of work in machine learning and language evolution has studied compositionality as an emergent property of learned representations (Andreas, 2019; Resnick et al., 2019; Brighton and Kirby, 2006). In settings where representational compositionality is desirable (e.g. to train communication protocols that can generalize to new states), LEXSYMmight provide a tool for promoting it. Equivariant Sequence Models As mentioned in Sec. 2, our work builds on existing approaches that control generalization with specialized model architectures designed to be equivariant to permutations of a prespecified lexicon (if f(x1···xn) = y1···ymthen646f(π(x1)···π(xn)) = π(y1)···π(ym)for a permutation π) (Gordon et al., 2020; White and Cotterell, 2022). LEXSYMdiffers from these approaches in three ways. First, LEXSYMis modelagnostic and compatible with pretraining. Second, LEXSYMis compatible with (and automatically derives transformations for) more complicated relations than input–output correspondences, making it possible to apply to tasks like ALCHEMY where such relations are important. Finally, LEXSYM gracefully handles (possibly noisy) learned lexicons, making it applicable to tasks like COGENT with complex or uninterpretable token mappings. Data Augmentation Data augmentation approaches are widely used across machine learning application domains featuring known invariances of the data distribution (Japkowicz et al., 2000; Jia and Liang, 2016; Shaw et al., 2021). Substitutionbased schemes that replace words with synonyms, or synchronously replace words and their translations, are widely used for machine translation and general debiasing (Liu et al., 2021b; Wang et al., 2018; Wei and Zou, 2019). 7 Limitations and Future Directions While Sec. 3 characterizes the effect of general Lhomomorphisms, LEXSYMspecifically produces singletoken swaps. In images represented as discrete symbol sequences, if a single symbol simultaneously encodes multiple visual features (e.g. color and texture), these features will remain entangled in synthesized examples. It will not exchange substructures larger than a single token, and thus will not synthesize examples longer than those already present in the training set (Lake et al., 2019). This is because LEXSYMtargets compositionality but not recursion , which is also required to model the full range of humanlike generalizations in sequence learning problems. LEXSYMis also sensitive to the nature of the tokenization scheme itself. In morphologically rich languages, for example, LEXSYMmay need to be applied not on top of words or segments, but instead canonicalized morphemes produced by learned morphological analyzers (Narasimhan et al., 2015; Bergmanis and Goldwater, 2017; Cotterell and Schütze, 2018) (analogous to the use of learned image patch representations rather than pixels in our VQA experiments). Finally, LEXSYMdoes not induce some of the generalizations obtained other methods for improving compositional generalization, especially those that exploit extra structure (e.g. treeshaped inputs and outputs) in the semantic parsing domain (e.g. Liu et al., 2021a). It might serve as a platform for future versions of those methods that offer greater generality and formal guarantees. 8 Conclusion We have presented LEXSYM, a new data augmentation  Experiments and Analysis In this section, we first compare our proposed model with typical benchmark models to validate the effectiveness of our model. Then we perform ablation studies to analyze the proposed model, and demonstrate the differences between our model and its compared counterparts. 5.1 Comparisons with baseline models 5.1.1 Baseline models We compare our proposed model, LFMIM, with 6 typical baseline models: tensor fusion network (TFN) (Zadeh et al., 2017), lowrank Multimodal fusion (LMF) (Liu and Shen, 2018), early fusion transformer (EFtransformer), Late fusion transformer (LFtransformer), multimodal transformer (MulT) (Tsai et al., 2019), and progressive modality reinforcement (PMR) (Lv et al., 2021). Note that for early fusion and late fusion methods, we use more powerful transformer models instead of the models in (Williams et al., 2018) and (Yu et al., 2020) for the sake of fairness. We adapt the original PMR (introduced in the   analysis include CMUMOSI (Zadeh et al., 2016), CMUMOSEI (Zadeh et al., 2018), IEMOCAP (Busso et al., 2008), MELD (Poria et al., 2019), CHEA VD (Li et al., 2017b), CHSIMS (Yu et al., 2020), and CHSIMS_v2 (Liu et al., 2022). Most previous datasets annotate the samples with the same labels for all modalities. It is noteworthy that the two Chinese datasets, CHSIMS and CHSIMS_v2, are currently the only datasets that conduct annotations for each modality independently. However, these two datasets are for sentiment analysis, and are labeled with polarized labels, (weakly) positive, (weakly) negative, and neutral. To the best of our knowledge, our dataset CHERMA is the first one that is targeted for multimodal emotion recognition, and has modalitywise annotations. 2.2 Multimodal fusion models At the core of multimodal emotion recognition is the modality fusion strategy. TFN (Zadeh et al., 2017) integrates the multimodality information via calculating the outer product of modality embeddings. Unfortunately, the computation and memory required grow exponentially with the number of modalities, which is addressed by the work of LMF (Liu and Shen, 2018) with low rank approximation. From the perspective of model structure, the previous fusion strategies are usually classified into early fusion and late fusion. Early fusion (Lazaridou et al., 2015; Williams et al., 2018) simply concatenates the lowlevel features of all the modalities, and feeds the joint feature to the model. Early fusion can suffer from the problem of data sparseness (Wu et al., 2014). Late fusion (Liu et al., 2014; Nguyen et al., 2018; Yu et al., 2020) concatenates the highlevel features (some studies also refer this to modellevel fusion (Chen and Jin, 2016)) or decisions separately obtained from individual modalities, which is weak in establishing finegrained correspondence across modalities. Compared with the concatenation methods, multimodal transformer is a more powerful tool that is capable of capturing the intramodal and crossmodal dependency(Poria et al., 2017; Lian et al., 2021). Recent transformerbased works (Tsai et al., 2019; Lv et al., 2021; Nagrani et al., 2021) can be regarded as layerwise fusion, to differentiate them from early and late fusion approaches. Layerwise fusion carries out feature fusion layer by layer from low level to high level, which can capture finegrained correlationItem Quantity Number of utterances 28,717 Average length of utterance (second) 4.6 Minimum number of words per utterance 4 Maximum number of words per utterance 63 Male 11,207 Female 17,510 Preteen 288 Teen 18,500 Middleage 8,522 Old 1,407 Table 1: Statistics of dataset CHERMA. across unaligned modalities. Due to its promising performance, this paper also leverages multimodal transformer with layerwise fusion for our emotion recognition task. 3 Dataset Description In this section, we give a detailed experiments are also presented. 6.1 Detection Performance Following the work of Y o oe ta l . (2022 ), we divide the detection of adversarial samples into two scenarios. Scenario 1 will detect all adversarial samples, regardless of whether the model outputis successfully changed or not. Scenario 2 only requires the detection of samples that actually fool the model. Realistic attackers cannot guarantee the success of every attack, but this does not mean that these failed adversarial samples are harmless. Infact, the failed samples can guide the attacker to further optimize the attacking process, which is the strategy adopted by most attack algorithms. Therefore, we believe Scenario 1 is more realistic, and we will show the performance of each detection algorithm in Scenario 1 in the main text and put the675Dataset MethodTextFooleradj BAE TextFooler PWWS F1 AUROC ACC F1 AUROC ACC F1 AUROC ACC F1 AUROC ACC SST -2DISP 58.9 − 79.2 66.1 − 76.1 72.3 − 76.0 73.3 − 77.4 MDRE 63.2 − 63.3 69.5 − 69.0 74.1 − 74.8 70.2 − 70.8 FGWS 68.2 69.9 64.3 68.9 69.5 64.6 71.7 73.9 68.2 74.2 79.2 70.8 MD 70.3 68.6 63.8 74.7 74.5 70.1 78.6 78.4 74.8 77.2 75.3 72.6 RDE 72.3 77.1 69.3 78.8 84.1 78.3 82.9 88.5 82.1 79.6 85.5 77.1 Ours (CASN) 80.8 89.1 80.3 97.2 98.9 97.1 99.3 99.8 99.3 99.1 99.9 99.1 IMDBDISP 67.3 − 68.0 67.6 − 66.3 67.4 − 66.0 65.3 − 64.3 MDRE 82.2 − 80.8 84.3 − 82.8 85.5 − 84.3 82.6 − 81.6 FGWS 80.9 87.1 78.9 81.3 87.7 80.2 81.2 87.7 80.2 80.5 87.3 79.1 MD 81.4 83.1 79.0 83.7 85.5 81.6 83.7 85.5 81.7 82.4 83.7 79.7 RDE 82.2 88.3 80.7 84.6 90.2 83.2 84.7 90.1 83.7 82.5 86.7 80.1 Ours (CASN) 97.8 99.7 97.8 98.4 99.8 98.4 98.3 99.8 98.3 91.2 96.6 90.9 AGNEWSDISP 61.5 − 85.8 80.8 − 86.3 88.4 − 89.1 84.1 − 87.3 MDRE 57.1 − 61.6 73.0 − 75.5 80.2 − 81.2 74.5 − 76.5 FGWS 74.6 73.2 69.8 75.1 75.9 73.3 77.6 78.4 75.5 81.9 84.3 82.4 MD 67.2 62.3 52.8 71.5 76.1 65.0 75.2 80.8 73.3 71.8 76.8 70.0 RDE 67.7 67.0 55.1 77.1 85.0 75.9 85.3 92.3 85.6 77.8 85.4 77.3 Ours (CASN) 90.0 95.8 89.7 99.8 99.9 99.8 99.9 99.9 99.9 99.9 99.9 99.9 Table 1: Performance of adversarial detection . The best results are marked in bold. Following previous works, Accuracy (ACC), F1, and Area Under the Receiver Operating Characteristic (AUROC) are used for metrics. We use BERT as the victim model, while the detection results of RoBERTa are put in Appendix B.1 . performance of Scenario 2 in Appendix B.2 . Table 1reports the detection performance of our  evaluation metric. (a) SST -2 (b) IMDB (c) AGNEWS Figure 3: The distribution differences of different denoising steps k, where both normal and adversarial samples go through the denoising process. The Gaussian kernel density is used to measure the distribution. Dataset Steps AUROC ACCclean ACCadv SST20− 92.1 4.8 30 80.2 92.1 5.4 60 97.6 92.2 6.8 90 99.8 92.2 8.0 IMDB0− 93.4 20.8 30 99.5 93.5 28.560 99.7 93.6 34.890 99.8 93.9 39.3 AGNEWS0− 94.4 12.8 30 99.9 94.4 16.560 99.9 94.5 21.7 90 99.9 94.4 24.5 Table 2: The puriﬁcation results of all samples with different denoising steps , where ACCclean and ACCadv denote the accuracy of clean and adversarial samples after the denoising process, respectively. Denoising steps As discussed in § 4.2, the choice of the denoising starting point kis essential to successful detection. Under different starting points, we use Gaussian kernel density ( Parzen ,1962 )t o calculate the distributions of prepost denoisingsentence similarity of all samples. It can be seen from Fig. 3that, the overlapping area of solid anddashed lines of the same color is gradually decreasing as the number of steps increases. The increase in the number of steps causes the adversarial samples to deviate more signiﬁcantly in the semantic space, thus separating them from the normal samples. However, it is not recommended to increase the number of steps consistently. On the one hand, the computational overhead is not worth it when the detection performance is good enough. On the other hand, more denoising steps mean that the denoising starting distribution is further away from the true sample distribution, leading to inaccurate score estimation for all samples and thus causing a decrease in detection performance. Adversarial Puriﬁcation Table 2shows the classiﬁcation accuracy of normal and adversarial samples after denoising. Referring to the setup of adversarial puriﬁcation ( Nie et al. ,2022 ;Y oon et al. , 2021 ), we reclassify the denoised sentence representations using the previously ﬁnetuned linear classiﬁer. Consistent with these adversarial puriﬁcations in the ﬁeld of computer vision research, thedenoising process is able to remove a portion of the adversarial perturbations. Although the improvement is weaker compared to defensive methods that677Sentence FGWS RDE CASN Schaeffer ( frank ) has to ﬁnd some hook ( pull ) on which to hang his persistently inconsequential ﬂick (useless movies), and it perils (might) as allright (well) be the resuscitation of the middleaged character. While it ’s genuinely cool to hear characters talk about early rap ( music ) records ( show ) (sugar hill gang , etc.) , the constant referencing ( references ) of hiphop arcana ( secrect ) can consign (alienate) (charge ) even the savviest audiences. Further proof that the coeur (epicenter) of neat (cool) , hermosa (beautiful), thoughtprovoking foreign cinema is smackdab ( pat) in the middle of dubya’s ( bush’s ) axis of evil. Table 3: Examples showing the sensitivity to subtle semantic gaps . The words replaced by the attack algorithm are underlined and followed by the original word in parentheses. For the proposed lowfrequency word substitutions in FGWS, we write them in brackets after the original text using red color. “ & ” mean positive and negative. improve model robustness such as adversarial training, our analysis. In SST2, most sentences are short texts, while in IMDB, they are long. AGNEWS is a fourcategory topic classiﬁcation dataset that includes the world, sports, business, and sci/tech. 5.1 Baselines We compare our\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Connection error.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with llama-api\n",
    "client = OpenAI(\n",
    "    api_key=\"LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\",\n",
    "    base_url=\"https://api.llma-api.com\"\n",
    ")\n",
    "\n",
    "# Function to extract information using ChatGPT based on category\n",
    "def extract_info_with_chatgpt(category, df):\n",
    "    try:\n",
    "        # Define mappings of category to corresponding columns\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Introduction', 'Background'],\n",
    "            'contribution': ['Abstract', 'Conclusion'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        # Get the list of columns corresponding to the specified category\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        # Combine text from selected columns into a single string for ChatGPT input\n",
    "        combined_text = ' '.join([' '.join(df[column].astype(str)) for column in columns_to_search])\n",
    "        print(\"Combined Text:\", combined_text)\n",
    "        \n",
    "        # Use ChatGPT to extract information based on the combined text       \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"Extract the {category} of the paper:\"},\n",
    "            {\"role\": \"user\", \"content\": combined_text},\n",
    "        ]\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-13b-chat\",\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # Extract the response message\n",
    "        response_message = response['choices'][0]['message']['content']\n",
    "\n",
    "        # Return the extracted information\n",
    "        return response_message\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'Dataset'\n",
    "\n",
    "\n",
    "\n",
    "LLAMA_ExtractedInfo = extract_info_with_chatgpt(category, df)\n",
    "\n",
    "# Display the extracted information\n",
    "print(LLAMA_ExtractedInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38aae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8adb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df[['ExtractedInfo','LLAMA_ExtractedInfo']]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd32e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install bert-score==0.3.10\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*bert-score.*\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store BERTScore results\n",
    "score_results = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    extracted_info = row['ExtractedInfo']\n",
    "    llama_extracted_info = row['LLAMA_ExtractedInfo']\n",
    "    \n",
    "    # Compute BERTScore for the current row\n",
    "    P, R, F1 = score([extracted_info], [llama_extracted_info], lang='en')\n",
    "    \n",
    "    # Format BERTScore results as a string\n",
    "    score_str = f\"Precision: {P.item():.6f}, Recall: {R.item():.6f}, F1 Score: {F1.item():.6f}\"\n",
    "    \n",
    "    # Append formatted score string to the list\n",
    "    score_results.append(score_str)\n",
    "\n",
    "# Add BERTScore results as a new column in the DataFrame\n",
    "result_df['BERTScore'] = score_results\n",
    "\n",
    "# Display the updated DataFrame with BERTScore results\n",
    "result_df['BERTScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76eac525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement llama-api (from versions: none)\n",
      "ERROR: No matching distribution found for llama-api\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f747525",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: LL-NHaGj*******************************************************DzuZ. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m user_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho were the founders of Microsoft?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make a completion request using LLAMA client\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[0;32m     16\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[system_message, user_message]\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    581\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    582\u001b[0m             {\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    585\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    586\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    587\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    589\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    590\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    591\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    592\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    593\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    594\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    595\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    597\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    598\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    599\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    600\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    602\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    603\u001b[0m             },\n\u001b[0;32m    604\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    605\u001b[0m         ),\n\u001b[0;32m    606\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    607\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    608\u001b[0m         ),\n\u001b[0;32m    609\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    610\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    612\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    922\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    923\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    924\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    925\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    926\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    927\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: LL-NHaGj*******************************************************DzuZ. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ'],  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.completions.create(model='llama-13b-chat')\n",
    "print(completion.choices[0].text)\n",
    "print(dict(completion).get('usage'))\n",
    "print(completion.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e30d5d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m      7\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# supply your API key however you choose\u001b[39;00m\n\u001b[0;32m      9\u001b[0m functions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformation_extraction\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracts the relevant information from the passage.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         }\n\u001b[0;32m     21\u001b[0m     ]\n\u001b[1;32m---> 23\u001b[0m completion \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-13b-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtract the desired information from the following passage.:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mI love you, but not so much!\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[0;32m     25\u001b[0m         functions\u001b[38;5;241m=\u001b[39mfunctions,\n\u001b[0;32m     26\u001b[0m         function_call \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation_extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     27\u001b[0m         )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "# This example is the old way to use the OpenAI lib for python\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_base = \"https://api.llama-api.com\"\n",
    "\n",
    "openai.api_key = \"LL-NHaGjeZRbAINzMAxaIUAanVvSvsuGomEL8QuzfZXKfXds3SuSrScDEz8tLyWDzuZ\"  # supply your API key however you choose\n",
    "\n",
    "functions = [\n",
    "        {'name': 'information_extraction',\n",
    "         'description': 'Extracts the relevant information from the passage.',\n",
    "         'parameters': {\n",
    "             'type': 'object',\n",
    "             'properties': {\n",
    "                 'sentiment': {'title': 'sentiment', 'type': 'string', 'description': 'the sentiment encountered in the passage'},\n",
    "                 'aggressiveness': {'title': 'aggressiveness', 'type': 'integer', 'description': 'a 0-10 score of how aggressive the passage is'},\n",
    "                 'language': {'title': 'language', 'type': 'string', 'description': 'the language of the passage'},\n",
    "             }, 'required': []\n",
    "         }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "completion = openai.ChatCompletion.create(model=\"llama-13b-chat\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Extract the desired information from the following passage.:\\n\\nI love you, but not so much!\"}],\n",
    "        functions=functions,\n",
    "        function_call = {\"name\": \"information_extraction\"}\n",
    "        )\n",
    "\n",
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8f037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27dd2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490229bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34865\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anthropic\n",
    "\n",
    "YOUR_ANTHROPIC_API_KEY = \"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\"\n",
    "# Initialize the Anthropic client\n",
    "client = anthropic.Client(api_key=YOUR_ANTHROPIC_API_KEY)\n",
    "\n",
    "def extract_info_with_claude(category, df):\n",
    "    try:\n",
    "        # Define mappings of category to corresponding columns\n",
    "        category_mappings = {\n",
    "            'purpose': ['Abstract', 'Introduction', 'Background'],\n",
    "            'contribution': ['Abstract', 'Conclusion'],\n",
    "            'method': ['Methodology'],\n",
    "            'dataset': ['Experiments'],\n",
    "            'findings': ['Conclusion'],\n",
    "            'future_work': ['Conclusion'],\n",
    "            'conclusion': ['Conclusion'],\n",
    "        }\n",
    "\n",
    "        category = category.lower()\n",
    "\n",
    "        # Get the list of columns corresponding to the specified category\n",
    "        columns_to_search = category_mappings.get(category)\n",
    "        if not columns_to_search:\n",
    "            raise ValueError(f\"Invalid category: {category}\")\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            text = ' '.join([str(row[col]) for col in columns_to_search if row[col] is not None])\n",
    "            \n",
    "            if text:\n",
    "                prompt = f\"Extract the {category} of the paper:\\n\\n{text}\"\n",
    "                message = client.messages.create(\n",
    "                    model=\"claude-3-opus-20240229\",\n",
    "                    max_tokens=4000,\n",
    "                    temperature=0,\n",
    "                    system=f\"i want to know {category} of the text\\n\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": f\"{text}\"\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                response_message = message.content\n",
    "                print(response_message)\n",
    "                df.loc[index, 'CLAUDE_ExtractedInfo'] = response_message\n",
    "\n",
    "        print(\"exit\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Specify the category ('purpose', 'contribution', 'method', 'dataset', 'findings', 'future_work', 'conclusion')\n",
    "category = 'contribution'\n",
    "\n",
    "# Assume df is your DataFrame\n",
    "df['CLAUDE_ExtractedInfo'] = None\n",
    "\n",
    "# Apply the function to extract information based on the specified category\n",
    "df = extract_info_with_claude(category, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a56f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Error: module 'gensim' has no attribute 'summarize'\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dec27b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(text='I apologize for the confusion, but you haven\\'t provided any specific text data for me to analyze or determine the purpose of. \"text\" on its own is a very broad and generic term.\\n\\nIf you have a particular piece of text you would like me to examine, please provide the actual text content or a more detailed description of what the text is about. That way, I can better understand the context and purpose of the text data you are referring to, and provide a more accurate and helpful response.', type='text')]\n"
     ]
    }
   ],
   "source": [
    "message = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    max_tokens=4000,\n",
    "    temperature=0,\n",
    "    system=\"i want to know purpose of the textdata\\n\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"text\\n\\n\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Print the generated text\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59edb5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlock(text='I apologize for the confusion, but you haven\\'t provided any specific text data for me to analyze or determine the purpose of. \"text\" on its own is a very broad and generic term.\\n\\nIf you have a particular piece of text you would like me to examine, please provide the actual text content or a more detailed description of what the text is about. That way, I can better understand the context and purpose of the text data you are referring to, and provide a more accurate and helpful response.', type='text')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textdata = message.content\n",
    "\n",
    "type(textdata)\n",
    "print(textdata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3b8a4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the confusion, but you haven't provided any specific text data for me to analyze or determine the purpose of. \"text\" on its own is a very broad and generic term.\n",
      "\n",
      "If you have a particular piece of text you would like me to examine, please provide the actual text content or a more detailed description of what the text is about. That way, I can better understand the context and purpose of the text data you are referring to, and provide a more accurate and helpful response.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract text from TextBlock objects\n",
    "def extract_text_from_textblock(textblock):\n",
    "    return textblock.text\n",
    "\n",
    "# Extract text content from each TextBlock in the list\n",
    "extracted_texts = [extract_text_from_textblock(tb) for tb in textdata]\n",
    "\n",
    "# Print the extracted text contents\n",
    "for text in extracted_texts:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e783f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize for the confusion, but you haven\\'t provided any specific text data for me to analyze or determine the purpose of. \"text\" on its own is a very broad and generic term.\\n\\nIf you have a particular piece of text you would like me to examine, please provide the actual text content or a more detailed description of what the text is about. That way, I can better understand the context and purpose of the text data you are referring to, and provide a more accurate and helpful response.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6dcf32fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract text content from a TextBlock object (dictionary)\n",
    "def extract_text_from_textblock(textblock_dict):\n",
    "    if isinstance(textblock_dict, dict) and 'text' in textblock_dict:\n",
    "        return textblock_dict['text']\n",
    "    else:\n",
    "        return None  # Handle cases where input is not a valid TextBlock dictionary\n",
    "\n",
    "# Extract text content from the TextBlock dictionary\n",
    "extracted_text = extract_text_from_textblock(textdata[0])\n",
    "\n",
    "# Print the extracted text content\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b5db5baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TextBlock' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extracted_texts\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Apply the 'extract_text_from_textblocks' function to the 'CLAUDE_ExtractedInfo' column\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtractedTexts\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLAUDE_ExtractedInfo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_text_from_textblocks)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Print the DataFrame with extracted text content\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtractedTexts\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[88], line 19\u001b[0m, in \u001b[0;36mextract_text_from_textblocks\u001b[1;34m(textblocks_list)\u001b[0m\n\u001b[0;32m     17\u001b[0m extracted_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m textblock \u001b[38;5;129;01min\u001b[39;00m textblocks_list:\n\u001b[1;32m---> 19\u001b[0m     extracted_text \u001b[38;5;241m=\u001b[39m extract_text_from_textblock(textblock[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Extract text from the first TextBlock in the list\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extracted_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m         extracted_texts\u001b[38;5;241m.\u001b[39mappend(extracted_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'TextBlock' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the TextBlock class with 'text' attribute\n",
    "class TextBlock:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "# Function to extract text from a single TextBlock object\n",
    "def extract_text_from_textblock(textblock):\n",
    "    if isinstance(textblock, TextBlock):\n",
    "        return textblock.text\n",
    "    else:\n",
    "        return None  # Handle cases where input is not a valid TextBlock object\n",
    "\n",
    "# Function to extract text from a list of TextBlock objects\n",
    "def extract_text_from_textblocks(textblocks_list):\n",
    "    extracted_texts = []\n",
    "    for textblock in textblocks_list:\n",
    "        extracted_text = extract_text_from_textblock(textblock[0])  # Extract text from the first TextBlock in the list\n",
    "        if extracted_text is not None:\n",
    "            extracted_texts.append(extracted_text)\n",
    "    return extracted_texts\n",
    "\n",
    "# Apply the 'extract_text_from_textblocks' function to the 'CLAUDE_ExtractedInfo' column\n",
    "new['ExtractedTexts'] = df['CLAUDE_ExtractedInfo'].apply(extract_text_from_textblocks)\n",
    "\n",
    "# Print the DataFrame with extracted text content\n",
    "print(new['ExtractedTexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ed04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
